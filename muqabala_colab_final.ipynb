{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4992059cdf0b4b3eb206f37b32f97373": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_78f7d66e31fc4423b941e7ee7041205c",
              "IPY_MODEL_dd9486b3102549ca97f8ce68aad0a107",
              "IPY_MODEL_13b78b4412b9427d8539fbc77bd4ea3b"
            ],
            "layout": "IPY_MODEL_be9fc135ac0b45129725ef42415c8e9d"
          }
        },
        "78f7d66e31fc4423b941e7ee7041205c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a75efdcc775410dbad367e9fa406fd0",
            "placeholder": "​",
            "style": "IPY_MODEL_61ed3a9682994d0ca0328bbf86f15ef5",
            "value": "config.json: 100%"
          }
        },
        "dd9486b3102549ca97f8ce68aad0a107": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b07d810c5cba485aa04d44a0fd9cbcf8",
            "max": 629,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_21629b56990e4c6e8e593d3b9d5567ac",
            "value": 629
          }
        },
        "13b78b4412b9427d8539fbc77bd4ea3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5fdcb0cd817462eac9c626aaf7304f8",
            "placeholder": "​",
            "style": "IPY_MODEL_f7687eaef51040fda53b1f63506791ca",
            "value": " 629/629 [00:00&lt;00:00, 18.5kB/s]"
          }
        },
        "be9fc135ac0b45129725ef42415c8e9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a75efdcc775410dbad367e9fa406fd0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61ed3a9682994d0ca0328bbf86f15ef5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b07d810c5cba485aa04d44a0fd9cbcf8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21629b56990e4c6e8e593d3b9d5567ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d5fdcb0cd817462eac9c626aaf7304f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7687eaef51040fda53b1f63506791ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "06528d769e2e4330a0ce23941372c6c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_63cb38ebbe42492e95d8360492dc641b",
              "IPY_MODEL_af6fa5e3cba34ad3bea09a9aafbb87fd",
              "IPY_MODEL_95dd52118c7c461d865d0f2ea8d3a50f"
            ],
            "layout": "IPY_MODEL_7a480e40435746c68cc095a02966e9e4"
          }
        },
        "63cb38ebbe42492e95d8360492dc641b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e74763021c1b4762a04a18250de419f3",
            "placeholder": "​",
            "style": "IPY_MODEL_39671ce8e1c2437985982c048e60547c",
            "value": "model.safetensors: 100%"
          }
        },
        "af6fa5e3cba34ad3bea09a9aafbb87fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a32b31a2532d494d96157e9aadc5c3a5",
            "max": 267832558,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cc899ab0930b4eb0a3a4f785b4783179",
            "value": 267832558
          }
        },
        "95dd52118c7c461d865d0f2ea8d3a50f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da0add64da7644349170728727ade9c6",
            "placeholder": "​",
            "style": "IPY_MODEL_b29c963c9d0347adbca10b94d9588cea",
            "value": " 268M/268M [00:02&lt;00:00, 133MB/s]"
          }
        },
        "7a480e40435746c68cc095a02966e9e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e74763021c1b4762a04a18250de419f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39671ce8e1c2437985982c048e60547c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a32b31a2532d494d96157e9aadc5c3a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc899ab0930b4eb0a3a4f785b4783179": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "da0add64da7644349170728727ade9c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b29c963c9d0347adbca10b94d9588cea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b42a3e9d5014e729ba60a245ac9ec7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f5c48618753c492ebde5beb5f3ee6caa",
              "IPY_MODEL_8b9e7369abc04c0aacd560bfc6af99ec",
              "IPY_MODEL_b50482e58d4b4a5aa6038247513f8f8f"
            ],
            "layout": "IPY_MODEL_db76187685f740759896e69169793ba2"
          }
        },
        "f5c48618753c492ebde5beb5f3ee6caa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_832c2e9451354d9991358b68f009d8a4",
            "placeholder": "​",
            "style": "IPY_MODEL_a6b120977edf4b35a126212863ba8c67",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "8b9e7369abc04c0aacd560bfc6af99ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d46536c45d6d4e3eb86fb1a3adc2b6e9",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7ada6bc3d0ee494b86e25564d0492ad7",
            "value": 48
          }
        },
        "b50482e58d4b4a5aa6038247513f8f8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd44bbe82dd64182a0b877d7cefde2c4",
            "placeholder": "​",
            "style": "IPY_MODEL_9ac31765635442389d76ebf75beb3956",
            "value": " 48.0/48.0 [00:00&lt;00:00, 5.06kB/s]"
          }
        },
        "db76187685f740759896e69169793ba2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "832c2e9451354d9991358b68f009d8a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6b120977edf4b35a126212863ba8c67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d46536c45d6d4e3eb86fb1a3adc2b6e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ada6bc3d0ee494b86e25564d0492ad7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cd44bbe82dd64182a0b877d7cefde2c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ac31765635442389d76ebf75beb3956": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a62395f3fb60491e9e960f1fa547144c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3816f862d57f415591fa98c7fcbe2e37",
              "IPY_MODEL_e88a47939ad94ee0b1331540209a1d55",
              "IPY_MODEL_4900798395154733aab577884679eeeb"
            ],
            "layout": "IPY_MODEL_3bff72b703bb4bc394e2d6ad9b935b22"
          }
        },
        "3816f862d57f415591fa98c7fcbe2e37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a60c354b76b44b5b971a522fca8d3860",
            "placeholder": "​",
            "style": "IPY_MODEL_2b24a06b794b417bbeb118649045c98d",
            "value": "vocab.txt: 100%"
          }
        },
        "e88a47939ad94ee0b1331540209a1d55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5e5c2435f654f6baa4fc25d1c07082c",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_36890d7f44214f39a31a24081adea427",
            "value": 231508
          }
        },
        "4900798395154733aab577884679eeeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da131250558141a3b284ce9645a2acc2",
            "placeholder": "​",
            "style": "IPY_MODEL_c4e5b66c83eb4062a41e9777457b856d",
            "value": " 232k/232k [00:00&lt;00:00, 2.56MB/s]"
          }
        },
        "3bff72b703bb4bc394e2d6ad9b935b22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a60c354b76b44b5b971a522fca8d3860": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b24a06b794b417bbeb118649045c98d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b5e5c2435f654f6baa4fc25d1c07082c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36890d7f44214f39a31a24081adea427": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "da131250558141a3b284ce9645a2acc2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4e5b66c83eb4062a41e9777457b856d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Uzumaki-Naruto07/Data_Vis/blob/main/muqabala_colab_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1: SETUP AND IMPORTS"
      ],
      "metadata": {
        "id": "fKi8vWP4iqJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "un_exDGpdM4l",
        "outputId": "49d0d545-7853-497e-e0bc-e021b6978ebc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install libasound2-dev portaudio19-dev libportaudio2 libportaudiocpp0 ffmpeg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzguyJTIl0dt",
        "outputId": "e4d33b15-1726-4fa8-cbc4-7d82f74e4ce0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libasound2-dev is already the newest version (1.2.6.1-1ubuntu1).\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "Suggested packages:\n",
            "  portaudio19-doc\n",
            "The following NEW packages will be installed:\n",
            "  libportaudio2 libportaudiocpp0 portaudio19-dev\n",
            "0 upgraded, 3 newly installed, 0 to remove and 29 not upgraded.\n",
            "Need to get 188 kB of archives.\n",
            "After this operation, 927 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libportaudio2 amd64 19.6.0-1.1 [65.3 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libportaudiocpp0 amd64 19.6.0-1.1 [16.1 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 portaudio19-dev amd64 19.6.0-1.1 [106 kB]\n",
            "Fetched 188 kB in 0s (697 kB/s)\n",
            "Selecting previously unselected package libportaudio2:amd64.\n",
            "(Reading database ... 124947 files and directories currently installed.)\n",
            "Preparing to unpack .../libportaudio2_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking libportaudio2:amd64 (19.6.0-1.1) ...\n",
            "Selecting previously unselected package libportaudiocpp0:amd64.\n",
            "Preparing to unpack .../libportaudiocpp0_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking libportaudiocpp0:amd64 (19.6.0-1.1) ...\n",
            "Selecting previously unselected package portaudio19-dev:amd64.\n",
            "Preparing to unpack .../portaudio19-dev_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking portaudio19-dev:amd64 (19.6.0-1.1) ...\n",
            "Setting up libportaudio2:amd64 (19.6.0-1.1) ...\n",
            "Setting up libportaudiocpp0:amd64 (19.6.0-1.1) ...\n",
            "Setting up portaudio19-dev:amd64 (19.6.0-1.1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch huggingface_hub gtts pandas numpy scikit-learn joblib SpeechRecognition pyaudio tensorflow matplotlib datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yW-n4quokqQp",
        "outputId": "1b135035-8815-4d8b-a05d-eac3c991ba6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.28.1)\n",
            "Collecting gtts\n",
            "  Downloading gTTS-2.5.4-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.4.2)\n",
            "Collecting SpeechRecognition\n",
            "  Downloading SpeechRecognition-3.14.1-py3-none-any.whl.metadata (31 kB)\n",
            "Collecting pyaudio\n",
            "  Downloading PyAudio-0.2.14.tar.gz (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.11/dist-packages (from gtts) (8.1.8)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gTTS-2.5.4-py3-none-any.whl (29 kB)\n",
            "Downloading SpeechRecognition-3.14.1-py3-none-any.whl (32.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.9/32.9 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyaudio\n",
            "  Building wheel for pyaudio (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyaudio: filename=pyaudio-0.2.14-cp311-cp311-linux_x86_64.whl size=67396 sha256=2eafbc251e0221606f7ca046838b1eb537f8f66ba3251bded6d819656541d25e\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/b1/c1/67e4ef443de2665d86031d4760508094eab5de37d5d64d9c27\n",
            "Successfully built pyaudio\n",
            "Installing collected packages: pyaudio, xxhash, SpeechRecognition, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, gtts, nvidia-cusolver-cu12, datasets\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed SpeechRecognition-3.14.1 datasets-3.3.2 dill-0.3.8 gtts-2.5.4 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pyaudio-0.2.14 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============== SECTION 1: SETUP AND IMPORTS ==============\n",
        "# Install required packages with pip:\n",
        "# pip install transformers torch huggingface_hub gtts pandas numpy scikit-learn joblib SpeechRecognition pyaudio tensorflow matplotlib datasets torch\n",
        "\n",
        "# Initialize Colab detection variable at the very beginning\n",
        "USING_COLAB = False\n",
        "\n",
        "\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from getpass import getpass\n",
        "import joblib\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "# Initialize feature flags\n",
        "TTS_AVAILABLE = False\n",
        "SPEECH_RECOGNITION_AVAILABLE = False\n",
        "ML_AVAILABLE = False\n",
        "TF_AVAILABLE = False\n",
        "XGBOOST_AVAILABLE = False\n",
        "TRANSFORMERS_AVAILABLE = False\n",
        "SENTIMENT_ANALYSIS_AVAILABLE = False\n",
        "DATASETS_AVAILABLE = False\n",
        "\n",
        "# Try importing optional packages\n",
        "try:\n",
        "    from sklearn.linear_model import LinearRegression\n",
        "    from sklearn.ensemble import RandomForestClassifier, GradientBoostingRegressor\n",
        "    from sklearn.model_selection import train_test_split, cross_val_score\n",
        "    from sklearn.metrics import mean_squared_error, accuracy_score, r2_score\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    import joblib\n",
        "    import matplotlib.pyplot as plt\n",
        "    ML_AVAILABLE = True\n",
        "    print(\"✅ Machine learning support available\")\n",
        "except ImportError:\n",
        "    print(\"❌ Machine learning libraries not available\")\n",
        "\n",
        "try:\n",
        "    # TensorFlow is optional for neural network models\n",
        "    import tensorflow as tf\n",
        "    from tensorflow.keras.models import Sequential\n",
        "    from tensorflow.keras.layers import Dense, Dropout\n",
        "    from tensorflow.keras.optimizers import Adam\n",
        "    TF_AVAILABLE = True\n",
        "    print(\"✅ TensorFlow/Keras support available for neural networks\")\n",
        "except ImportError:\n",
        "    print(\"❌ TensorFlow not available - advanced ML models will be limited\")\n",
        "\n",
        "try:\n",
        "    from gtts import gTTS\n",
        "    import os\n",
        "    TTS_AVAILABLE = True\n",
        "    print(\"✅ Text-to-speech support available\")\n",
        "except ImportError:\n",
        "    print(\"❌ Text-to-speech not available\")\n",
        "\n",
        "try:\n",
        "    import speech_recognition as sr\n",
        "    SPEECH_RECOGNITION_AVAILABLE = True\n",
        "    print(\"✅ Speech recognition support available\")\n",
        "except ImportError:\n",
        "    print(\"❌ Speech recognition not available\")\n",
        "\n",
        "try:\n",
        "    from huggingface_hub import login\n",
        "    from transformers import pipeline\n",
        "\n",
        "    # Initialize the transformers-specific flags\n",
        "    TRANSFORMERS_AVAILABLE = True\n",
        "\n",
        "    # Try to set up sentiment analysis\n",
        "    try:\n",
        "        sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "        SENTIMENT_ANALYSIS_AVAILABLE = True\n",
        "        print(\"✅ Sentiment analysis model loaded and ready\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Sentiment analysis model couldn't be loaded: {str(e)}\")\n",
        "\n",
        "    print(\"✅ Hugging Face support available\")\n",
        "except ImportError:\n",
        "    print(\"❌ Hugging Face libraries not available, LLM functionality will be limited\")\n",
        "\n",
        "# Try loading the datasets library for sentiment dataset\n",
        "try:\n",
        "    from datasets import load_dataset\n",
        "    DATASETS_AVAILABLE = True\n",
        "    print(\"✅ Hugging Face datasets library available\")\n",
        "except ImportError:\n",
        "    print(\"❌ Hugging Face datasets library not available\")\n",
        "\n",
        "print(\"\\nSection 1 complete: Setup and imports finished.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336,
          "referenced_widgets": [
            "4992059cdf0b4b3eb206f37b32f97373",
            "78f7d66e31fc4423b941e7ee7041205c",
            "dd9486b3102549ca97f8ce68aad0a107",
            "13b78b4412b9427d8539fbc77bd4ea3b",
            "be9fc135ac0b45129725ef42415c8e9d",
            "0a75efdcc775410dbad367e9fa406fd0",
            "61ed3a9682994d0ca0328bbf86f15ef5",
            "b07d810c5cba485aa04d44a0fd9cbcf8",
            "21629b56990e4c6e8e593d3b9d5567ac",
            "d5fdcb0cd817462eac9c626aaf7304f8",
            "f7687eaef51040fda53b1f63506791ca",
            "06528d769e2e4330a0ce23941372c6c6",
            "63cb38ebbe42492e95d8360492dc641b",
            "af6fa5e3cba34ad3bea09a9aafbb87fd",
            "95dd52118c7c461d865d0f2ea8d3a50f",
            "7a480e40435746c68cc095a02966e9e4",
            "e74763021c1b4762a04a18250de419f3",
            "39671ce8e1c2437985982c048e60547c",
            "a32b31a2532d494d96157e9aadc5c3a5",
            "cc899ab0930b4eb0a3a4f785b4783179",
            "da0add64da7644349170728727ade9c6",
            "b29c963c9d0347adbca10b94d9588cea",
            "6b42a3e9d5014e729ba60a245ac9ec7c",
            "f5c48618753c492ebde5beb5f3ee6caa",
            "8b9e7369abc04c0aacd560bfc6af99ec",
            "b50482e58d4b4a5aa6038247513f8f8f",
            "db76187685f740759896e69169793ba2",
            "832c2e9451354d9991358b68f009d8a4",
            "a6b120977edf4b35a126212863ba8c67",
            "d46536c45d6d4e3eb86fb1a3adc2b6e9",
            "7ada6bc3d0ee494b86e25564d0492ad7",
            "cd44bbe82dd64182a0b877d7cefde2c4",
            "9ac31765635442389d76ebf75beb3956",
            "a62395f3fb60491e9e960f1fa547144c",
            "3816f862d57f415591fa98c7fcbe2e37",
            "e88a47939ad94ee0b1331540209a1d55",
            "4900798395154733aab577884679eeeb",
            "3bff72b703bb4bc394e2d6ad9b935b22",
            "a60c354b76b44b5b971a522fca8d3860",
            "2b24a06b794b417bbeb118649045c98d",
            "b5e5c2435f654f6baa4fc25d1c07082c",
            "36890d7f44214f39a31a24081adea427",
            "da131250558141a3b284ce9645a2acc2",
            "c4e5b66c83eb4062a41e9777457b856d"
          ]
        },
        "id": "mNvqx-hcj1vX",
        "outputId": "375145fe-05c3-438b-ae31-0613f583fc48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Machine learning support available\n",
            "✅ TensorFlow/Keras support available for neural networks\n",
            "✅ Text-to-speech support available\n",
            "✅ Speech recognition support available\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4992059cdf0b4b3eb206f37b32f97373"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "06528d769e2e4330a0ce23941372c6c6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6b42a3e9d5014e729ba60a245ac9ec7c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a62395f3fb60491e9e960f1fa547144c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Sentiment analysis model loaded and ready\n",
            "✅ Hugging Face support available\n",
            "✅ Hugging Face datasets library available\n",
            "\n",
            "Section 1 complete: Setup and imports finished.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 2: API SETUP"
      ],
      "metadata": {
        "id": "TzhZXge_inJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============== SECTION 2: API SETUP ==============\n",
        "# Set up the Hugging Face API\n",
        "\n",
        "def setup_huggingface_api():\n",
        "    \"\"\"Get Hugging Face API token from Colab userdata, environment, or user input\"\"\"\n",
        "    hf_token = None\n",
        "\n",
        "    # First try to get from Colab userdata\n",
        "    if USING_COLAB:\n",
        "        try:\n",
        "            hf_token = userdata.get('HUGGINGFACE_API_KEY')\n",
        "            print(\"Retrieved Hugging Face API key from Colab secrets\")\n",
        "        except Exception as e:\n",
        "            print(f\"Could not retrieve API key from Colab secrets: {e}\")\n",
        "\n",
        "    # If not found in Colab, check environment\n",
        "    if not hf_token:\n",
        "        hf_token = os.environ.get(\"HUGGINGFACE_API_KEY\")\n",
        "\n",
        "    # If still not found, ask user\n",
        "    if not hf_token:\n",
        "        hf_token = getpass(\"Enter your Hugging Face API token: \")\n",
        "        os.environ[\"HUGGINGFACE_API_KEY\"] = hf_token\n",
        "\n",
        "    # Log in to Hugging Face\n",
        "    login(token=hf_token)\n",
        "    print(\"Hugging Face API setup complete!\")\n",
        "    return hf_token\n",
        "\n",
        "# Execute this section\n",
        "try:\n",
        "    api_token = setup_huggingface_api()\n",
        "    print(\"Section 2 complete: API setup successful.\\n\")\n",
        "except Exception as e:\n",
        "    print(f\"Error setting up API: {e}\")\n",
        "    print(\"Please ensure you have a valid Hugging Face API token.\\n\")\n"
      ],
      "metadata": {
        "id": "JFYVP7sh8mnA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05f5031e-9a5a-484a-9460-f0065a52b4bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your Hugging Face API token: ··········\n",
            "Hugging Face API setup complete!\n",
            "Section 2 complete: API setup successful.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vPJrTE3BYXaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 3: LLM Configuration (Optional)"
      ],
      "metadata": {
        "id": "r4Gp8oBUij4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============== SECTION 3: LANGUAGE MODEL SETUP ==============\n",
        "# Configure the LLM using Hugging Face API\n",
        "\n",
        "def generate_with_api(prompt, model_id=\"microsoft/phi-2\"):\n",
        "    \"\"\"Generate text using the Hugging Face API\"\"\"\n",
        "    API_URL = f\"https://api-inference.huggingface.co/models/{model_id}\"\n",
        "    headers = {\"Authorization\": f\"Bearer {os.environ['HUGGINGFACE_API_KEY']}\"}\n",
        "\n",
        "    payload = {\n",
        "        \"inputs\": prompt,\n",
        "        \"parameters\": {\n",
        "            \"max_new_tokens\": 1024,\n",
        "            \"temperature\": 0.7,\n",
        "            \"top_p\": 0.9,\n",
        "            \"do_sample\": True\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Add retry logic for API calls\n",
        "    max_retries = 3\n",
        "    retry_delay = 2  # seconds\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = requests.post(API_URL, headers=headers, json=payload, timeout=30)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                result = response.json()\n",
        "                if isinstance(result, list) and len(result) > 0:\n",
        "                    # Extract response text\n",
        "                    generated_text = result[0].get(\"generated_text\", \"\")\n",
        "                    # Remove the prompt from the response\n",
        "                    if generated_text.startswith(prompt):\n",
        "                        generated_text = generated_text[len(prompt):]\n",
        "                    return generated_text.strip()\n",
        "                return str(result)\n",
        "            elif response.status_code == 503:\n",
        "                print(f\"Model is loading. Attempt {attempt+1}/{max_retries}. Waiting {retry_delay} seconds...\")\n",
        "                time.sleep(retry_delay)\n",
        "                retry_delay *= 2  # Exponential backoff\n",
        "            else:\n",
        "                error_msg = f\"Error: {response.status_code} - {response.text}\"\n",
        "                print(error_msg)\n",
        "                if attempt == max_retries - 1:\n",
        "                    return error_msg\n",
        "                time.sleep(retry_delay)\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Error calling API: {str(e)}\"\n",
        "            print(error_msg)\n",
        "            if attempt == max_retries - 1:\n",
        "                return error_msg\n",
        "            time.sleep(retry_delay)\n",
        "\n",
        "    return \"Failed to get response after multiple attempts\"\n",
        "\n",
        "# Test the API with a simple prompt\n",
        "try:\n",
        "    test_prompt = \"Hello, this is a test.\"\n",
        "    response = generate_with_api(test_prompt)\n",
        "    print(f\"API Test Response: {response}\")\n",
        "\n",
        "    # Create a reusable function for generating responses\n",
        "    generate_response = lambda prompt: generate_with_api(prompt)\n",
        "    print(\"Section 3 complete: Language model setup successful.\\n\")\n",
        "except Exception as e:\n",
        "    print(f\"Error testing API: {e}\")\n",
        "    print(\"Section 3 incomplete: Language model setup failed.\\n\")"
      ],
      "metadata": {
        "id": "gpQvnHEn8o2j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "031cc8d5-1389-48ff-8417-a193165e8f2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model is loading. Attempt 1/3. Waiting 2 seconds...\n",
            "Model is loading. Attempt 2/3. Waiting 4 seconds...\n",
            "Model is loading. Attempt 3/3. Waiting 8 seconds...\n",
            "API Test Response: Failed to get response after multiple attempts\n",
            "Section 3 complete: Language model setup successful.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 4: TTS"
      ],
      "metadata": {
        "id": "-zC0ik3tigf4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============== SECTION 4: TEXT-TO-SPEECH AND SPEECH RECOGNITION ==============\n",
        "# Setup for bilingual text-to-speech and speech recognition\n",
        "\n",
        "def text_to_speech(text, lang='en'):\n",
        "    \"\"\"Convert text to speech using gTTS in the specified language\"\"\"\n",
        "    if not TTS_AVAILABLE:\n",
        "        print(f\"Text-to-speech not available. Text content: {text}\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # Check for a valid language code\n",
        "        valid_langs = {'en': 'English', 'ar': 'Arabic'}\n",
        "        if lang not in valid_langs:\n",
        "            print(f\"Language '{lang}' not supported. Defaulting to English.\")\n",
        "            lang = 'en'\n",
        "\n",
        "        print(f\"Converting text to speech ({valid_langs[lang]})...\")\n",
        "        tts = gTTS(text=text, lang=lang, slow=False)\n",
        "        audio_file = \"temp_audio.mp3\"\n",
        "        tts.save(audio_file)\n",
        "\n",
        "        # Play the audio file using the operating system's default player\n",
        "        if os.name == 'posix':  # For Linux/Mac\n",
        "            os.system(f\"afplay {audio_file}\" if os.uname().sysname == 'Darwin' else f\"mpg123 {audio_file}\")\n",
        "        else:  # For Windows\n",
        "            os.system(f\"start {audio_file}\")\n",
        "\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating speech: {e}\")\n",
        "        print(f\"Text content: {text}\")\n",
        "        return None\n",
        "\n",
        "def speech_to_text(lang='en'):\n",
        "    \"\"\"Convert speech to text using the specified language\"\"\"\n",
        "    if not SPEECH_RECOGNITION_AVAILABLE:\n",
        "        print(\"Speech recognition not available.\")\n",
        "        text = input(\"Please type your response instead: \")\n",
        "        return text\n",
        "\n",
        "    recognizer = sr.Recognizer()\n",
        "\n",
        "    try:\n",
        "        # Map language codes to recognition language\n",
        "        lang_map = {'en': 'en-US', 'ar': 'ar-SA'}\n",
        "        recognition_lang = lang_map.get(lang, 'en-US')\n",
        "\n",
        "        with sr.Microphone() as source:\n",
        "            print(f\"Listening for {lang} speech... (speak now)\")\n",
        "            recognizer.adjust_for_ambient_noise(source)\n",
        "            audio = recognizer.listen(source, timeout=10)\n",
        "\n",
        "        print(\"Processing speech...\")\n",
        "        text = recognizer.recognize_google(audio, language=recognition_lang)\n",
        "        print(f\"Recognized: {text}\")\n",
        "        return text\n",
        "    except sr.RequestError:\n",
        "        print(\"Could not request results from speech recognition service\")\n",
        "    except sr.UnknownValueError:\n",
        "        print(\"Could not understand audio\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error in speech recognition: {e}\")\n",
        "\n",
        "    # Fallback to text input\n",
        "    text = input(\"Please type your response instead: \")\n",
        "    return text\n",
        "\n",
        "# Test TTS with both languages\n",
        "if TTS_AVAILABLE:\n",
        "    print(\"Testing Text-to-Speech...\")\n",
        "    text_to_speech(\"Hello, this is a test in English.\", \"en\")\n",
        "    text_to_speech(\"مرحبا، هذا اختبار باللغة العربية.\", \"ar\")\n",
        "\n",
        "# Note: We won't test speech recognition here as it requires user interaction\n",
        "print(\"Section 4 complete: Text-to-speech and speech recognition setup finished.\\n\")"
      ],
      "metadata": {
        "id": "prvS4Kn-8q1z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "231bdd2f-9da0-41a4-dc98-d8acbf2030dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Text-to-Speech...\n",
            "Converting text to speech (English)...\n",
            "Converting text to speech (Arabic)...\n",
            "Section 4 complete: Text-to-speech and speech recognition setup finished.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 5a: Generate Data"
      ],
      "metadata": {
        "id": "Oga_lYmx_zKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# Section 5a: Generate Data\"\"\"\n",
        "\n",
        "# ============== SECTION 5: DATASET LOADING AND PREPARATION ==============\n",
        "# Load and process the Software Engineering interview datasets\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import traceback\n",
        "from datasets import load_dataset\n",
        "import time\n",
        "\n",
        "# Check ML libraries availability\n",
        "try:\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    ML_AVAILABLE = True\n",
        "except ImportError:\n",
        "    ML_AVAILABLE = False\n",
        "    print(\"Warning: Machine learning libraries not available.\")\n",
        "\n",
        "def load_software_engineer_datasets(use_huggingface=True, generate_if_missing=False, num_samples=100000):\n",
        "    \"\"\"Load Software Engineering interview datasets from Hugging Face or generate synthetic data\n",
        "\n",
        "    This function:\n",
        "    1. Attempts to load data from Hugging Face\n",
        "    2. Falls back to locally processed data if available\n",
        "    3. Generates synthetic data as a last resort\n",
        "\n",
        "    Args:\n",
        "        use_huggingface: Whether to try loading from Hugging Face first\n",
        "        generate_if_missing: If True and datasets are missing, generate synthetic data\n",
        "        num_samples: Number of samples to generate if needed\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with processed Software Engineering interview data\n",
        "    \"\"\"\n",
        "    if not ML_AVAILABLE:\n",
        "        print(\"Machine learning libraries not available. Cannot load and process datasets.\")\n",
        "        return None\n",
        "\n",
        "    # First try loading from Hugging Face if requested\n",
        "    if use_huggingface:\n",
        "        try:\n",
        "            print(\"Attempting to load Software Engineering interview dataset from Hugging Face...\")\n",
        "            # Load from Hugging Face\n",
        "            splits = {'train': 'data/train-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet'}\n",
        "            df = pd.read_parquet(\"hf://datasets/Vineeshsuiii/Software_Engineering_interview_datasets/\" + splits[\"train\"])\n",
        "\n",
        "            print(f\"Loaded {len(df)} records from Hugging Face dataset\")\n",
        "\n",
        "            # Generate synthetic scores for each record\n",
        "            print(\"Generating synthetic scores with controlled noise for realistic training...\")\n",
        "\n",
        "            # Function to add controlled noise to scores\n",
        "            def add_noise(score, noise_level=0.15):\n",
        "                noise = np.random.normal(0, noise_level * score, size=len(score))\n",
        "                return np.clip(score + noise, 0, 100)\n",
        "\n",
        "            # Generate initial scores based on text features\n",
        "            def calculate_base_score(row):\n",
        "                # Extract text features that might indicate question complexity\n",
        "                question = str(row.get('question', ''))\n",
        "                answer = str(row.get('answer', ''))\n",
        "\n",
        "                # Simple complexity indicators\n",
        "                complexity_indicators = ['complex', 'difficult', 'advanced', 'explain', 'design', 'implement']\n",
        "                basic_indicators = ['basic', 'simple', 'describe', 'what is', 'define']\n",
        "\n",
        "                # Calculate base score\n",
        "                base = 75  # Start with average score\n",
        "\n",
        "                # Adjust based on text length and complexity\n",
        "                word_count = len(question.split())\n",
        "                base += min(10, word_count / 20)  # Longer questions might be more complex\n",
        "\n",
        "                # Adjust for complexity indicators\n",
        "                complex_count = sum(1 for ind in complexity_indicators if ind in question.lower())\n",
        "                basic_count = sum(1 for ind in basic_indicators if ind in question.lower())\n",
        "\n",
        "                base += complex_count * 3\n",
        "                base -= basic_count * 2\n",
        "\n",
        "                # Add some randomness\n",
        "                base += np.random.normal(0, 5)\n",
        "\n",
        "                return np.clip(base, 50, 95)\n",
        "\n",
        "            # Calculate initial scores based on question content\n",
        "            df['base_score'] = df.apply(calculate_base_score, axis=1)\n",
        "\n",
        "            # Generate component scores with realistic correlations and noise\n",
        "            df['technical'] = add_noise(df['base_score'] + np.random.normal(0, 8, size=len(df)), noise_level=0.2)\n",
        "            df['problem_solving'] = add_noise(df['base_score'] + np.random.normal(0, 7, size=len(df)), noise_level=0.18)\n",
        "            df['communication'] = add_noise(df['base_score'] + np.random.normal(-2, 10, size=len(df)), noise_level=0.25)\n",
        "            df['cultural_fit'] = add_noise(df['base_score'] + np.random.normal(-5, 12, size=len(df)), noise_level=0.3)\n",
        "\n",
        "            # Introduce some outliers (about 5% of the data)\n",
        "            outlier_mask = np.random.choice([True, False], size=len(df), p=[0.05, 0.95])\n",
        "            for col in ['technical', 'problem_solving', 'communication', 'cultural_fit']:\n",
        "                df.loc[outlier_mask, col] = np.random.choice([\n",
        "                    np.random.uniform(30, 50),  # Low outliers\n",
        "                    np.random.uniform(90, 100)  # High outliers\n",
        "                ], size=sum(outlier_mask))\n",
        "\n",
        "            # Calculate overall score with weighted average and additional noise\n",
        "            df['overall'] = add_noise(\n",
        "                0.4 * df['technical'] +\n",
        "                0.3 * df['problem_solving'] +\n",
        "                0.2 * df['communication'] +\n",
        "                0.1 * df['cultural_fit'],\n",
        "                noise_level=0.1\n",
        "            )\n",
        "\n",
        "            # Round all scores to integers\n",
        "            score_columns = ['technical', 'communication', 'problem_solving', 'cultural_fit', 'overall']\n",
        "            for col in score_columns:\n",
        "                df[col] = df[col].round().astype(int)\n",
        "                df[col] = df[col].clip(0, 100)  # Ensure within valid range\n",
        "\n",
        "            # Drop the temporary base_score column\n",
        "            df = df.drop('base_score', axis=1)\n",
        "\n",
        "            print(\"\\nGenerated synthetic scores with realistic noise and correlations\")\n",
        "            print(f\"Score statistics:\")\n",
        "            for col in score_columns:\n",
        "                print(f\"- {col}:\")\n",
        "                print(f\"  Range: {df[col].min()}-{df[col].max()}\")\n",
        "                print(f\"  Mean: {df[col].mean():.1f}\")\n",
        "                print(f\"  Std: {df[col].std():.1f}\")\n",
        "\n",
        "            # Process and save the dataset\n",
        "            ml_dataset = process_dataset_for_ml(df)\n",
        "            ml_dataset.to_csv(\"processed_se_interview_data.csv\", index=False)\n",
        "            print(f\"\\nProcessed dataset saved with {len(ml_dataset)} records\")\n",
        "            return ml_dataset\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading from Hugging Face: {e}\")\n",
        "            print(\"Falling back to local processed dataset or synthetic data generation...\")\n",
        "\n",
        "    # Then try to load local processed dataset\n",
        "    try:\n",
        "        ml_dataset = pd.read_csv(\"processed_se_interview_data.csv\")\n",
        "        print(f\"Loaded pre-processed Software Engineering dataset with {len(ml_dataset)} records\")\n",
        "        return ml_dataset\n",
        "    except FileNotFoundError:\n",
        "        print(\"No processed Software Engineering dataset found, checking raw data files...\")\n",
        "\n",
        "    # Try loading from raw files\n",
        "    try:\n",
        "        # Load the datasets\n",
        "        questions_df = pd.read_csv(\"se_interview_questions.csv\")\n",
        "        evaluations_df = pd.read_csv(\"se_evaluation_scores.csv\")\n",
        "\n",
        "        print(f\"Loaded datasets:\")\n",
        "        print(f\"- SE Interview Questions: {len(questions_df)} records\")\n",
        "        print(f\"- SE Evaluation Scores: {len(evaluations_df)} records\")\n",
        "\n",
        "        # Merge datasets\n",
        "        merged_df = pd.merge(evaluations_df, questions_df, on='question_id', how='inner')\n",
        "        print(f\"- Merged Dataset: {len(merged_df)} records after joining\")\n",
        "\n",
        "        # Check if we have enough data\n",
        "        if len(merged_df) < 1000:\n",
        "            print(f\"Warning: Only {len(merged_df)} records available for training.\")\n",
        "            if generate_if_missing:\n",
        "                print(f\"Generating {num_samples} synthetic records as requested...\")\n",
        "                from generate_dataset import generate_software_engineer_dataset\n",
        "                return generate_software_engineer_dataset(num_samples)\n",
        "            else:\n",
        "                print(\"Consider generating more data for better model training.\")\n",
        "\n",
        "        # Process for ML using the helper function\n",
        "        ml_dataset = process_dataset_for_ml(merged_df)\n",
        "        ml_dataset.to_csv(\"processed_se_interview_data.csv\", index=False)\n",
        "\n",
        "        return ml_dataset\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        print(\"Required dataset files not found.\")\n",
        "\n",
        "        if generate_if_missing:\n",
        "            print(f\"Generating {num_samples} synthetic Software Engineer records...\")\n",
        "            from generate_dataset import generate_software_engineer_dataset\n",
        "            return generate_software_engineer_dataset(num_samples)\n",
        "        else:\n",
        "            print(\"To generate synthetic data, call load_software_engineer_datasets(generate_if_missing=True)\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing datasets: {e}\")\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "def process_dataset_for_ml(data):\n",
        "    \"\"\"Process a dataset to create ML-ready features for Software Engineering interviews.\n",
        "\n",
        "    Handles both HuggingFace datasets and synthetic data formats.\n",
        "    \"\"\"\n",
        "    print(\"Processing dataset for machine learning...\")\n",
        "    processed_df = data.copy()\n",
        "    feature_columns = []\n",
        "\n",
        "    # Ensure we have the expected target columns\n",
        "    required_targets = ['technical', 'communication', 'problem_solving', 'cultural_fit', 'overall']\n",
        "    missing_targets = [target for target in required_targets if target not in processed_df.columns]\n",
        "\n",
        "    if missing_targets:\n",
        "        print(f\"Generating synthetic values for missing targets: {missing_targets}\")\n",
        "\n",
        "        # Generate base score if no score/overall exists\n",
        "        if 'score' in processed_df.columns:\n",
        "            base = processed_df['score']\n",
        "        elif 'overall' in processed_df.columns:\n",
        "            base = processed_df['overall']\n",
        "        else:\n",
        "            base = np.random.randint(60, 100, size=len(processed_df))\n",
        "\n",
        "        # Generate missing targets with correlation to base score\n",
        "        for target in missing_targets:\n",
        "            processed_df[target] = np.clip(base + np.random.randint(-10, 11, size=len(processed_df)), 0, 100)\n",
        "            print(f\"Generated synthetic values for {target}\")\n",
        "\n",
        "    # Basic numeric features\n",
        "    if 'question_length' not in processed_df.columns and 'question' in processed_df.columns:\n",
        "        processed_df['question_length'] = processed_df['question'].apply(lambda x: len(str(x)) if pd.notna(x) else 0)\n",
        "        feature_columns.append('question_length')\n",
        "    else:\n",
        "        if 'question_length' in processed_df.columns:\n",
        "            feature_columns.append('question_length')\n",
        "\n",
        "    if 'word_count' not in processed_df.columns and 'question' in processed_df.columns:\n",
        "        processed_df['word_count'] = processed_df['question'].apply(lambda x: len(str(x).split()) if pd.notna(x) else 0)\n",
        "        feature_columns.append('word_count')\n",
        "    else:\n",
        "        if 'word_count' in processed_df.columns:\n",
        "            feature_columns.append('word_count')\n",
        "\n",
        "    # Answer-based features if available\n",
        "    if 'answer' in processed_df.columns:\n",
        "        if 'answer_length' not in processed_df.columns:\n",
        "            processed_df['answer_length'] = processed_df['answer'].apply(lambda x: len(str(x)) if pd.notna(x) else 0)\n",
        "        if 'answer_word_count' not in processed_df.columns:\n",
        "            processed_df['answer_word_count'] = processed_df['answer'].apply(lambda x: len(str(x).split()) if pd.notna(x) else 0)\n",
        "\n",
        "        feature_columns.extend(['answer_length', 'answer_word_count'])\n",
        "\n",
        "    # Handle candidate_id if present\n",
        "    if 'candidate_id' in processed_df.columns:\n",
        "        feature_columns.append('candidate_id')\n",
        "\n",
        "    # Date features\n",
        "    if 'evaluation_date' in processed_df.columns:\n",
        "        processed_df['evaluation_date'] = pd.to_datetime(processed_df['evaluation_date'])\n",
        "        processed_df['month'] = processed_df['evaluation_date'].dt.month\n",
        "        processed_df['year'] = processed_df['evaluation_date'].dt.year\n",
        "        feature_columns.extend(['month', 'year'])\n",
        "\n",
        "    # Handle categorical features - create dummies\n",
        "    categorical_features = []\n",
        "\n",
        "    if 'question_type' in processed_df.columns:\n",
        "        question_type_dummies = pd.get_dummies(processed_df['question_type'], prefix='question_type')\n",
        "        categorical_features.append(question_type_dummies)\n",
        "\n",
        "    if 'difficulty_level' in processed_df.columns:\n",
        "        difficulty_dummies = pd.get_dummies(processed_df['difficulty_level'], prefix='difficulty')\n",
        "        categorical_features.append(difficulty_dummies)\n",
        "\n",
        "    if 'topic' in processed_df.columns:\n",
        "        topic_dummies = pd.get_dummies(processed_df['topic'], prefix='topic')\n",
        "        categorical_features.append(topic_dummies)\n",
        "\n",
        "    # Create the ML dataset\n",
        "    ml_df = processed_df[feature_columns].copy() if feature_columns else pd.DataFrame()\n",
        "\n",
        "    # Add categorical features\n",
        "    for cat_feature in categorical_features:\n",
        "        ml_df = pd.concat([ml_df, cat_feature], axis=1)\n",
        "\n",
        "    # Add target variables (including the synthetic ones we generated)\n",
        "    for target in required_targets:\n",
        "        if target in processed_df.columns:\n",
        "            ml_df[target] = processed_df[target]\n",
        "\n",
        "    # Create recommendation classification if overall score is available\n",
        "    if 'overall' in ml_df.columns:\n",
        "        ml_df['recommendation'] = pd.cut(\n",
        "            ml_df['overall'],\n",
        "            bins=[0, 60, 80, 101],\n",
        "            labels=['Not Recommended', 'Consider', 'Strongly Recommended']\n",
        "        )\n",
        "\n",
        "    # Keep original text columns for reference\n",
        "    text_columns = ['question', 'answer', 'question_type', 'difficulty_level', 'topic']\n",
        "    for col in text_columns:\n",
        "        if col in processed_df.columns:\n",
        "            ml_df[col + '_text'] = processed_df[col]\n",
        "\n",
        "    print(f\"Dataset processed with {len(ml_df)} samples and {len(ml_df.columns)} features.\")\n",
        "    return ml_df\n",
        "\n",
        "# Replace the previous data loading with our new Software Engineering focused approach\n",
        "if ML_AVAILABLE:\n",
        "    print(\"Loading and preprocessing Software Engineering interview datasets...\")\n",
        "    training_data = load_software_engineer_datasets(use_huggingface=True)\n",
        "    if training_data is not None:\n",
        "        print(\"\\nTraining data sample:\")\n",
        "        print(training_data.head())\n",
        "    else:\n",
        "        print(\"Failed to load Software Engineering interview datasets. Please check for errors above.\")\n",
        "else:\n",
        "    print(\"Skipping dataset loading due to missing ML libraries.\")\n",
        "\n",
        "print(\"Section 5 complete: Software Engineering dataset preparation finished.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfGyFUpw_2TZ",
        "outputId": "d47c083f-c1d7-4e98-d380-055f17f0557b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and preprocessing Software Engineering interview datasets...\n",
            "Attempting to load Software Engineering interview dataset from Hugging Face...\n",
            "Loaded 2858 records from Hugging Face dataset\n",
            "Generating synthetic scores with controlled noise for realistic training...\n",
            "\n",
            "Generated synthetic scores with realistic noise and correlations\n",
            "Score statistics:\n",
            "- technical:\n",
            "  Range: 20-100\n",
            "  Mean: 74.0\n",
            "  Std: 17.3\n",
            "- communication:\n",
            "  Range: 0-100\n",
            "  Mean: 71.1\n",
            "  Std: 19.8\n",
            "- problem_solving:\n",
            "  Range: 30-100\n",
            "  Mean: 74.2\n",
            "  Std: 16.0\n",
            "- cultural_fit:\n",
            "  Range: 0-100\n",
            "  Mean: 67.4\n",
            "  Std: 22.2\n",
            "- overall:\n",
            "  Range: 34-100\n",
            "  Mean: 72.9\n",
            "  Std: 12.2\n",
            "Processing dataset for machine learning...\n",
            "Dataset processed with 2858 samples and 6 features.\n",
            "\n",
            "Processed dataset saved with 2858 records\n",
            "\n",
            "Training data sample:\n",
            "   technical  communication  problem_solving  cultural_fit  overall  \\\n",
            "0         67             94               71            20       73   \n",
            "1         71             46               81            61       61   \n",
            "2        100             84               78            67      100   \n",
            "3         92             50               81            66       90   \n",
            "4         98             96              100            98      100   \n",
            "\n",
            "         recommendation  \n",
            "0              Consider  \n",
            "1              Consider  \n",
            "2  Strongly Recommended  \n",
            "3  Strongly Recommended  \n",
            "4  Strongly Recommended  \n",
            "Section 5 complete: Software Engineering dataset preparation finished.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 5b: Data Retrieval"
      ],
      "metadata": {
        "id": "xxG0QGqRiwJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# Section 5b: Data Retrieval\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "from datetime import datetime, timedelta\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Import our hybrid dataset generator\n",
        "try:\n",
        "    from hybrid_dataset_generator import generate_hybrid_dataset\n",
        "    HYBRID_GENERATOR_AVAILABLE = True\n",
        "except ImportError:\n",
        "    HYBRID_GENERATOR_AVAILABLE = False\n",
        "    print(\"Hybrid dataset generator not available, will use standard methods\")\n",
        "\n",
        "def load_hf_dataset():\n",
        "    \"\"\"Load Software Engineering interview dataset from Hugging Face\"\"\"\n",
        "    print(\"Loading Software Engineering interview dataset from Hugging Face...\")\n",
        "\n",
        "    try:\n",
        "        # Load the dataset from Hugging Face\n",
        "        splits = {'train': 'data/train-00000-of-00001.parquet', 'test': 'data/test-00000-of-00001.parquet'}\n",
        "        df = pd.read_parquet(\"hf://datasets/Vineeshsuiii/Software_Engineering_interview_datasets/\" + splits[\"train\"])\n",
        "\n",
        "        # Filter for Software Engineer roles only\n",
        "        if 'profession' in df.columns:\n",
        "            df = df[df['profession'] == 'Software Engineer']\n",
        "            print(f\"Filtered dataset to include only Software Engineer roles: {len(df)} records\")\n",
        "        else:\n",
        "            print(\"Warning: 'profession' column not found in dataset. Using all data.\")\n",
        "\n",
        "        # Ensure we have all required target columns\n",
        "        required_targets = ['technical', 'communication', 'problem_solving', 'cultural_fit', 'overall']\n",
        "        missing_targets = [target for target in required_targets if target not in df.columns]\n",
        "\n",
        "        if missing_targets:\n",
        "            print(f\"Warning: Missing target columns: {missing_targets}\")\n",
        "            print(\"Will generate synthetic values for missing targets\")\n",
        "\n",
        "            # Generate missing targets with some correlation to existing ones\n",
        "            for target in missing_targets:\n",
        "                if 'score' in df.columns:  # Use overall score as base if available\n",
        "                    base = df['score']\n",
        "                elif 'overall' in df.columns and target != 'overall':\n",
        "                    base = df['overall']\n",
        "                else:\n",
        "                    base = np.random.randint(60, 100, size=len(df))\n",
        "\n",
        "                # Add some random variation\n",
        "                df[target] = np.clip(base + np.random.randint(-10, 11, size=len(df)), 0, 100)\n",
        "\n",
        "        # Create any missing features that might be needed for modeling\n",
        "        if 'question_length' not in df.columns and 'question' in df.columns:\n",
        "            df['question_length'] = df['question'].apply(lambda x: len(str(x)) if pd.notna(x) else 0)\n",
        "\n",
        "        if 'word_count' not in df.columns and 'question' in df.columns:\n",
        "            df['word_count'] = df['question'].apply(lambda x: len(str(x).split()) if pd.notna(x) else 0)\n",
        "\n",
        "        print(f\"Dataset loaded successfully with {len(df)} records\")\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading dataset from Hugging Face: {str(e)}\")\n",
        "        print(\"Falling back to synthetic data generation...\")\n",
        "        return None\n",
        "\n",
        "def generate_software_engineer_dataset(num_samples=10000):\n",
        "    \"\"\"Generate a synthetic dataset focused only on Software Engineer roles\"\"\"\n",
        "    print(f\"Generating {num_samples} Software Engineer interview samples...\")\n",
        "\n",
        "    # Define data distributions for Software Engineer questions\n",
        "    question_types = [\"Technical\", \"Behavioral\", \"Problem-Solving\", \"Cultural Fit\", \"Leadership\"]\n",
        "    difficulty_levels = [\"Easy\", \"Medium\", \"Hard\"]\n",
        "\n",
        "    # Software Engineer specific topics\n",
        "    se_topics = [\n",
        "        \"Algorithms\", \"Data Structures\", \"Object-Oriented Programming\",\n",
        "        \"System Design\", \"Database Design\", \"Web Development\",\n",
        "        \"DevOps\", \"Cloud Infrastructure\", \"Testing\", \"Debugging\",\n",
        "        \"Version Control\", \"API Design\", \"Microservices\", \"Security\"\n",
        "    ]\n",
        "\n",
        "    # Ensure a more balanced distribution across categories\n",
        "    # For a 100K dataset, aim for roughly 25K weak, 50K average, 25K strong\n",
        "    weak_proportion = 0.25\n",
        "    average_proportion = 0.5\n",
        "    strong_proportion = 0.25\n",
        "\n",
        "    # Calculate sample counts\n",
        "    weak_count = int(num_samples * weak_proportion)\n",
        "    average_count = int(num_samples * average_proportion)\n",
        "    strong_count = num_samples - weak_count - average_count\n",
        "\n",
        "    # Generate questions and evaluations\n",
        "    questions_data = []\n",
        "    evaluations_data = []\n",
        "\n",
        "    # Track the current counts for each category\n",
        "    current_weak = 0\n",
        "    current_average = 0\n",
        "    current_strong = 0\n",
        "\n",
        "    # Generate short responses for some of the weak examples (minimal/poor answers)\n",
        "    minimal_responses = [\n",
        "        \"I don't know.\",\n",
        "        \"Not sure\",\n",
        "        \"No idea\",\n",
        "        \"idk\",\n",
        "        \"Skip\",\n",
        "        \"I have no experience with that\",\n",
        "        \"Never done that before\",\n",
        "        \"What do you mean?\",\n",
        "        \"Can you explain the question?\",\n",
        "        \"This isn't my area\",\n",
        "        \"I'm not familiar with this\",\n",
        "        \"I'd have to look that up\",\n",
        "        \"I haven't worked on that\",\n",
        "        \"I'll get back to you on that\",\n",
        "        \"That's not in my skill set\",\n",
        "        \"Haven't learned that yet\",\n",
        "        \"I'm confused by your question\",\n",
        "        \"Can we move to another question?\",\n",
        "        \"I'm drawing a blank\",\n",
        "        \"I'd need to ask someone else\",\n",
        "    ]\n",
        "\n",
        "    for i in range(1, num_samples + 1):\n",
        "        # Generate question\n",
        "        question_id = i\n",
        "        topic = random.choice(se_topics)\n",
        "        question_type = random.choice(question_types)\n",
        "        difficulty = random.choice(difficulty_levels)\n",
        "\n",
        "        # Determine which category this sample should be (weak, average, strong)\n",
        "        if current_weak < weak_count:\n",
        "            target_category = \"weak\"\n",
        "            current_weak += 1\n",
        "        elif current_average < average_count:\n",
        "            target_category = \"average\"\n",
        "            current_average += 1\n",
        "        else:\n",
        "            target_category = \"strong\"\n",
        "            current_strong += 1\n",
        "\n",
        "        # Generate questions based on type\n",
        "        if question_type == \"Technical\":\n",
        "            question = f\"Explain how you would implement {topic} in a real-world software project.\"\n",
        "        elif question_type == \"Behavioral\":\n",
        "            question = f\"Describe a time when you had to collaborate with others to solve a {topic.lower()} challenge.\"\n",
        "        elif question_type == \"Problem-Solving\":\n",
        "            question = f\"How would you approach debugging a complex issue related to {topic}?\"\n",
        "        elif question_type == \"Cultural Fit\":\n",
        "            question = f\"How do you stay up-to-date with the latest developments in {topic}?\"\n",
        "        else:  # Leadership\n",
        "            question = f\"Tell me about a time you led a project involving {topic}.\"\n",
        "\n",
        "        # Add to questions data\n",
        "        questions_data.append({\n",
        "            \"id\": question_id,\n",
        "            \"question_text\": question,\n",
        "            \"question_type\": question_type,\n",
        "            \"topic\": topic,\n",
        "            \"difficulty\": difficulty\n",
        "        })\n",
        "\n",
        "        # Generate answer based on target category\n",
        "        if target_category == \"weak\":\n",
        "            # For weak responses, we'll have some minimal responses and some longer but low-quality ones\n",
        "            if random.random() < 0.4:  # 40% chance of minimal response\n",
        "                answer = random.choice(minimal_responses)\n",
        "                answer_length = \"Very Short\"\n",
        "                technical_accuracy = \"Low\"\n",
        "                communication_clarity = \"Low\"\n",
        "                problem_solving = \"Weak\"\n",
        "                cultural_fit = \"Poor\"\n",
        "            else:\n",
        "                answer = f\"I think {topic} is important but I'm not very familiar with it. I would probably need to do some research to understand how to apply it properly.\"\n",
        "                answer_length = \"Short\"\n",
        "                technical_accuracy = random.choices([\"Low\", \"Medium\"], weights=[0.8, 0.2])[0]\n",
        "                communication_clarity = random.choices([\"Low\", \"Medium\"], weights=[0.7, 0.3])[0]\n",
        "                problem_solving = random.choices([\"Weak\", \"Adequate\"], weights=[0.8, 0.2])[0]\n",
        "                cultural_fit = random.choices([\"Poor\", \"Neutral\"], weights=[0.7, 0.3])[0]\n",
        "\n",
        "            # Set scores in the appropriate range for weak responses\n",
        "            technical_score = random.uniform(10, 35)\n",
        "            communication_score = random.uniform(10, 35)\n",
        "            problem_solving_score = random.uniform(10, 35)\n",
        "            cultural_fit_score = random.uniform(10, 35)\n",
        "            overall_score = (technical_score + communication_score + problem_solving_score + cultural_fit_score) / 4\n",
        "\n",
        "        elif target_category == \"average\":\n",
        "            answer = f\"When working with {topic}, I try to follow best practices. I've used it in a few projects before. \" \\\n",
        "                     f\"For example, in one project, we implemented a {topic} solution that helped improve our process.\"\n",
        "\n",
        "            answer_length = random.choices([\"Medium\", \"Long\"], weights=[0.7, 0.3])[0]\n",
        "            technical_accuracy = random.choices([\"Medium\", \"High\"], weights=[0.8, 0.2])[0]\n",
        "            communication_clarity = random.choices([\"Medium\", \"High\"], weights=[0.7, 0.3])[0]\n",
        "            problem_solving = random.choices([\"Adequate\", \"Strong\"], weights=[0.7, 0.3])[0]\n",
        "            cultural_fit = random.choices([\"Neutral\", \"Good\"], weights=[0.6, 0.4])[0]\n",
        "\n",
        "            # Set scores in the appropriate range for average responses\n",
        "            technical_score = random.uniform(40, 70)\n",
        "            communication_score = random.uniform(40, 70)\n",
        "            problem_solving_score = random.uniform(40, 70)\n",
        "            cultural_fit_score = random.uniform(40, 70)\n",
        "            overall_score = (technical_score + communication_score + problem_solving_score + cultural_fit_score) / 4\n",
        "\n",
        "        else:  # strong\n",
        "            answer = f\"I have extensive experience with {topic}. In my previous role, I led the implementation of a \" \\\n",
        "                     f\"{topic} solution that resulted in a 30% improvement in performance. \" \\\n",
        "                     f\"I follow a systematic approach where I first analyze requirements, then design a solution \" \\\n",
        "                     f\"considering scalability and maintainability. I've also mentored junior engineers on best practices for {topic}.\"\n",
        "\n",
        "            answer_length = \"Long\"\n",
        "            technical_accuracy = \"High\"\n",
        "            communication_clarity = random.choices([\"Medium\", \"High\"], weights=[0.2, 0.8])[0]\n",
        "            problem_solving = \"Strong\"\n",
        "            cultural_fit = random.choices([\"Neutral\", \"Good\"], weights=[0.1, 0.9])[0]\n",
        "\n",
        "            # Set scores in the appropriate range for strong responses\n",
        "            technical_score = random.uniform(75, 100)\n",
        "            communication_score = random.uniform(75, 100)\n",
        "            problem_solving_score = random.uniform(75, 100)\n",
        "            cultural_fit_score = random.uniform(75, 100)\n",
        "            overall_score = (technical_score + communication_score + problem_solving_score + cultural_fit_score) / 4\n",
        "\n",
        "        # Add to evaluations data\n",
        "        evaluations_data.append({\n",
        "            \"id\": question_id,\n",
        "            \"answer_text\": answer,\n",
        "            \"technical_score\": technical_score,\n",
        "            \"communication_score\": communication_score,\n",
        "            \"problem_solving_score\": problem_solving_score,\n",
        "            \"cultural_fit_score\": cultural_fit_score,\n",
        "            \"overall_score\": overall_score,\n",
        "            \"answer_length\": answer_length,\n",
        "            \"technical_accuracy\": technical_accuracy,\n",
        "            \"communication_clarity\": communication_clarity,\n",
        "            \"problem_solving\": problem_solving,\n",
        "            \"cultural_fit\": cultural_fit,\n",
        "            \"category\": target_category.capitalize(),  # \"Weak\", \"Average\", or \"Strong\"\n",
        "            \"evaluation_date\": datetime.now().strftime(\"%Y-%m-%d\")  # Add today's date\n",
        "        })\n",
        "\n",
        "        # Show progress\n",
        "        if i % 2000 == 0 or i == num_samples:\n",
        "            print(f\"Generated {i}/{num_samples} questions and {len(evaluations_data)} evaluations\")\n",
        "\n",
        "    # Create DataFrames\n",
        "    questions_df = pd.DataFrame(questions_data)\n",
        "    evaluations_df = pd.DataFrame(evaluations_data)\n",
        "\n",
        "    # Merge the datasets\n",
        "    merged_df = pd.merge(questions_df, evaluations_df, on=\"id\")\n",
        "\n",
        "    # Save to CSV\n",
        "    print(\"Saving CSV files...\")\n",
        "    merged_df.to_csv(\"se_interview_questions.csv\", index=False)\n",
        "\n",
        "    print(f\"Dataset generation complete!\")\n",
        "    print(f\"- Questions dataset: {len(questions_df)} records\")\n",
        "    print(f\"- Evaluations dataset: {len(evaluations_df)} records\")\n",
        "\n",
        "    # Now create a merged dataset for ML\n",
        "    print(\"Creating merged ML dataset...\")\n",
        "    merged_df = pd.merge(evaluations_df, questions_df, on='id', how='inner')\n",
        "\n",
        "    # Create dummy variables for categorical features\n",
        "    question_type_dummies = pd.get_dummies(merged_df['question_type'], prefix='question_type')\n",
        "    difficulty_dummies = pd.get_dummies(merged_df['difficulty'], prefix='difficulty')\n",
        "    topic_dummies = pd.get_dummies(merged_df['topic'], prefix='topic')\n",
        "\n",
        "    # Create date-related features\n",
        "    try:\n",
        "        merged_df['evaluation_date'] = pd.to_datetime(merged_df['evaluation_date'])\n",
        "        merged_df['month'] = merged_df['evaluation_date'].dt.month\n",
        "        merged_df['year'] = merged_df['evaluation_date'].dt.year\n",
        "    except (KeyError, ValueError) as e:\n",
        "        print(f\"Warning: Error processing dates: {e}. Using default values.\")\n",
        "        # Use current date information if there's an issue\n",
        "        current_date = datetime.now()\n",
        "        merged_df['month'] = current_date.month\n",
        "        merged_df['year'] = current_date.year\n",
        "\n",
        "    # Text-based features from questions\n",
        "    merged_df['question_length'] = merged_df['question_text'].apply(len)\n",
        "    merged_df['word_count'] = merged_df['question_text'].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "    # Create the final feature dataset\n",
        "    features = pd.concat([\n",
        "        merged_df[['id', 'question_length', 'word_count', 'month', 'year']],\n",
        "        question_type_dummies,\n",
        "        difficulty_dummies,\n",
        "        topic_dummies\n",
        "    ], axis=1)\n",
        "\n",
        "    # Create ML dataset\n",
        "    ml_dataset = features.copy()\n",
        "    ml_dataset['score'] = merged_df['overall_score']\n",
        "    ml_dataset['overall'] = merged_df['overall_score']\n",
        "    ml_dataset['technical'] = merged_df['technical_score']\n",
        "    ml_dataset['communication'] = merged_df['communication_score']\n",
        "    ml_dataset['problem_solving'] = merged_df['problem_solving_score']\n",
        "    ml_dataset['cultural_fit'] = merged_df['cultural_fit_score']\n",
        "\n",
        "    # Map recommendation categories\n",
        "    def map_to_category(score):\n",
        "        if score >= 75:\n",
        "            return 2  # Strong\n",
        "        elif score >= 40:\n",
        "            return 1  # Average\n",
        "        else:\n",
        "            return 0  # Weak\n",
        "\n",
        "    # Add classifier target that maps directly to the category field (Weak=0, Average=1, Strong=2)\n",
        "    ml_dataset['classifier'] = merged_df['category'].map({'Weak': 0, 'Average': 1, 'Strong': 2})\n",
        "\n",
        "    # Also calculate the classifier based on overall score (use this as a fallback)\n",
        "    ml_dataset['classifier_score_based'] = ml_dataset['overall'].apply(map_to_category)\n",
        "\n",
        "    # Store text fields for reference\n",
        "    ml_dataset['question_text'] = merged_df['question_text']\n",
        "    ml_dataset['question_type'] = merged_df['question_type']\n",
        "    ml_dataset['difficulty_level'] = merged_df['difficulty']\n",
        "    ml_dataset['topic'] = merged_df['topic']\n",
        "\n",
        "    # Add the answer_text column that's required by process_dataset_for_ml\n",
        "    if 'answer_text' not in ml_dataset.columns:\n",
        "        # Create sample answers based on the category\n",
        "        def generate_sample_answer(row):\n",
        "            quality = \"excellent\" if row['classifier'] == 2 else \"average\" if row['classifier'] == 1 else \"poor\"\n",
        "            question = row['question_text']\n",
        "            topic = row['topic']\n",
        "\n",
        "            if quality == \"excellent\":\n",
        "                return f\"This is a comprehensive answer to the {topic} question about '{question[:30]}...'. The answer demonstrates strong technical knowledge and detailed understanding.\"\n",
        "            elif quality == \"average\":\n",
        "                return f\"This is a satisfactory answer to the {topic} question about '{question[:30]}...'. The answer shows basic understanding but lacks some details.\"\n",
        "            else:\n",
        "                return f\"This is a basic answer to the {topic} question about '{question[:30]}...'. The answer has several gaps in understanding.\"\n",
        "\n",
        "        ml_dataset['answer_text'] = ml_dataset.apply(generate_sample_answer, axis=1)\n",
        "        print(\"Generated answer_text column required for processing\")\n",
        "\n",
        "    # Save processed dataset\n",
        "    ml_dataset.to_csv(\"processed_se_interview_data.csv\", index=False)\n",
        "    print(f\"Processed dataset saved with {len(ml_dataset)} records\")\n",
        "\n",
        "    return ml_dataset\n",
        "\n",
        "def get_interview_dataset(num_samples=10000, use_huggingface=True, evaluation_method='hybrid'):\n",
        "    \"\"\"Main function to get software engineering interview dataset\n",
        "\n",
        "    Args:\n",
        "        num_samples: Number of samples to generate if needed\n",
        "        use_huggingface: Whether to try using Hugging Face data\n",
        "        evaluation_method: 'direct' to use scores from HF, 'hybrid' to generate scores from answer quality\n",
        "\n",
        "    Returns:\n",
        "        Processed dataset ready for ML training\n",
        "    \"\"\"\n",
        "\n",
        "    # Try hybrid approach with answer evaluation\n",
        "    if use_huggingface and evaluation_method == 'hybrid' and HYBRID_GENERATOR_AVAILABLE:\n",
        "        print(\"Using hybrid approach with Hugging Face Q&A and generated evaluations...\")\n",
        "        hybrid_data = generate_hybrid_dataset(num_samples)\n",
        "        if hybrid_data is not None:\n",
        "            return hybrid_data\n",
        "\n",
        "    # Direct Hugging Face approach\n",
        "    if use_huggingface and evaluation_method == 'direct':\n",
        "        hf_data = load_hf_dataset()\n",
        "        if hf_data is not None:\n",
        "            return hf_data\n",
        "\n",
        "    # Fall back to synthetic data generation\n",
        "    print(f\"Generating synthetic interview dataset with {num_samples} samples...\")\n",
        "    raw_data = generate_software_engineer_dataset(num_samples)\n",
        "\n",
        "    # Process the raw data into ML-ready format\n",
        "    processed_data = process_dataset_for_ml(raw_data)\n",
        "\n",
        "    if processed_data is not None:\n",
        "        print(f\"Successfully processed {len(processed_data)} records\")\n",
        "        return processed_data\n",
        "    else:\n",
        "        print(\"Warning: Data processing failed, returning raw data\")\n",
        "        # Check if raw_data already has essential ML columns\n",
        "        if all(col in raw_data.columns for col in ['classifier', 'technical', 'communication', 'problem_solving', 'overall']):\n",
        "            print(\"Raw data already contains essential ML columns, using as is\")\n",
        "        else:\n",
        "            # Add minimal processing to ensure raw_data is usable\n",
        "            print(\"Adding minimal processing to raw data\")\n",
        "            if 'classifier' not in raw_data.columns and 'overall' in raw_data.columns:\n",
        "                raw_data['classifier'] = raw_data['overall'].apply(map_to_category)\n",
        "        return raw_data\n",
        "\n",
        "def load_sentiment_dataset():\n",
        "    \"\"\"Load multiclass sentiment analysis dataset from Hugging Face\"\"\"\n",
        "    print(\"Loading sentiment analysis dataset from Hugging Face...\")\n",
        "\n",
        "    try:\n",
        "        # Load the sentiment analysis dataset from Hugging Face\n",
        "        sentiment_dataset = load_dataset(\"Sp1786/multiclass-sentiment-analysis-dataset\")\n",
        "\n",
        "        # Convert to DataFrame for easier processing\n",
        "        df = pd.DataFrame({\n",
        "            'text': sentiment_dataset['train']['text'],\n",
        "            'sentiment_label': sentiment_dataset['train']['label'],\n",
        "            'sentiment': sentiment_dataset['train']['sentiment']\n",
        "        })\n",
        "\n",
        "        print(f\"Loaded {len(df)} records from sentiment analysis dataset\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading sentiment dataset: {e}\")\n",
        "        return None\n",
        "\n",
        "def process_dataset_for_ml(data):\n",
        "    \"\"\"\n",
        "    Process raw data into a format suitable for ML modeling.\n",
        "    This extracts features from interview responses and prepares the data for training.\n",
        "\n",
        "    Args:\n",
        "        data: Raw dataset with interview questions and responses\n",
        "\n",
        "    Returns:\n",
        "        Processed dataset with features and targets\n",
        "    \"\"\"\n",
        "    if 'answer_text' not in data.columns:\n",
        "        print(\"Warning: answer_text column not found.\")\n",
        "\n",
        "        # Try to create an answer_text column from existing data\n",
        "        if 'response' in data.columns:\n",
        "            print(\"Using 'response' column as answer_text\")\n",
        "            data['answer_text'] = data['response']\n",
        "        elif 'answer' in data.columns:\n",
        "            print(\"Using 'answer' column as answer_text\")\n",
        "            data['answer_text'] = data['answer']\n",
        "        elif 'question_text' in data.columns and 'classifier' in data.columns:\n",
        "            print(\"Generating sample answer_text from question_text and classifier\")\n",
        "            # Generate simple dummy answers based on the classification\n",
        "            def generate_fallback_answer(row):\n",
        "                quality = \"excellent\" if row['classifier'] == 2 else \"average\" if row['classifier'] == 1 else \"poor\"\n",
        "                question = row['question_text'] if isinstance(row['question_text'], str) else \"the question\"\n",
        "                question_prefix = question[:30] if len(question) > 30 else question\n",
        "\n",
        "                if quality == \"excellent\":\n",
        "                    return f\"This is a detailed answer to {question_prefix}... demonstrating excellent knowledge.\"\n",
        "                elif quality == \"average\":\n",
        "                    return f\"This is an adequate answer to {question_prefix}... with some correct information.\"\n",
        "                else:\n",
        "                    return f\"This is a minimal answer to {question_prefix}... with several inaccuracies.\"\n",
        "\n",
        "            data['answer_text'] = data.apply(generate_fallback_answer, axis=1)\n",
        "        else:\n",
        "            print(\"Cannot generate answer_text. Cannot process this dataset.\")\n",
        "            return None\n",
        "\n",
        "    print(\"Processing dataset for ML...\")\n",
        "\n",
        "    try:\n",
        "        # Try to import our feature extraction function\n",
        "        import sys\n",
        "        from pathlib import Path\n",
        "        import os\n",
        "\n",
        "        # Get current directory in a way that works in both scripts and notebooks\n",
        "        try:\n",
        "            # For regular Python scripts\n",
        "            current_dir = Path(__file__).parent\n",
        "        except NameError:\n",
        "            # For Jupyter/Colab notebooks\n",
        "            import os\n",
        "            current_dir = Path(os.getcwd())\n",
        "\n",
        "        parent_dir = current_dir.parent\n",
        "        if str(parent_dir) not in sys.path:\n",
        "            sys.path.append(str(parent_dir))\n",
        "\n",
        "        # Import feature extraction\n",
        "        try:\n",
        "            from section_8_interview_process import extract_features_from_text\n",
        "            FEATURE_EXTRACTOR_AVAILABLE = True\n",
        "        except ImportError:\n",
        "            FEATURE_EXTRACTOR_AVAILABLE = False\n",
        "            print(\"Feature extractor not available, will use basic features\")\n",
        "\n",
        "        # Process each row to extract features\n",
        "        processed_rows = []\n",
        "\n",
        "        for idx, row in data.iterrows():\n",
        "            if idx % 1000 == 0:\n",
        "                print(f\"Processing row {idx}/{len(data)}...\")\n",
        "\n",
        "            # Get the answer text\n",
        "            answer = row['answer_text']\n",
        "\n",
        "            # Extract features\n",
        "            if FEATURE_EXTRACTOR_AVAILABLE:\n",
        "                features = extract_features_from_text(answer)\n",
        "            else:\n",
        "                # Basic feature extraction\n",
        "                word_count = len(answer.split())\n",
        "                char_count = len(answer)\n",
        "\n",
        "                # Detect minimal responses\n",
        "                minimal_response = word_count <= 5\n",
        "                minimal_response_penalty = 0.2 if minimal_response else 1.0\n",
        "\n",
        "                # Calculate basic metrics\n",
        "                avg_word_length = char_count / max(word_count, 1)\n",
        "                sentence_count = answer.count('.') + answer.count('!') + answer.count('?')\n",
        "\n",
        "                # Simple word-based sentiment approximation (very basic)\n",
        "                positive_words = [\"good\", \"great\", \"excellent\", \"amazing\", \"awesome\", \"yes\"]\n",
        "                negative_words = [\"bad\", \"poor\", \"no\", \"don't\", \"cant\", \"idk\", \"not\"]\n",
        "\n",
        "                positive_count = sum(1 for word in positive_words if word in answer.lower())\n",
        "                negative_count = sum(1 for word in negative_words if word in answer.lower())\n",
        "                sentiment = 0.3 if minimal_response else (0.5 + (positive_count - negative_count) * 0.1)\n",
        "\n",
        "                # Create feature dictionary\n",
        "                features = {\n",
        "                    'feature_0': float(word_count * minimal_response_penalty),  # Word count\n",
        "                    'feature_1': float(char_count * minimal_response_penalty),  # Character count\n",
        "                    'feature_2': float(avg_word_length),                        # Avg word length\n",
        "                    'feature_3': float(sentence_count * minimal_response_penalty),  # Sentence count\n",
        "                    'feature_4': float(0 if minimal_response else 2),           # Technical keywords\n",
        "                    'feature_5': float(0 if minimal_response else 2),           # Communication keywords\n",
        "                    'feature_6': float(0 if minimal_response else 2),           # Problem-solving keywords\n",
        "                    'feature_7': float(0 if minimal_response else 2),           # Cultural fit keywords\n",
        "                    'feature_8': float(30 if minimal_response else 60),         # Confidence score\n",
        "                    'feature_9': float(0.1 if minimal_response else 0.5),       # Keyword ratio\n",
        "                    'feature_10': float(sentiment)                              # Sentiment\n",
        "                }\n",
        "\n",
        "            # Add targets\n",
        "            if 'technical_score' in row:\n",
        "                features['technical'] = row['technical_score']\n",
        "            if 'communication_score' in row:\n",
        "                features['communication'] = row['communication_score']\n",
        "            if 'problem_solving_score' in row:\n",
        "                features['problem_solving'] = row['problem_solving_score']\n",
        "            if 'cultural_fit_score' in row:\n",
        "                features['cultural_fit'] = row['cultural_fit_score']\n",
        "            if 'overall_score' in row:\n",
        "                features['overall'] = row['overall_score']\n",
        "            if 'category' in row:\n",
        "                category_map = {'Weak': 0, 'Average': 1, 'Strong': 2}\n",
        "                features['classifier'] = category_map.get(row['category'], 1)  # Default to Average if missing\n",
        "\n",
        "            processed_rows.append(features)\n",
        "\n",
        "        # Convert to DataFrame\n",
        "        processed_df = pd.DataFrame(processed_rows)\n",
        "        print(f\"Processed {len(processed_df)} rows successfully\")\n",
        "\n",
        "        # Ensure we have all the required feature columns\n",
        "        required_features = [f'feature_{i}' for i in range(11)]\n",
        "        for feature in required_features:\n",
        "            if feature not in processed_df.columns:\n",
        "                processed_df[feature] = 0\n",
        "\n",
        "        # Ensure we have all the required target columns\n",
        "        required_targets = ['technical', 'communication', 'problem_solving', 'cultural_fit', 'overall', 'classifier']\n",
        "        for target in required_targets:\n",
        "            if target not in processed_df.columns:\n",
        "                processed_df[target] = 50  # Default value (except for classifier)\n",
        "\n",
        "        if 'classifier' not in processed_df.columns:\n",
        "            processed_df['classifier'] = 1  # Default to Average\n",
        "\n",
        "        return processed_df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing dataset: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "    return None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Generate hybrid dataset with answer evaluations by default\n",
        "    get_interview_dataset(num_samples=100000, use_huggingface=True, evaluation_method='hybrid')"
      ],
      "metadata": {
        "id": "IKMbQ0h5izhf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61d34337-3145-463e-c4ac-75262cdd45fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hybrid dataset generator not available, will use standard methods\n",
            "Generating synthetic interview dataset with 100000 samples...\n",
            "Generating 100000 Software Engineer interview samples...\n",
            "Generated 2000/100000 questions and 2000 evaluations\n",
            "Generated 4000/100000 questions and 4000 evaluations\n",
            "Generated 6000/100000 questions and 6000 evaluations\n",
            "Generated 8000/100000 questions and 8000 evaluations\n",
            "Generated 10000/100000 questions and 10000 evaluations\n",
            "Generated 12000/100000 questions and 12000 evaluations\n",
            "Generated 14000/100000 questions and 14000 evaluations\n",
            "Generated 16000/100000 questions and 16000 evaluations\n",
            "Generated 18000/100000 questions and 18000 evaluations\n",
            "Generated 20000/100000 questions and 20000 evaluations\n",
            "Generated 22000/100000 questions and 22000 evaluations\n",
            "Generated 24000/100000 questions and 24000 evaluations\n",
            "Generated 26000/100000 questions and 26000 evaluations\n",
            "Generated 28000/100000 questions and 28000 evaluations\n",
            "Generated 30000/100000 questions and 30000 evaluations\n",
            "Generated 32000/100000 questions and 32000 evaluations\n",
            "Generated 34000/100000 questions and 34000 evaluations\n",
            "Generated 36000/100000 questions and 36000 evaluations\n",
            "Generated 38000/100000 questions and 38000 evaluations\n",
            "Generated 40000/100000 questions and 40000 evaluations\n",
            "Generated 42000/100000 questions and 42000 evaluations\n",
            "Generated 44000/100000 questions and 44000 evaluations\n",
            "Generated 46000/100000 questions and 46000 evaluations\n",
            "Generated 48000/100000 questions and 48000 evaluations\n",
            "Generated 50000/100000 questions and 50000 evaluations\n",
            "Generated 52000/100000 questions and 52000 evaluations\n",
            "Generated 54000/100000 questions and 54000 evaluations\n",
            "Generated 56000/100000 questions and 56000 evaluations\n",
            "Generated 58000/100000 questions and 58000 evaluations\n",
            "Generated 60000/100000 questions and 60000 evaluations\n",
            "Generated 62000/100000 questions and 62000 evaluations\n",
            "Generated 64000/100000 questions and 64000 evaluations\n",
            "Generated 66000/100000 questions and 66000 evaluations\n",
            "Generated 68000/100000 questions and 68000 evaluations\n",
            "Generated 70000/100000 questions and 70000 evaluations\n",
            "Generated 72000/100000 questions and 72000 evaluations\n",
            "Generated 74000/100000 questions and 74000 evaluations\n",
            "Generated 76000/100000 questions and 76000 evaluations\n",
            "Generated 78000/100000 questions and 78000 evaluations\n",
            "Generated 80000/100000 questions and 80000 evaluations\n",
            "Generated 82000/100000 questions and 82000 evaluations\n",
            "Generated 84000/100000 questions and 84000 evaluations\n",
            "Generated 86000/100000 questions and 86000 evaluations\n",
            "Generated 88000/100000 questions and 88000 evaluations\n",
            "Generated 90000/100000 questions and 90000 evaluations\n",
            "Generated 92000/100000 questions and 92000 evaluations\n",
            "Generated 94000/100000 questions and 94000 evaluations\n",
            "Generated 96000/100000 questions and 96000 evaluations\n",
            "Generated 98000/100000 questions and 98000 evaluations\n",
            "Generated 100000/100000 questions and 100000 evaluations\n",
            "Saving CSV files...\n",
            "Dataset generation complete!\n",
            "- Questions dataset: 100000 records\n",
            "- Evaluations dataset: 100000 records\n",
            "Creating merged ML dataset...\n",
            "Generated answer_text column required for processing\n",
            "Processed dataset saved with 100000 records\n",
            "Processing dataset for ML...\n",
            "Feature extractor not available, will use basic features\n",
            "Processing row 0/100000...\n",
            "Processing row 1000/100000...\n",
            "Processing row 2000/100000...\n",
            "Processing row 3000/100000...\n",
            "Processing row 4000/100000...\n",
            "Processing row 5000/100000...\n",
            "Processing row 6000/100000...\n",
            "Processing row 7000/100000...\n",
            "Processing row 8000/100000...\n",
            "Processing row 9000/100000...\n",
            "Processing row 10000/100000...\n",
            "Processing row 11000/100000...\n",
            "Processing row 12000/100000...\n",
            "Processing row 13000/100000...\n",
            "Processing row 14000/100000...\n",
            "Processing row 15000/100000...\n",
            "Processing row 16000/100000...\n",
            "Processing row 17000/100000...\n",
            "Processing row 18000/100000...\n",
            "Processing row 19000/100000...\n",
            "Processing row 20000/100000...\n",
            "Processing row 21000/100000...\n",
            "Processing row 22000/100000...\n",
            "Processing row 23000/100000...\n",
            "Processing row 24000/100000...\n",
            "Processing row 25000/100000...\n",
            "Processing row 26000/100000...\n",
            "Processing row 27000/100000...\n",
            "Processing row 28000/100000...\n",
            "Processing row 29000/100000...\n",
            "Processing row 30000/100000...\n",
            "Processing row 31000/100000...\n",
            "Processing row 32000/100000...\n",
            "Processing row 33000/100000...\n",
            "Processing row 34000/100000...\n",
            "Processing row 35000/100000...\n",
            "Processing row 36000/100000...\n",
            "Processing row 37000/100000...\n",
            "Processing row 38000/100000...\n",
            "Processing row 39000/100000...\n",
            "Processing row 40000/100000...\n",
            "Processing row 41000/100000...\n",
            "Processing row 42000/100000...\n",
            "Processing row 43000/100000...\n",
            "Processing row 44000/100000...\n",
            "Processing row 45000/100000...\n",
            "Processing row 46000/100000...\n",
            "Processing row 47000/100000...\n",
            "Processing row 48000/100000...\n",
            "Processing row 49000/100000...\n",
            "Processing row 50000/100000...\n",
            "Processing row 51000/100000...\n",
            "Processing row 52000/100000...\n",
            "Processing row 53000/100000...\n",
            "Processing row 54000/100000...\n",
            "Processing row 55000/100000...\n",
            "Processing row 56000/100000...\n",
            "Processing row 57000/100000...\n",
            "Processing row 58000/100000...\n",
            "Processing row 59000/100000...\n",
            "Processing row 60000/100000...\n",
            "Processing row 61000/100000...\n",
            "Processing row 62000/100000...\n",
            "Processing row 63000/100000...\n",
            "Processing row 64000/100000...\n",
            "Processing row 65000/100000...\n",
            "Processing row 66000/100000...\n",
            "Processing row 67000/100000...\n",
            "Processing row 68000/100000...\n",
            "Processing row 69000/100000...\n",
            "Processing row 70000/100000...\n",
            "Processing row 71000/100000...\n",
            "Processing row 72000/100000...\n",
            "Processing row 73000/100000...\n",
            "Processing row 74000/100000...\n",
            "Processing row 75000/100000...\n",
            "Processing row 76000/100000...\n",
            "Processing row 77000/100000...\n",
            "Processing row 78000/100000...\n",
            "Processing row 79000/100000...\n",
            "Processing row 80000/100000...\n",
            "Processing row 81000/100000...\n",
            "Processing row 82000/100000...\n",
            "Processing row 83000/100000...\n",
            "Processing row 84000/100000...\n",
            "Processing row 85000/100000...\n",
            "Processing row 86000/100000...\n",
            "Processing row 87000/100000...\n",
            "Processing row 88000/100000...\n",
            "Processing row 89000/100000...\n",
            "Processing row 90000/100000...\n",
            "Processing row 91000/100000...\n",
            "Processing row 92000/100000...\n",
            "Processing row 93000/100000...\n",
            "Processing row 94000/100000...\n",
            "Processing row 95000/100000...\n",
            "Processing row 96000/100000...\n",
            "Processing row 97000/100000...\n",
            "Processing row 98000/100000...\n",
            "Processing row 99000/100000...\n",
            "Processed 100000 rows successfully\n",
            "Successfully processed 100000 records\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 7: Model Training"
      ],
      "metadata": {
        "id": "sNXSKaXHi86u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# Section 7: Model Training\"\"\"\n",
        "\n",
        "# ==== SECTION 7: TRAINING ML MODELS ====\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Initialize GPU availability flag\n",
        "use_gpu = False\n",
        "GPU_AVAILABLE = False\n",
        "TORCH_GPU_AVAILABLE = False\n",
        "\n",
        "# Check for GPU availability\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    print(f\"TensorFlow version: {tf.__version__}\")\n",
        "    gpus = tf.config.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        print(f\"GPU devices available: {len(gpus)}\")\n",
        "        print(f\"GPU details: {gpus}\")\n",
        "        # Set memory growth to avoid taking all GPU memory\n",
        "        for gpu in gpus:\n",
        "            try:\n",
        "                tf.config.experimental.set_memory_growth(gpu, True)\n",
        "            except Exception as e:\n",
        "                print(f\"Error setting memory growth: {e}\")\n",
        "        GPU_AVAILABLE = True\n",
        "        use_gpu = True  # Set the global use_gpu flag\n",
        "    else:\n",
        "        print(\"No GPU devices detected by TensorFlow\")\n",
        "        GPU_AVAILABLE = False\n",
        "\n",
        "    # Also check if CUDA is available for PyTorch\n",
        "    try:\n",
        "        import torch\n",
        "        TORCH_GPU_AVAILABLE = torch.cuda.is_available()\n",
        "        if TORCH_GPU_AVAILABLE:\n",
        "            print(f\"PyTorch CUDA available: {TORCH_GPU_AVAILABLE}\")\n",
        "            print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
        "            print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")\n",
        "            use_gpu = True  # Set the global use_gpu flag\n",
        "        else:\n",
        "            print(\"PyTorch CUDA not available\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error checking PyTorch CUDA: {e}\")\n",
        "        TORCH_GPU_AVAILABLE = False\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error checking TensorFlow GPU: {e}\")\n",
        "    GPU_AVAILABLE = False\n",
        "    TORCH_GPU_AVAILABLE = False\n",
        "    use_gpu = False\n",
        "\n",
        "# Try to import Optuna for hyperparameter optimization\n",
        "try:\n",
        "    import optuna\n",
        "    optuna_available = True\n",
        "    print(\"Optuna available for hyperparameter optimization\")\n",
        "except ImportError:\n",
        "    optuna_available = False\n",
        "    print(\"Optuna not available, will use RandomizedSearchCV instead\")\n",
        "\n",
        "# Try to import XGBoost for advanced modeling\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    xgboost_available = True\n",
        "    print(\"XGBoost available for advanced modeling\")\n",
        "except ImportError:\n",
        "    xgboost_available = False\n",
        "    print(\"XGBoost not available, will use RandomForest instead\")\n",
        "\n",
        "# ==== 6.1: TRAINING REGRESSION MODELS ====\n",
        "\n",
        "def train_regression_models(X_train, X_test, y_train, y_test, target_name, use_gpu=False, verbose=True):\n",
        "    \"\"\"\n",
        "    Train and evaluate multiple regression models for a specific target\n",
        "\n",
        "    Args:\n",
        "        X_train: Training features\n",
        "        X_test: Testing features\n",
        "        y_train: Training target values\n",
        "        y_test: Testing target values\n",
        "        target_name: Name of the target variable\n",
        "        use_gpu: Whether to use GPU acceleration\n",
        "        verbose: Whether to print progress info\n",
        "\n",
        "    Returns:\n",
        "        Best performing model and its metrics\n",
        "    \"\"\"\n",
        "    models = {}\n",
        "    results = {}\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\nTraining regression models for {target_name}...\")\n",
        "        print(f\"GPU acceleration: {'Enabled' if use_gpu and (GPU_AVAILABLE or TORCH_GPU_AVAILABLE) else 'Disabled'}\")\n",
        "\n",
        "    # 1. Random Forest Regressor\n",
        "    if verbose:\n",
        "        print(\"1. Training Random Forest Regressor...\")\n",
        "    rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "    rf.fit(X_train, y_train)\n",
        "    rf_pred = rf.predict(X_test)\n",
        "    rf_mse = mean_squared_error(y_test, rf_pred)\n",
        "    rf_mae = mean_absolute_error(y_test, rf_pred)\n",
        "    rf_r2 = r2_score(y_test, rf_pred)\n",
        "\n",
        "    models[\"random_forest\"] = rf\n",
        "    results[\"random_forest\"] = {\n",
        "        \"mse\": rf_mse,\n",
        "        \"mae\": rf_mae,\n",
        "        \"r2\": rf_r2\n",
        "    }\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"   MSE: {rf_mse:.4f}, MAE: {rf_mae:.4f}, R²: {rf_r2:.4f}\")\n",
        "\n",
        "    # 2. Gradient Boosting Regressor\n",
        "    if verbose:\n",
        "        print(\"2. Training Gradient Boosting Regressor...\")\n",
        "    gb = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "    gb.fit(X_train, y_train)\n",
        "    gb_pred = gb.predict(X_test)\n",
        "    gb_mse = mean_squared_error(y_test, gb_pred)\n",
        "    gb_mae = mean_absolute_error(y_test, gb_pred)\n",
        "    gb_r2 = r2_score(y_test, gb_pred)\n",
        "\n",
        "    models[\"gradient_boosting\"] = gb\n",
        "    results[\"gradient_boosting\"] = {\n",
        "        \"mse\": gb_mse,\n",
        "        \"mae\": gb_mae,\n",
        "        \"r2\": gb_r2\n",
        "    }\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"   MSE: {gb_mse:.4f}, MAE: {gb_mae:.4f}, R²: {gb_r2:.4f}\")\n",
        "\n",
        "    # 3. XGBoost Regressor (if available)\n",
        "    if xgboost_available:\n",
        "        if verbose:\n",
        "            print(\"3. Training XGBoost Regressor...\")\n",
        "\n",
        "        # Set up XGBoost parameters with GPU support if available\n",
        "        if use_gpu and (GPU_AVAILABLE or TORCH_GPU_AVAILABLE):\n",
        "            params = {\n",
        "                'objective': 'reg:squarederror',\n",
        "                'eval_metric': 'rmse',\n",
        "                'tree_method': 'gpu_hist',  # Use GPU acceleration\n",
        "                'gpu_id': 0,\n",
        "                'predictor': 'gpu_predictor',\n",
        "                'learning_rate': 0.1,\n",
        "                'max_depth': 6,\n",
        "                'min_child_weight': 1,\n",
        "                'n_estimators': 100,\n",
        "                'early_stopping_rounds': 10,\n",
        "                'verbosity': 1 if verbose else 0\n",
        "            }\n",
        "            if verbose:\n",
        "                print(\"   Using GPU acceleration for XGBoost\")\n",
        "        else:\n",
        "            params = {\n",
        "                'objective': 'reg:squarederror',\n",
        "                'eval_metric': 'rmse',\n",
        "                'tree_method': 'hist',  # CPU-based histogram algorithm\n",
        "                'learning_rate': 0.1,\n",
        "                'max_depth': 6,\n",
        "                'min_child_weight': 1,\n",
        "                'n_estimators': 100,\n",
        "                'early_stopping_rounds': 10,\n",
        "                'verbosity': 1 if verbose else 0\n",
        "            }\n",
        "\n",
        "        # Create DMatrix for efficient training\n",
        "        dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "        dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "        # Train model\n",
        "        xgb_model = xgb.train(\n",
        "            params,\n",
        "            dtrain,\n",
        "            num_boost_round=100,\n",
        "            evals=[(dtest, 'test')],\n",
        "            early_stopping_rounds=10,\n",
        "            verbose_eval=verbose\n",
        "        )\n",
        "\n",
        "        # Make predictions\n",
        "        xgb_pred = xgb_model.predict(dtest)\n",
        "        xgb_mse = mean_squared_error(y_test, xgb_pred)\n",
        "        xgb_mae = mean_absolute_error(y_test, xgb_pred)\n",
        "        xgb_r2 = r2_score(y_test, xgb_pred)\n",
        "\n",
        "        models[\"xgboost\"] = xgb_model\n",
        "        results[\"xgboost\"] = {\n",
        "            \"mse\": xgb_mse,\n",
        "            \"mae\": xgb_mae,\n",
        "            \"r2\": xgb_r2\n",
        "        }\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"   MSE: {xgb_mse:.4f}, MAE: {xgb_mae:.4f}, R²: {xgb_r2:.4f}\")\n",
        "\n",
        "    # Find the best model based on MSE\n",
        "    best_model_name = min(results, key=lambda x: results[x]['mse'])\n",
        "    best_model = models[best_model_name]\n",
        "    best_metrics = results[best_model_name]\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\nBest model for {target_name}: {best_model_name}\")\n",
        "        print(f\"  MSE: {best_metrics['mse']:.4f}\")\n",
        "        print(f\"  MAE: {best_metrics['mae']:.4f}\")\n",
        "        print(f\"  R²: {best_metrics['r2']:.4f}\")\n",
        "\n",
        "    return best_model, best_metrics\n",
        "\n",
        "def optimize_hyperparameters(X_train, y_train, model_type='xgboost', use_gpu=False,\n",
        "                            is_classification=False, n_trials=50, verbose=True):\n",
        "    \"\"\"\n",
        "    Optimize hyperparameters for a given model type\n",
        "\n",
        "    Args:\n",
        "        X_train: Training features\n",
        "        y_train: Training target values\n",
        "        model_type: Type of model to optimize ('xgboost', 'random_forest', etc.)\n",
        "        use_gpu: Whether to use GPU acceleration\n",
        "        is_classification: Whether this is a classification task\n",
        "        n_trials: Number of optimization trials\n",
        "        verbose: Whether to print progress\n",
        "\n",
        "    Returns:\n",
        "        Optimized model\n",
        "    \"\"\"\n",
        "    if optuna_available:\n",
        "        if verbose:\n",
        "            print(f\"Optimizing {model_type} hyperparameters with Optuna...\")\n",
        "\n",
        "        def objective(trial):\n",
        "            if model_type == 'xgboost' and xgboost_available:\n",
        "                params = {\n",
        "                    'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
        "                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
        "                    'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "                    'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
        "                    'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
        "                    'random_state': 42\n",
        "                }\n",
        "\n",
        "                if use_gpu:\n",
        "                    params['tree_method'] = 'gpu_hist'\n",
        "                    params['gpu_id'] = 0\n",
        "\n",
        "                if is_classification:\n",
        "                    model = xgb.XGBClassifier(**params)\n",
        "                else:\n",
        "                    model = xgb.XGBRegressor(**params)\n",
        "\n",
        "            elif model_type == 'random_forest':\n",
        "                params = {\n",
        "                    'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
        "                    'max_depth': trial.suggest_int('max_depth', 5, 20),\n",
        "                    'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
        "                    'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 5),\n",
        "                    'random_state': 42\n",
        "                }\n",
        "\n",
        "                if is_classification:\n",
        "                    model = RandomForestClassifier(**params)\n",
        "                else:\n",
        "                    model = RandomForestRegressor(**params)\n",
        "\n",
        "            else:\n",
        "                # Default to RandomForest if model_type is not supported\n",
        "                params = {\n",
        "                    'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
        "                    'max_depth': trial.suggest_int('max_depth', 5, 20),\n",
        "                    'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
        "                    'random_state': 42\n",
        "                }\n",
        "\n",
        "                if is_classification:\n",
        "                    model = RandomForestClassifier(**params)\n",
        "                else:\n",
        "                    model = RandomForestRegressor(**params)\n",
        "\n",
        "            # Train and evaluate with cross-validation\n",
        "            from sklearn.model_selection import cross_val_score\n",
        "\n",
        "            if is_classification:\n",
        "                score = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy').mean()\n",
        "            else:\n",
        "                # Use negative MSE for regression (Optuna minimizes the objective)\n",
        "                score = -cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error').mean()\n",
        "\n",
        "            return score\n",
        "\n",
        "        # Create and run the study\n",
        "        if is_classification:\n",
        "            direction = 'maximize'  # Maximize accuracy for classification\n",
        "        else:\n",
        "            direction = 'minimize'  # Minimize MSE for regression\n",
        "\n",
        "        study = optuna.create_study(direction=direction)\n",
        "        study.optimize(objective, n_trials=n_trials)\n",
        "\n",
        "        # Get the best parameters\n",
        "        best_params = study.best_params\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Best parameters: {best_params}\")\n",
        "\n",
        "        # Create and return the best model\n",
        "        if model_type == 'xgboost' and xgboost_available:\n",
        "            if use_gpu:\n",
        "                best_params['tree_method'] = 'gpu_hist'\n",
        "                best_params['gpu_id'] = 0\n",
        "\n",
        "            if is_classification:\n",
        "                best_model = xgb.XGBClassifier(**best_params)\n",
        "            else:\n",
        "                best_model = xgb.XGBRegressor(**best_params)\n",
        "\n",
        "        elif model_type == 'random_forest':\n",
        "            if is_classification:\n",
        "                best_model = RandomForestClassifier(**best_params)\n",
        "            else:\n",
        "                best_model = RandomForestRegressor(**best_params)\n",
        "\n",
        "        else:\n",
        "            if is_classification:\n",
        "                best_model = RandomForestClassifier(**best_params)\n",
        "            else:\n",
        "                best_model = RandomForestRegressor(**best_params)\n",
        "\n",
        "        # Train the final model\n",
        "        best_model.fit(X_train, y_train)\n",
        "        return best_model\n",
        "\n",
        "    else:\n",
        "        # Use RandomizedSearchCV if Optuna is not available\n",
        "        if verbose:\n",
        "            print(f\"Optimizing {model_type} hyperparameters with RandomizedSearchCV...\")\n",
        "\n",
        "        if model_type == 'xgboost' and xgboost_available:\n",
        "            param_grid = {\n",
        "                'n_estimators': [50, 100, 200, 300],\n",
        "                'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "                'max_depth': [3, 5, 7, 9],\n",
        "                'subsample': [0.6, 0.8, 1.0],\n",
        "                'colsample_bytree': [0.6, 0.8, 1.0]\n",
        "            }\n",
        "\n",
        "            if is_classification:\n",
        "                model = xgb.XGBClassifier(random_state=42)\n",
        "            else:\n",
        "                model = xgb.XGBRegressor(random_state=42)\n",
        "\n",
        "            if use_gpu:\n",
        "                model.set_params(tree_method='gpu_hist', gpu_id=0)\n",
        "\n",
        "        elif model_type == 'random_forest':\n",
        "            param_grid = {\n",
        "                'n_estimators': [50, 100, 200, 300],\n",
        "                'max_depth': [5, 10, 15, 20],\n",
        "                'min_samples_split': [2, 5, 10],\n",
        "                'min_samples_leaf': [1, 2, 4]\n",
        "            }\n",
        "\n",
        "            if is_classification:\n",
        "                model = RandomForestClassifier(random_state=42)\n",
        "            else:\n",
        "                model = RandomForestRegressor(random_state=42)\n",
        "\n",
        "        else:\n",
        "            # Default to RandomForest\n",
        "            param_grid = {\n",
        "                'n_estimators': [50, 100, 200, 300],\n",
        "                'max_depth': [5, 10, 15, 20],\n",
        "                'min_samples_split': [2, 5, 10]\n",
        "            }\n",
        "\n",
        "            if is_classification:\n",
        "                model = RandomForestClassifier(random_state=42)\n",
        "            else:\n",
        "                model = RandomForestRegressor(random_state=42)\n",
        "\n",
        "        # Create and run the search\n",
        "        if is_classification:\n",
        "            scoring = 'accuracy'\n",
        "        else:\n",
        "            scoring = 'neg_mean_squared_error'\n",
        "\n",
        "        search = RandomizedSearchCV(\n",
        "            model, param_grid, n_iter=10, cv=5,\n",
        "            scoring=scoring, random_state=42, n_jobs=-1\n",
        "        )\n",
        "\n",
        "        search.fit(X_train, y_train)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Best parameters: {search.best_params_}\")\n",
        "\n",
        "        return search.best_estimator_\n",
        "\n",
        "# ==== 6.2: TRAINING CLASSIFICATION MODELS ====\n",
        "\n",
        "def train_classifier(X_train, X_test, y_train, y_test, use_gpu=False, verbose=True):\n",
        "    \"\"\"\n",
        "    Train and evaluate a classifier for the Strong/Average/Weak categories\n",
        "\n",
        "    Args:\n",
        "        X_train: Training features\n",
        "        X_test: Testing features\n",
        "        y_train: Training target values (0: Weak, 1: Average, 2: Strong)\n",
        "        y_test: Testing target values\n",
        "        use_gpu: Whether to use GPU acceleration\n",
        "        verbose: Whether to print progress\n",
        "\n",
        "    Returns:\n",
        "        Best performing classifier model\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(\"\\nTraining classifier for Strong/Average/Weak categories...\")\n",
        "        print(f\"GPU acceleration: {'Enabled' if use_gpu and (GPU_AVAILABLE or TORCH_GPU_AVAILABLE) else 'Disabled'}\")\n",
        "\n",
        "    models = {}\n",
        "    results = {}\n",
        "\n",
        "    # 1. Random Forest Classifier\n",
        "    if verbose:\n",
        "        print(\"1. Training Random Forest Classifier...\")\n",
        "    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "    rf_classifier.fit(X_train, y_train)\n",
        "    rf_preds = rf_classifier.predict(X_test)\n",
        "    rf_acc = accuracy_score(y_test, rf_preds)\n",
        "\n",
        "    models[\"random_forest\"] = rf_classifier\n",
        "    results[\"random_forest\"] = {\n",
        "        \"accuracy\": rf_acc\n",
        "    }\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"   Accuracy: {rf_acc:.4f}\")\n",
        "\n",
        "    # 2. XGBoost Classifier (if available)\n",
        "    if xgboost_available:\n",
        "        if verbose:\n",
        "            print(\"2. Training XGBoost Classifier...\")\n",
        "\n",
        "        # Set up XGBoost parameters with GPU support if available\n",
        "        if use_gpu and (GPU_AVAILABLE or TORCH_GPU_AVAILABLE):\n",
        "            params = {\n",
        "                'objective': 'multi:softmax',\n",
        "                'eval_metric': 'mlogloss',\n",
        "                'num_class': 3,  # 0: Weak, 1: Average, 2: Strong\n",
        "                'tree_method': 'gpu_hist',  # Use GPU acceleration\n",
        "                'gpu_id': 0,\n",
        "                'predictor': 'gpu_predictor',\n",
        "                'learning_rate': 0.1,\n",
        "                'max_depth': 6,\n",
        "                'min_child_weight': 1,\n",
        "                'verbosity': 1 if verbose else 0\n",
        "            }\n",
        "            if verbose:\n",
        "                print(\"   Using GPU acceleration for XGBoost\")\n",
        "        else:\n",
        "            params = {\n",
        "                'objective': 'multi:softmax',\n",
        "                'eval_metric': 'mlogloss',\n",
        "                'num_class': 3,  # 0: Weak, 1: Average, 2: Strong\n",
        "                'tree_method': 'hist',  # CPU-based histogram algorithm\n",
        "                'learning_rate': 0.1,\n",
        "                'max_depth': 6,\n",
        "                'min_child_weight': 1,\n",
        "                'verbosity': 1 if verbose else 0\n",
        "            }\n",
        "\n",
        "        # Create DMatrix for efficient training\n",
        "        dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "        dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "        # Train model\n",
        "        xgb_model = xgb.train(\n",
        "            params,\n",
        "            dtrain,\n",
        "            num_boost_round=100,\n",
        "            evals=[(dtest, 'test')],\n",
        "            early_stopping_rounds=10,\n",
        "            verbose_eval=verbose\n",
        "        )\n",
        "\n",
        "        # Make predictions\n",
        "        xgb_preds = xgb_model.predict(dtest)\n",
        "        xgb_preds = xgb_preds.astype(int)  # Convert to integers\n",
        "        xgb_acc = accuracy_score(y_test, xgb_preds)\n",
        "\n",
        "        models[\"xgboost\"] = xgb_model\n",
        "        results[\"xgboost\"] = {\n",
        "            \"accuracy\": xgb_acc\n",
        "        }\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"   Accuracy: {xgb_acc:.4f}\")\n",
        "\n",
        "    # Find the best model based on accuracy\n",
        "    best_model_name = max(results, key=lambda x: results[x]['accuracy'])\n",
        "    best_model = models[best_model_name]\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\nBest classifier: {best_model_name} (Accuracy: {results[best_model_name]['accuracy']:.4f})\")\n",
        "        print(\"\\nClassification Report:\")\n",
        "        if best_model_name == \"xgboost\":\n",
        "            best_preds = best_model.predict(dtest).astype(int)\n",
        "        else:\n",
        "            best_preds = best_model.predict(X_test)\n",
        "        print(classification_report(y_test, best_preds,\n",
        "                                  target_names=['Weak', 'Average', 'Strong']))\n",
        "\n",
        "    return best_model\n",
        "\n",
        "def generate_minimal_sample_data(num_samples=100000):\n",
        "    \"\"\"\n",
        "    Generate a very minimal dataset for emergency fallback purposes.\n",
        "\n",
        "    Args:\n",
        "        num_samples: Number of samples to generate\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with basic feature and target columns\n",
        "    \"\"\"\n",
        "    print(f\"Generating minimal emergency dataset with {num_samples} samples...\")\n",
        "\n",
        "    # Import needed libraries\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "\n",
        "    # Create empty dataframe\n",
        "    data = pd.DataFrame()\n",
        "\n",
        "    # Generate features\n",
        "    for i in range(11):\n",
        "        feature_name = f'feature_{i}'\n",
        "        # Random values calibrated to reasonable ranges\n",
        "        data[feature_name] = np.random.rand(num_samples) * (100 if i in [0, 1, 8] else 10)\n",
        "\n",
        "    # Generate target variables - ensure good distribution\n",
        "    categories = np.random.choice([0, 1, 2], size=num_samples, p=[0.25, 0.5, 0.25])\n",
        "\n",
        "    # Add scores based on categories\n",
        "    data['technical'] = np.where(categories == 0, np.random.uniform(10, 35, num_samples),\n",
        "                       np.where(categories == 1, np.random.uniform(40, 70, num_samples),\n",
        "                                np.random.uniform(75, 95, num_samples)))\n",
        "\n",
        "    data['communication'] = np.where(categories == 0, np.random.uniform(10, 35, num_samples),\n",
        "                           np.where(categories == 1, np.random.uniform(40, 70, num_samples),\n",
        "                                    np.random.uniform(75, 95, num_samples)))\n",
        "\n",
        "    data['problem_solving'] = np.where(categories == 0, np.random.uniform(10, 35, num_samples),\n",
        "                              np.where(categories == 1, np.random.uniform(40, 70, num_samples),\n",
        "                                      np.random.uniform(75, 95, num_samples)))\n",
        "\n",
        "    data['cultural_fit'] = np.where(categories == 0, np.random.uniform(10, 35, num_samples),\n",
        "                           np.where(categories == 1, np.random.uniform(40, 70, num_samples),\n",
        "                                   np.random.uniform(75, 95, num_samples)))\n",
        "\n",
        "    data['overall'] = np.where(categories == 0, np.random.uniform(10, 35, num_samples),\n",
        "                      np.where(categories == 1, np.random.uniform(40, 70, num_samples),\n",
        "                              np.random.uniform(75, 95, num_samples)))\n",
        "\n",
        "    data['classifier'] = categories\n",
        "\n",
        "    print(f\"Minimal emergency dataset generated with {len(data)} rows and {len(data.columns)} columns\")\n",
        "    print(f\"Category distribution: Weak: {sum(categories == 0)}, Average: {sum(categories == 1)}, Strong: {sum(categories == 2)}\")\n",
        "\n",
        "    return data\n",
        "\n",
        "def train_evaluation_models(data=None, retrain=False, verbose=True, use_gpu=True):\n",
        "    \"\"\"\n",
        "    Train and save regression models for interview evaluation.\n",
        "\n",
        "    Args:\n",
        "        data: DataFrame with features and targets\n",
        "        retrain: Whether to retrain models even if they exist\n",
        "        verbose: Whether to print progress information\n",
        "        use_gpu: Whether to use GPU acceleration if available\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with trained models and scaler\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(\"\\n==== INTERVIEW EVALUATION MODEL TRAINING ====\")\n",
        "\n",
        "    # Check if models already exist and we're not forced to retrain\n",
        "    if os.path.exists(\"interview_evaluation_models.pkl\") and not retrain:\n",
        "        if verbose:\n",
        "            print(\"Interview evaluation models already exist.\")\n",
        "            print(\"Use retrain=True to force retraining.\")\n",
        "        try:\n",
        "            model_package = joblib.load(\"interview_evaluation_models.pkl\")\n",
        "            return model_package\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading models: {e}\")\n",
        "            print(\"Will retrain models...\")\n",
        "\n",
        "    # Load data if not provided\n",
        "    if data is None:\n",
        "        if verbose:\n",
        "            print(\"No data provided. Attempting to load interview data...\")\n",
        "        try:\n",
        "            # Try to load existing processed data\n",
        "            data = pd.read_csv(\"interview_data.csv\")\n",
        "            if verbose:\n",
        "                print(f\"Successfully loaded data with {len(data)} records\")\n",
        "        except Exception as e:\n",
        "            if verbose:\n",
        "                print(f\"Error loading interview_data.csv: {e}\")\n",
        "                print(\"Generating sample data for demonstration...\")\n",
        "\n",
        "            try:\n",
        "                # Import the necessary module for data generation\n",
        "                import sys\n",
        "                from pathlib import Path\n",
        "\n",
        "                # Add parent directory to path if needed\n",
        "                current_dir = Path(__file__).parent\n",
        "                parent_dir = current_dir.parent\n",
        "                if str(parent_dir) not in sys.path:\n",
        "                    sys.path.append(str(parent_dir))\n",
        "\n",
        "                # Import the data retrieval module\n",
        "                try:\n",
        "                    from section_5b_data_retrieval import get_interview_dataset\n",
        "\n",
        "                    # Generate a moderate dataset for testing (5K samples)\n",
        "                    num_samples = 50000  # Starting with 5K samples for initial testing\n",
        "                    data = get_interview_dataset(num_samples=num_samples)\n",
        "\n",
        "                    # Save the generated data for future use\n",
        "                    data.to_csv(f\"interview_data_sample_{num_samples}.csv\", index=False)\n",
        "                    if verbose:\n",
        "                        print(f\"Generated and saved sample data with {len(data)} records to interview_data_sample_{num_samples}.csv\")\n",
        "                except ImportError as e:\n",
        "                    print(f\"Error importing data retrieval module: {e}\")\n",
        "                    # Generate minimal sample data\n",
        "                    data = generate_minimal_sample_data()\n",
        "                    data.to_csv(\"interview_data_sample.csv\", index=False)\n",
        "                    if verbose:\n",
        "                        print(f\"Generated and saved minimal sample data with {len(data)} records to interview_data_sample.csv\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error generating data: {e}\")\n",
        "                return None\n",
        "\n",
        "    # Continue with the rest of the function for model training\n",
        "    if verbose:\n",
        "        print(\"Starting ML model training...\")\n",
        "        if use_gpu:\n",
        "            print(\"Using GPU acceleration if available\")\n",
        "\n",
        "    # Check if models already exist and we don't want to retrain\n",
        "    if not retrain and os.path.exists(\"interview_evaluation_models.pkl\"):\n",
        "        try:\n",
        "            models = joblib.load(\"interview_evaluation_models.pkl\")\n",
        "            if verbose:\n",
        "                print(\"Loaded existing models from interview_evaluation_models.pkl\")\n",
        "            return models\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading existing models: {e}\")\n",
        "            print(\"Will train new models instead\")\n",
        "\n",
        "    if data is None or len(data) == 0:\n",
        "        print(\"No data available for training\")\n",
        "        return None\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Training ML models on {len(data)} samples...\")\n",
        "\n",
        "    # Extract features and targets\n",
        "    feature_columns = [col for col in data.columns if col.startswith('feature_')]\n",
        "    target_columns = ['technical', 'communication', 'problem_solving', 'cultural_fit', 'overall']\n",
        "\n",
        "    # Check which target columns are available\n",
        "    available_targets = [col for col in target_columns if col in data.columns]\n",
        "\n",
        "    if len(feature_columns) == 0:\n",
        "        print(\"No feature columns found in data\")\n",
        "        return None\n",
        "\n",
        "    if len(available_targets) == 0:\n",
        "        print(\"No target columns found in data\")\n",
        "        return None\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Using {len(feature_columns)} features for model training\")\n",
        "        print(f\"Using {len(available_targets)} evaluation metrics as targets: {available_targets}\")\n",
        "\n",
        "    # Extract features\n",
        "    X = data[feature_columns].copy()\n",
        "\n",
        "    # Ensure all feature values are numeric\n",
        "    for col in X.columns:\n",
        "        if X[col].dtype == 'object':\n",
        "            try:\n",
        "                X[col] = pd.to_numeric(X[col], errors='coerce')\n",
        "            except:\n",
        "                print(f\"Warning: Could not convert column {col} to numeric\")\n",
        "                # Drop the column if it can't be converted\n",
        "                X = X.drop(columns=[col])\n",
        "\n",
        "    # Fill NaN values with column means\n",
        "    X = X.fillna(X.mean())\n",
        "\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    X_scaled = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
        "\n",
        "    # Dictionary to store trained models\n",
        "    trained_models = {}\n",
        "\n",
        "    # Train regression models for each target\n",
        "    for target_name in available_targets:\n",
        "        if verbose:\n",
        "            print(f\"\\n=== Training models for {target_name} ===\")\n",
        "\n",
        "        # Get target values, ensuring they're numeric\n",
        "        y = data[target_name].copy()\n",
        "        if y.dtype == 'object':\n",
        "            try:\n",
        "                y = pd.to_numeric(y, errors='coerce')\n",
        "            except:\n",
        "                print(f\"Warning: Could not convert target {target_name} to numeric\")\n",
        "                continue\n",
        "\n",
        "        # Drop rows with missing target values\n",
        "        valid_mask = ~y.isna()\n",
        "        X_target = X_scaled[valid_mask].copy()\n",
        "        y_target = y[valid_mask].copy()\n",
        "\n",
        "        if len(y_target) == 0:\n",
        "            print(f\"No valid data for target {target_name}\")\n",
        "            continue\n",
        "\n",
        "        # Split data for training and testing\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X_target, y_target, test_size=0.2, random_state=42\n",
        "        )\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Train set: {X_train.shape[0]} samples, Test set: {X_test.shape[0]} samples\")\n",
        "\n",
        "        # Train regression models\n",
        "        best_model, _ = train_regression_models(\n",
        "            X_train, X_test, y_train, y_test,\n",
        "            target_name=target_name,\n",
        "            use_gpu=use_gpu,\n",
        "            verbose=verbose\n",
        "        )\n",
        "\n",
        "        # Store the best model\n",
        "        trained_models[target_name] = best_model\n",
        "\n",
        "    # Train classifier for Strong/Average/Weak if 'category' column exists\n",
        "    if 'category' in data.columns:\n",
        "        if verbose:\n",
        "            print(\"\\n=== Training classifier for Strong/Average/Weak categories ===\")\n",
        "\n",
        "        # Map categories to numerical values\n",
        "        category_mapping = {'Strong': 2, 'Average': 1, 'Weak': 0}\n",
        "\n",
        "        # Create a copy to avoid warnings\n",
        "        data_for_class = data.copy()\n",
        "\n",
        "        # Convert categories to numeric values\n",
        "        if data_for_class['category'].dtype == 'object':\n",
        "            # Map categorical values to numbers\n",
        "            data_for_class['category_encoded'] = data_for_class['category'].map(category_mapping)\n",
        "\n",
        "            # Check for invalid categories\n",
        "            if data_for_class['category_encoded'].isna().any():\n",
        "                invalid_cats = data_for_class.loc[data_for_class['category_encoded'].isna(), 'category'].unique()\n",
        "                print(f\"Warning: Found invalid categories: {invalid_cats}\")\n",
        "                print(\"These will be treated as missing values\")\n",
        "                # Fill NaN with a default value\n",
        "                data_for_class = data_for_class.dropna(subset=['category_encoded'])\n",
        "\n",
        "            # Ensure it's an integer\n",
        "            data_for_class['category_encoded'] = data_for_class['category_encoded'].astype(int)\n",
        "\n",
        "        # Use the same feature set as for regression\n",
        "        X_class = X_scaled.loc[data_for_class.index].copy()\n",
        "        y_class = data_for_class['category_encoded'].copy()\n",
        "\n",
        "        # Train-test split\n",
        "        X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(\n",
        "            X_class, y_class, test_size=0.2, random_state=42, stratify=y_class\n",
        "        )\n",
        "\n",
        "        # Train the classifier\n",
        "        classifier_model = train_classifier(\n",
        "            X_train_class, X_test_class, y_train_class, y_test_class,\n",
        "            use_gpu=use_gpu,\n",
        "            verbose=verbose\n",
        "        )\n",
        "\n",
        "        # Store the classifier\n",
        "        trained_models['classifier'] = classifier_model\n",
        "        # Also store the classifier type (for prediction handling)\n",
        "        trained_models['classifier_type'] = 'xgboost' if isinstance(classifier_model, xgb.Booster) else 'sklearn'\n",
        "\n",
        "    # ==== 6.3: EVALUATING ML MODELS ====\n",
        "\n",
        "    if verbose:\n",
        "        print(\"\\n=== Evaluating models on full dataset ===\")\n",
        "\n",
        "    # Try to run predictions on the full dataset\n",
        "    try:\n",
        "        # Convert the full dataset to DMatrix for XGBoost models\n",
        "        X_dmatrix = xgb.DMatrix(X_scaled)\n",
        "\n",
        "        # Predict using all models\n",
        "        predictions = {}\n",
        "\n",
        "        for target_name, model in trained_models.items():\n",
        "            if target_name == 'classifier_type':\n",
        "                continue  # Skip the type indicator\n",
        "\n",
        "            if target_name == 'classifier':\n",
        "                # Handle classifier predictions based on type\n",
        "                if trained_models['classifier_type'] == 'xgboost':\n",
        "                    # For XGBoost Booster, use DMatrix\n",
        "                    class_preds_num = model.predict(X_dmatrix).astype(int)\n",
        "                    # Map back to labels\n",
        "                    class_mapping = {0: 'Weak', 1: 'Average', 2: 'Strong'}\n",
        "                    class_preds = [class_mapping[pred] for pred in class_preds_num]\n",
        "                else:\n",
        "                    # For sklearn models\n",
        "                    class_preds_num = model.predict(X_scaled)\n",
        "                    # Map back to labels\n",
        "                    class_mapping = {0: 'Weak', 1: 'Average', 2: 'Strong'}\n",
        "                    class_preds = [class_mapping[pred] for pred in class_preds_num]\n",
        "\n",
        "                predictions['category'] = class_preds\n",
        "\n",
        "                if verbose:\n",
        "                    # Show distribution of predictions\n",
        "                    counts = {label: class_preds.count(label) for label in set(class_preds)}\n",
        "                    print(f\"    Classification results: {counts}\")\n",
        "            else:\n",
        "                # Handle regression model predictions\n",
        "                if isinstance(model, xgb.Booster):\n",
        "                    # For XGBoost Booster, use DMatrix\n",
        "                    target_preds = model.predict(X_dmatrix)\n",
        "                else:\n",
        "                    # For sklearn models\n",
        "                    target_preds = model.predict(X_scaled)\n",
        "\n",
        "                predictions[target_name] = target_preds\n",
        "\n",
        "                if verbose:\n",
        "                    # Show summary statistics of predictions\n",
        "                    pred_mean = np.mean(target_preds)\n",
        "                    pred_min = np.min(target_preds)\n",
        "                    pred_max = np.max(target_preds)\n",
        "                    print(f\"    Score range: {pred_min:.2f} to {pred_max:.2f}, average: {pred_mean:.2f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Error during evaluation: {e}\")\n",
        "        print(\"This won't affect the saved models, but you may want to investigate\")\n",
        "\n",
        "    # ==== 6.4: SAVING ML MODELS ====\n",
        "\n",
        "    # Package models with scaler for deployment\n",
        "    model_package = {\n",
        "        'models': trained_models,\n",
        "        'scaler': scaler,\n",
        "        'feature_columns': feature_columns,\n",
        "        'target_columns': available_targets\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # Save the model package\n",
        "        joblib.dump(model_package, \"interview_evaluation_models.pkl\")\n",
        "        if verbose:\n",
        "            print(\"\\nAll models saved successfully to interview_evaluation_models.pkl\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving models: {e}\")\n",
        "\n",
        "    return model_package\n",
        "\n",
        "# Function to make predictions with the trained models\n",
        "def predict_with_models(data, model_package=None, verbose=True):\n",
        "    \"\"\"\n",
        "    Make predictions using trained models\n",
        "\n",
        "    Args:\n",
        "        data: DataFrame with features\n",
        "        model_package: Dictionary with trained models and scaler\n",
        "        verbose: Whether to print progress information\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of predictions\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(\"Starting prediction process...\")\n",
        "\n",
        "    # Load models if not provided\n",
        "    if model_package is None:\n",
        "        if verbose:\n",
        "            print(\"No model package provided, attempting to load from file...\")\n",
        "        try:\n",
        "            model_package = joblib.load(\"interview_evaluation_models.pkl\")\n",
        "            if verbose:\n",
        "                print(\"Successfully loaded models from interview_evaluation_models.pkl\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading models: {e}\")\n",
        "            return None\n",
        "\n",
        "    # Extract components from the package\n",
        "    models = model_package['models']\n",
        "    scaler = model_package['scaler']\n",
        "    feature_columns = model_package['feature_columns']\n",
        "\n",
        "    # Ensure all needed features are present\n",
        "    for col in feature_columns:\n",
        "        if col not in data.columns:\n",
        "            data[col] = 0  # Default value for missing features\n",
        "\n",
        "    # Extract required features in the correct order\n",
        "    X = data[feature_columns].copy()\n",
        "\n",
        "    # Ensure all feature values are numeric\n",
        "    for col in X.columns:\n",
        "        if not np.issubdtype(X[col].dtype, np.number):\n",
        "            X[col] = X[col].astype(float)\n",
        "\n",
        "    # Scale features\n",
        "    X_scaled = scaler.transform(X)\n",
        "\n",
        "    # Import XGBoost if needed for predictions\n",
        "    try:\n",
        "        import xgboost as xgb\n",
        "        xgboost_available = True\n",
        "    except ImportError:\n",
        "        xgboost_available = False\n",
        "\n",
        "    # Create DMatrix for XGBoost models if needed\n",
        "    needs_dmatrix = False\n",
        "    for model_name, model in models.items():\n",
        "        if model_name != 'classifier_type' and isinstance(model, xgb.Booster):\n",
        "            needs_dmatrix = True\n",
        "            break\n",
        "\n",
        "    if needs_dmatrix and xgboost_available:\n",
        "        X_dmatrix = xgb.DMatrix(X_scaled)\n",
        "\n",
        "    # Make predictions with all models\n",
        "    predictions = {}\n",
        "    for target_name, model in models.items():\n",
        "        if target_name == 'classifier_type':\n",
        "            continue  # Skip the type indicator\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Predicting {target_name}...\")\n",
        "\n",
        "        if target_name == 'classifier':\n",
        "            # Check classifier type\n",
        "            if 'classifier_type' in models and models['classifier_type'] == 'xgboost':\n",
        "                # For XGBoost booster\n",
        "                class_preds_num = model.predict(X_dmatrix).astype(int)\n",
        "                # Map back to labels\n",
        "                class_mapping = {0: 'Weak', 1: 'Average', 2: 'Strong'}\n",
        "                class_preds = [class_mapping[pred] for pred in class_preds_num]\n",
        "                predictions['category'] = class_preds\n",
        "            else:\n",
        "                # For sklearn classifiers\n",
        "                class_preds_num = model.predict(X_scaled)\n",
        "                # Map back to labels\n",
        "                class_mapping = {0: 'Weak', 1: 'Average', 2: 'Strong'}\n",
        "                class_preds = [class_mapping[pred] for pred in class_preds_num]\n",
        "                predictions['category'] = class_preds\n",
        "        else:\n",
        "            # Regression predictions\n",
        "            if isinstance(model, xgb.Booster):\n",
        "                # For XGBoost booster\n",
        "                target_preds = model.predict(X_dmatrix)\n",
        "            else:\n",
        "                # For sklearn models\n",
        "                target_preds = model.predict(X_scaled)\n",
        "\n",
        "            predictions[target_name] = target_preds\n",
        "\n",
        "    # Apply additional sentiment-based adjustment\n",
        "    # Penalize inappropriate responses more heavily based on sentiment\n",
        "    if 'feature_10' in data.columns:  # Check if sentiment feature exists\n",
        "        sentiment_score = data['feature_10'].values[0]\n",
        "        minimal_words = data['feature_0'].values[0] < 5  # Check if response is too short\n",
        "\n",
        "        # Detect extremely negative or inappropriate responses\n",
        "        # Lower sentiment with few words indicates potential inappropriate response\n",
        "        if minimal_words and sentiment_score < 0.4:\n",
        "            # Apply stronger penalty to all scores\n",
        "            penalty_factor = 0.3\n",
        "            for key in predictions:\n",
        "                if key != 'category':  # Don't penalize the category\n",
        "                    predictions[key] = np.array([max(20, score * penalty_factor) for score in predictions[key]])\n",
        "\n",
        "    # Ensure all scores are within valid range (0-100)\n",
        "    for key in predictions:\n",
        "        if key != 'category':  # Don't adjust the category\n",
        "            predictions[key] = np.array([max(0, min(100, score)) for score in predictions[key]])\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"ML Model Training Module\")\n",
        "    print(\"Starting direct execution of model training...\")\n",
        "\n",
        "    try:\n",
        "        # Attempt to load data\n",
        "        print(\"\\nAttempting to load interview data...\")\n",
        "        try:\n",
        "            data = pd.read_csv(\"interview_data.csv\")\n",
        "            print(f\"Successfully loaded data with {len(data)} samples\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading interview_data.csv: {e}\")\n",
        "            print(\"Generating sample data for demonstration...\")\n",
        "\n",
        "            # Create sample data if file doesn't exist\n",
        "            np.random.seed(42)\n",
        "            n_samples = 100000\n",
        "            n_features = 10\n",
        "\n",
        "            # Generate features\n",
        "            features = np.random.randn(n_samples, n_features)\n",
        "            feature_cols = [f'feature_{i}' for i in range(n_features)]\n",
        "\n",
        "            # Generate target values\n",
        "            technical = 0.7 * features[:, 0] + 0.3 * features[:, 1] + np.random.randn(n_samples) * 0.2\n",
        "            communication = 0.5 * features[:, 2] + 0.5 * features[:, 3] + np.random.randn(n_samples) * 0.2\n",
        "            problem_solving = 0.6 * features[:, 4] + 0.4 * features[:, 5] + np.random.randn(n_samples) * 0.2\n",
        "            cultural_fit = 0.4 * features[:, 6] + 0.6 * features[:, 7] + np.random.randn(n_samples) * 0.2\n",
        "            overall = 0.25 * technical + 0.25 * communication + 0.25 * problem_solving + 0.25 * cultural_fit\n",
        "\n",
        "            # Scale to 0-100 range\n",
        "            def scale_to_range(arr, min_val=0, max_val=100):\n",
        "                return ((arr - arr.min()) / (arr.max() - arr.min())) * (max_val - min_val) + min_val\n",
        "\n",
        "            technical = scale_to_range(technical)\n",
        "            communication = scale_to_range(communication)\n",
        "            problem_solving = scale_to_range(problem_solving)\n",
        "            cultural_fit = scale_to_range(cultural_fit)\n",
        "            overall = scale_to_range(overall)\n",
        "\n",
        "            # Generate categories based on overall score\n",
        "            categories = []\n",
        "            for score in overall:\n",
        "                if score >= 70:\n",
        "                    categories.append('Strong')\n",
        "                elif score >= 40:\n",
        "                    categories.append('Average')\n",
        "                else:\n",
        "                    categories.append('Weak')\n",
        "\n",
        "            # Create DataFrame\n",
        "            data = pd.DataFrame(features, columns=feature_cols)\n",
        "            data['technical'] = technical\n",
        "            data['communication'] = communication\n",
        "            data['problem_solving'] = problem_solving\n",
        "            data['cultural_fit'] = cultural_fit\n",
        "            data['overall'] = overall\n",
        "            data['category'] = categories\n",
        "\n",
        "            # Save sample data for future use\n",
        "            try:\n",
        "                data.to_csv(\"interview_data_sample.csv\", index=False)\n",
        "                print(f\"Generated and saved sample data with {len(data)} records to interview_data_sample.csv\")\n",
        "            except:\n",
        "                print(\"Note: Could not save sample data to file\")\n",
        "\n",
        "        # Train models\n",
        "        print(\"\\nTraining models...\")\n",
        "        # The global use_gpu variable was defined at the top of the file\n",
        "        # It is automatically set based on GPU detection\n",
        "        model_package = train_evaluation_models(\n",
        "            data=data,\n",
        "            retrain=True,\n",
        "            verbose=True,\n",
        "            use_gpu=use_gpu  # Use the global variable defined at the top of the file\n",
        "        )\n",
        "\n",
        "        if model_package:\n",
        "            print(\"\\nModel training completed successfully!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn error occurred during execution: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "    print(\"\\nExecution complete!\")"
      ],
      "metadata": {
        "id": "KM2T04_6i-t-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2eec7fa5-32c3-4848-88cb-ae73d9d8253f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.18.0\n",
            "GPU devices available: 1\n",
            "GPU details: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "PyTorch CUDA available: True\n",
            "CUDA device count: 1\n",
            "CUDA device name: Tesla T4\n",
            "Optuna not available, will use RandomizedSearchCV instead\n",
            "XGBoost available for advanced modeling\n",
            "ML Model Training Module\n",
            "Starting direct execution of model training...\n",
            "\n",
            "Attempting to load interview data...\n",
            "Error loading interview_data.csv: [Errno 2] No such file or directory: 'interview_data.csv'\n",
            "Generating sample data for demonstration...\n",
            "Generated and saved sample data with 100000 records to interview_data_sample.csv\n",
            "\n",
            "Training models...\n",
            "\n",
            "==== INTERVIEW EVALUATION MODEL TRAINING ====\n",
            "Starting ML model training...\n",
            "Using GPU acceleration if available\n",
            "Training ML models on 100000 samples...\n",
            "Using 10 features for model training\n",
            "Using 5 evaluation metrics as targets: ['technical', 'communication', 'problem_solving', 'cultural_fit', 'overall']\n",
            "\n",
            "=== Training models for technical ===\n",
            "Train set: 80000 samples, Test set: 20000 samples\n",
            "\n",
            "Training regression models for technical...\n",
            "GPU acceleration: Enabled\n",
            "1. Training Random Forest Regressor...\n",
            "   MSE: 9.1146, MAE: 2.4084, R²: 0.9310\n",
            "2. Training Gradient Boosting Regressor...\n",
            "   MSE: 8.6578, MAE: 2.3517, R²: 0.9345\n",
            "3. Training XGBoost Regressor...\n",
            "   Using GPU acceleration for XGBoost\n",
            "[0]\ttest-rmse:10.45185\n",
            "[1]\ttest-rmse:9.51947\n",
            "[2]\ttest-rmse:8.68862\n",
            "[3]\ttest-rmse:7.94480\n",
            "[4]\ttest-rmse:7.28563\n",
            "[5]\ttest-rmse:6.70204\n",
            "[6]\ttest-rmse:6.18300\n",
            "[7]\ttest-rmse:5.72619\n",
            "[8]\ttest-rmse:5.32590\n",
            "[9]\ttest-rmse:4.97591\n",
            "[10]\ttest-rmse:4.67157\n",
            "[11]\ttest-rmse:4.40724\n",
            "[12]\ttest-rmse:4.17898\n",
            "[13]\ttest-rmse:3.98189\n",
            "[14]\ttest-rmse:3.81452\n",
            "[15]\ttest-rmse:3.67242\n",
            "[16]\ttest-rmse:3.55218\n",
            "[17]\ttest-rmse:3.45079\n",
            "[18]\ttest-rmse:3.36565\n",
            "[19]\ttest-rmse:3.29520\n",
            "[20]\ttest-rmse:3.23615\n",
            "[21]\ttest-rmse:3.18669\n",
            "[22]\ttest-rmse:3.14574\n",
            "[23]\ttest-rmse:3.11217\n",
            "[24]\ttest-rmse:3.08438\n",
            "[25]\ttest-rmse:3.06154\n",
            "[26]\ttest-rmse:3.04272\n",
            "[27]\ttest-rmse:3.02719\n",
            "[28]\ttest-rmse:3.01475\n",
            "[29]\ttest-rmse:3.00474\n",
            "[30]\ttest-rmse:2.99618\n",
            "[31]\ttest-rmse:2.98955\n",
            "[32]\ttest-rmse:2.98428\n",
            "[33]\ttest-rmse:2.98005\n",
            "[34]\ttest-rmse:2.97621\n",
            "[35]\ttest-rmse:2.97319\n",
            "[36]\ttest-rmse:2.97075\n",
            "[37]\ttest-rmse:2.96884\n",
            "[38]\ttest-rmse:2.96760\n",
            "[39]\ttest-rmse:2.96623\n",
            "[40]\ttest-rmse:2.96503\n",
            "[41]\ttest-rmse:2.96396\n",
            "[42]\ttest-rmse:2.96316\n",
            "[43]\ttest-rmse:2.96268\n",
            "[44]\ttest-rmse:2.96187\n",
            "[45]\ttest-rmse:2.96154\n",
            "[46]\ttest-rmse:2.96136\n",
            "[47]\ttest-rmse:2.96100\n",
            "[48]\ttest-rmse:2.96070\n",
            "[49]\ttest-rmse:2.96076\n",
            "[50]\ttest-rmse:2.96093\n",
            "[51]\ttest-rmse:2.96071\n",
            "[52]\ttest-rmse:2.96059\n",
            "[53]\ttest-rmse:2.96063\n",
            "[54]\ttest-rmse:2.96073\n",
            "[55]\ttest-rmse:2.96079\n",
            "[56]\ttest-rmse:2.96056\n",
            "[57]\ttest-rmse:2.96093\n",
            "[58]\ttest-rmse:2.96106\n",
            "[59]\ttest-rmse:2.96112\n",
            "[60]\ttest-rmse:2.96099\n",
            "[61]\ttest-rmse:2.96118\n",
            "[62]\ttest-rmse:2.96140\n",
            "[63]\ttest-rmse:2.96162\n",
            "[64]\ttest-rmse:2.96149\n",
            "[65]\ttest-rmse:2.96172\n",
            "   MSE: 8.7733, MAE: 2.3600, R²: 0.9336\n",
            "\n",
            "Best model for technical: gradient_boosting\n",
            "  MSE: 8.6578\n",
            "  MAE: 2.3517\n",
            "  R²: 0.9345\n",
            "\n",
            "=== Training models for communication ===\n",
            "Train set: 80000 samples, Test set: 20000 samples\n",
            "\n",
            "Training regression models for communication...\n",
            "GPU acceleration: Enabled\n",
            "1. Training Random Forest Regressor...\n",
            "   MSE: 10.6030, MAE: 2.5938, R²: 0.9205\n",
            "2. Training Gradient Boosting Regressor...\n",
            "   MSE: 10.1797, MAE: 2.5438, R²: 0.9236\n",
            "3. Training XGBoost Regressor...\n",
            "   Using GPU acceleration for XGBoost\n",
            "[0]\ttest-rmse:10.52103\n",
            "[1]\ttest-rmse:9.60835\n",
            "[2]\ttest-rmse:8.79264\n",
            "[3]\ttest-rmse:8.06398\n",
            "[4]\ttest-rmse:7.41714\n",
            "[5]\ttest-rmse:6.84087\n",
            "[6]\ttest-rmse:6.33382\n",
            "[7]\ttest-rmse:5.88687\n",
            "[8]\ttest-rmse:5.49603\n",
            "[9]\ttest-rmse:5.15229\n",
            "[10]\ttest-rmse:4.85529\n",
            "[11]\ttest-rmse:4.59846\n",
            "[12]\ttest-rmse:4.37840\n",
            "[13]\ttest-rmse:4.18854\n",
            "[14]\ttest-rmse:4.02693\n",
            "[15]\ttest-rmse:3.88975\n",
            "[16]\ttest-rmse:3.77356\n",
            "[17]\ttest-rmse:3.67465\n",
            "[18]\ttest-rmse:3.59334\n",
            "[19]\ttest-rmse:3.52459\n",
            "[20]\ttest-rmse:3.46721\n",
            "[21]\ttest-rmse:3.41997\n",
            "[22]\ttest-rmse:3.38021\n",
            "[23]\ttest-rmse:3.34727\n",
            "[24]\ttest-rmse:3.32091\n",
            "[25]\ttest-rmse:3.29903\n",
            "[26]\ttest-rmse:3.28127\n",
            "[27]\ttest-rmse:3.26623\n",
            "[28]\ttest-rmse:3.25384\n",
            "[29]\ttest-rmse:3.24382\n",
            "[30]\ttest-rmse:3.23540\n",
            "[31]\ttest-rmse:3.22870\n",
            "[32]\ttest-rmse:3.22313\n",
            "[33]\ttest-rmse:3.21889\n",
            "[34]\ttest-rmse:3.21535\n",
            "[35]\ttest-rmse:3.21253\n",
            "[36]\ttest-rmse:3.20963\n",
            "[37]\ttest-rmse:3.20768\n",
            "[38]\ttest-rmse:3.20567\n",
            "[39]\ttest-rmse:3.20418\n",
            "[40]\ttest-rmse:3.20303\n",
            "[41]\ttest-rmse:3.20221\n",
            "[42]\ttest-rmse:3.20160\n",
            "[43]\ttest-rmse:3.20077\n",
            "[44]\ttest-rmse:3.20044\n",
            "[45]\ttest-rmse:3.19989\n",
            "[46]\ttest-rmse:3.19958\n",
            "[47]\ttest-rmse:3.19926\n",
            "[48]\ttest-rmse:3.19898\n",
            "[49]\ttest-rmse:3.19872\n",
            "[50]\ttest-rmse:3.19863\n",
            "[51]\ttest-rmse:3.19847\n",
            "[52]\ttest-rmse:3.19817\n",
            "[53]\ttest-rmse:3.19800\n",
            "[54]\ttest-rmse:3.19823\n",
            "[55]\ttest-rmse:3.19823\n",
            "[56]\ttest-rmse:3.19821\n",
            "[57]\ttest-rmse:3.19814\n",
            "[58]\ttest-rmse:3.19801\n",
            "[59]\ttest-rmse:3.19797\n",
            "[60]\ttest-rmse:3.19817\n",
            "[61]\ttest-rmse:3.19827\n",
            "[62]\ttest-rmse:3.19821\n",
            "[63]\ttest-rmse:3.19797\n",
            "[64]\ttest-rmse:3.19794\n",
            "[65]\ttest-rmse:3.19790\n",
            "[66]\ttest-rmse:3.19792\n",
            "[67]\ttest-rmse:3.19791\n",
            "[68]\ttest-rmse:3.19787\n",
            "[69]\ttest-rmse:3.19784\n",
            "[70]\ttest-rmse:3.19764\n",
            "[71]\ttest-rmse:3.19768\n",
            "[72]\ttest-rmse:3.19769\n",
            "[73]\ttest-rmse:3.19770\n",
            "[74]\ttest-rmse:3.19785\n",
            "[75]\ttest-rmse:3.19783\n",
            "[76]\ttest-rmse:3.19786\n",
            "[77]\ttest-rmse:3.19775\n",
            "[78]\ttest-rmse:3.19768\n",
            "[79]\ttest-rmse:3.19753\n",
            "[80]\ttest-rmse:3.19771\n",
            "[81]\ttest-rmse:3.19773\n",
            "[82]\ttest-rmse:3.19810\n",
            "[83]\ttest-rmse:3.19834\n",
            "[84]\ttest-rmse:3.19879\n",
            "[85]\ttest-rmse:3.19901\n",
            "[86]\ttest-rmse:3.19913\n",
            "[87]\ttest-rmse:3.19913\n",
            "[88]\ttest-rmse:3.19922\n",
            "[89]\ttest-rmse:3.19921\n",
            "   MSE: 10.2349, MAE: 2.5472, R²: 0.9232\n",
            "\n",
            "Best model for communication: gradient_boosting\n",
            "  MSE: 10.1797\n",
            "  MAE: 2.5438\n",
            "  R²: 0.9236\n",
            "\n",
            "=== Training models for problem_solving ===\n",
            "Train set: 80000 samples, Test set: 20000 samples\n",
            "\n",
            "Training regression models for problem_solving...\n",
            "GPU acceleration: Enabled\n",
            "1. Training Random Forest Regressor...\n",
            "   MSE: 10.0123, MAE: 2.5239, R²: 0.9233\n",
            "2. Training Gradient Boosting Regressor...\n",
            "   MSE: 9.5263, MAE: 2.4628, R²: 0.9270\n",
            "3. Training XGBoost Regressor...\n",
            "   Using GPU acceleration for XGBoost\n",
            "[0]\ttest-rmse:10.40319\n",
            "[1]\ttest-rmse:9.49160\n",
            "[2]\ttest-rmse:8.67966\n",
            "[3]\ttest-rmse:7.95423\n",
            "[4]\ttest-rmse:7.30847\n",
            "[5]\ttest-rmse:6.73700\n",
            "[6]\ttest-rmse:6.23054\n",
            "[7]\ttest-rmse:5.78379\n",
            "[8]\ttest-rmse:5.39259\n",
            "[9]\ttest-rmse:5.04888\n",
            "[10]\ttest-rmse:4.75216\n",
            "[11]\ttest-rmse:4.49506\n",
            "[12]\ttest-rmse:4.27310\n",
            "[13]\ttest-rmse:4.08233\n",
            "[14]\ttest-rmse:3.91933\n",
            "[15]\ttest-rmse:3.78125\n",
            "[16]\ttest-rmse:3.66413\n",
            "[17]\ttest-rmse:3.56605\n",
            "[18]\ttest-rmse:3.48348\n",
            "[19]\ttest-rmse:3.41468\n",
            "[20]\ttest-rmse:3.35767\n",
            "[21]\ttest-rmse:3.30990\n",
            "[22]\ttest-rmse:3.27052\n",
            "[23]\ttest-rmse:3.23781\n",
            "[24]\ttest-rmse:3.21054\n",
            "[25]\ttest-rmse:3.18865\n",
            "[26]\ttest-rmse:3.17063\n",
            "[27]\ttest-rmse:3.15567\n",
            "[28]\ttest-rmse:3.14386\n",
            "[29]\ttest-rmse:3.13406\n",
            "[30]\ttest-rmse:3.12586\n",
            "[31]\ttest-rmse:3.11916\n",
            "[32]\ttest-rmse:3.11386\n",
            "[33]\ttest-rmse:3.10968\n",
            "[34]\ttest-rmse:3.10593\n",
            "[35]\ttest-rmse:3.10279\n",
            "[36]\ttest-rmse:3.10045\n",
            "[37]\ttest-rmse:3.09847\n",
            "[38]\ttest-rmse:3.09726\n",
            "[39]\ttest-rmse:3.09575\n",
            "[40]\ttest-rmse:3.09469\n",
            "[41]\ttest-rmse:3.09385\n",
            "[42]\ttest-rmse:3.09292\n",
            "[43]\ttest-rmse:3.09216\n",
            "[44]\ttest-rmse:3.09183\n",
            "[45]\ttest-rmse:3.09153\n",
            "[46]\ttest-rmse:3.09115\n",
            "[47]\ttest-rmse:3.09045\n",
            "[48]\ttest-rmse:3.09037\n",
            "[49]\ttest-rmse:3.09038\n",
            "[50]\ttest-rmse:3.09042\n",
            "[51]\ttest-rmse:3.09032\n",
            "[52]\ttest-rmse:3.09009\n",
            "[53]\ttest-rmse:3.09005\n",
            "[54]\ttest-rmse:3.08993\n",
            "[55]\ttest-rmse:3.09013\n",
            "[56]\ttest-rmse:3.08988\n",
            "[57]\ttest-rmse:3.08994\n",
            "[58]\ttest-rmse:3.08998\n",
            "[59]\ttest-rmse:3.08992\n",
            "[60]\ttest-rmse:3.08995\n",
            "[61]\ttest-rmse:3.08988\n",
            "[62]\ttest-rmse:3.08968\n",
            "[63]\ttest-rmse:3.08966\n",
            "[64]\ttest-rmse:3.08965\n",
            "[65]\ttest-rmse:3.08981\n",
            "[66]\ttest-rmse:3.09009\n",
            "[67]\ttest-rmse:3.09024\n",
            "[68]\ttest-rmse:3.09025\n",
            "[69]\ttest-rmse:3.09028\n",
            "[70]\ttest-rmse:3.09022\n",
            "[71]\ttest-rmse:3.09025\n",
            "[72]\ttest-rmse:3.09024\n",
            "[73]\ttest-rmse:3.09030\n",
            "   MSE: 9.5505, MAE: 2.4603, R²: 0.9268\n",
            "\n",
            "Best model for problem_solving: gradient_boosting\n",
            "  MSE: 9.5263\n",
            "  MAE: 2.4628\n",
            "  R²: 0.9270\n",
            "\n",
            "=== Training models for cultural_fit ===\n",
            "Train set: 80000 samples, Test set: 20000 samples\n",
            "\n",
            "Training regression models for cultural_fit...\n",
            "GPU acceleration: Enabled\n",
            "1. Training Random Forest Regressor...\n",
            "   MSE: 10.8659, MAE: 2.6229, R²: 0.9235\n",
            "2. Training Gradient Boosting Regressor...\n",
            "   MSE: 10.3837, MAE: 2.5593, R²: 0.9269\n",
            "3. Training XGBoost Regressor...\n",
            "   Using GPU acceleration for XGBoost\n",
            "[0]\ttest-rmse:10.85386\n",
            "[1]\ttest-rmse:9.90736\n",
            "[2]\ttest-rmse:9.05852\n",
            "[3]\ttest-rmse:8.30468\n",
            "[4]\ttest-rmse:7.63105\n",
            "[5]\ttest-rmse:7.03209\n",
            "[6]\ttest-rmse:6.50420\n",
            "[7]\ttest-rmse:6.03989\n",
            "[8]\ttest-rmse:5.63127\n",
            "[9]\ttest-rmse:5.27535\n",
            "[10]\ttest-rmse:4.96344\n",
            "[11]\ttest-rmse:4.69499\n",
            "[12]\ttest-rmse:4.46252\n",
            "[13]\ttest-rmse:4.26365\n",
            "[14]\ttest-rmse:4.09451\n",
            "[15]\ttest-rmse:3.95077\n",
            "[16]\ttest-rmse:3.82950\n",
            "[17]\ttest-rmse:3.72699\n",
            "[18]\ttest-rmse:3.64096\n",
            "[19]\ttest-rmse:3.56945\n",
            "[20]\ttest-rmse:3.50979\n",
            "[21]\ttest-rmse:3.45967\n",
            "[22]\ttest-rmse:3.41909\n",
            "[23]\ttest-rmse:3.38509\n",
            "[24]\ttest-rmse:3.35727\n",
            "[25]\ttest-rmse:3.33464\n",
            "[26]\ttest-rmse:3.31584\n",
            "[27]\ttest-rmse:3.30001\n",
            "[28]\ttest-rmse:3.28722\n",
            "[29]\ttest-rmse:3.27655\n",
            "[30]\ttest-rmse:3.26782\n",
            "[31]\ttest-rmse:3.26092\n",
            "[32]\ttest-rmse:3.25498\n",
            "[33]\ttest-rmse:3.25040\n",
            "[34]\ttest-rmse:3.24682\n",
            "[35]\ttest-rmse:3.24351\n",
            "[36]\ttest-rmse:3.24088\n",
            "[37]\ttest-rmse:3.23893\n",
            "[38]\ttest-rmse:3.23702\n",
            "[39]\ttest-rmse:3.23574\n",
            "[40]\ttest-rmse:3.23465\n",
            "[41]\ttest-rmse:3.23368\n",
            "[42]\ttest-rmse:3.23271\n",
            "[43]\ttest-rmse:3.23221\n",
            "[44]\ttest-rmse:3.23179\n",
            "[45]\ttest-rmse:3.23141\n",
            "[46]\ttest-rmse:3.23123\n",
            "[47]\ttest-rmse:3.23100\n",
            "[48]\ttest-rmse:3.23076\n",
            "[49]\ttest-rmse:3.23072\n",
            "[50]\ttest-rmse:3.23059\n",
            "[51]\ttest-rmse:3.23053\n",
            "[52]\ttest-rmse:3.23051\n",
            "[53]\ttest-rmse:3.23040\n",
            "[54]\ttest-rmse:3.23054\n",
            "[55]\ttest-rmse:3.23054\n",
            "[56]\ttest-rmse:3.23065\n",
            "[57]\ttest-rmse:3.23066\n",
            "[58]\ttest-rmse:3.23063\n",
            "[59]\ttest-rmse:3.23076\n",
            "[60]\ttest-rmse:3.23054\n",
            "[61]\ttest-rmse:3.23051\n",
            "[62]\ttest-rmse:3.23036\n",
            "[63]\ttest-rmse:3.23040\n",
            "[64]\ttest-rmse:3.23074\n",
            "[65]\ttest-rmse:3.23096\n",
            "[66]\ttest-rmse:3.23093\n",
            "[67]\ttest-rmse:3.23124\n",
            "[68]\ttest-rmse:3.23148\n",
            "[69]\ttest-rmse:3.23148\n",
            "[70]\ttest-rmse:3.23166\n",
            "[71]\ttest-rmse:3.23169\n",
            "[72]\ttest-rmse:3.23162\n",
            "   MSE: 10.4434, MAE: 2.5657, R²: 0.9264\n",
            "\n",
            "Best model for cultural_fit: gradient_boosting\n",
            "  MSE: 10.3837\n",
            "  MAE: 2.5593\n",
            "  R²: 0.9269\n",
            "\n",
            "=== Training models for overall ===\n",
            "Train set: 80000 samples, Test set: 20000 samples\n",
            "\n",
            "Training regression models for overall...\n",
            "GPU acceleration: Enabled\n",
            "1. Training Random Forest Regressor...\n",
            "   MSE: 19.2039, MAE: 3.4644, R²: 0.8742\n",
            "2. Training Gradient Boosting Regressor...\n",
            "   MSE: 15.9165, MAE: 3.1669, R²: 0.8957\n",
            "3. Training XGBoost Regressor...\n",
            "   Using GPU acceleration for XGBoost\n",
            "[0]\ttest-rmse:11.71655\n",
            "[1]\ttest-rmse:11.15518\n",
            "[2]\ttest-rmse:10.65748\n",
            "[3]\ttest-rmse:10.20422\n",
            "[4]\ttest-rmse:9.79108\n",
            "[5]\ttest-rmse:9.41646\n",
            "[6]\ttest-rmse:9.07748\n",
            "[7]\ttest-rmse:8.76255\n",
            "[8]\ttest-rmse:8.47263\n",
            "[9]\ttest-rmse:8.20413\n",
            "[10]\ttest-rmse:7.95364\n",
            "[11]\ttest-rmse:7.71782\n",
            "[12]\ttest-rmse:7.50131\n",
            "[13]\ttest-rmse:7.28921\n",
            "[14]\ttest-rmse:7.09882\n",
            "[15]\ttest-rmse:6.91763\n",
            "[16]\ttest-rmse:6.74249\n",
            "[17]\ttest-rmse:6.58229\n",
            "[18]\ttest-rmse:6.42911\n",
            "[19]\ttest-rmse:6.27902\n",
            "[20]\ttest-rmse:6.14412\n",
            "[21]\ttest-rmse:6.01159\n",
            "[22]\ttest-rmse:5.88456\n",
            "[23]\ttest-rmse:5.76117\n",
            "[24]\ttest-rmse:5.64775\n",
            "[25]\ttest-rmse:5.53607\n",
            "[26]\ttest-rmse:5.43466\n",
            "[27]\ttest-rmse:5.33200\n",
            "[28]\ttest-rmse:5.23899\n",
            "[29]\ttest-rmse:5.15064\n",
            "[30]\ttest-rmse:5.06790\n",
            "[31]\ttest-rmse:4.98832\n",
            "[32]\ttest-rmse:4.90990\n",
            "[33]\ttest-rmse:4.83763\n",
            "[34]\ttest-rmse:4.76591\n",
            "[35]\ttest-rmse:4.70350\n",
            "[36]\ttest-rmse:4.64296\n",
            "[37]\ttest-rmse:4.58387\n",
            "[38]\ttest-rmse:4.52930\n",
            "[39]\ttest-rmse:4.47583\n",
            "[40]\ttest-rmse:4.42589\n",
            "[41]\ttest-rmse:4.37638\n",
            "[42]\ttest-rmse:4.32935\n",
            "[43]\ttest-rmse:4.28648\n",
            "[44]\ttest-rmse:4.24642\n",
            "[45]\ttest-rmse:4.20720\n",
            "[46]\ttest-rmse:4.16912\n",
            "[47]\ttest-rmse:4.13459\n",
            "[48]\ttest-rmse:4.10243\n",
            "[49]\ttest-rmse:4.07173\n",
            "[50]\ttest-rmse:4.04182\n",
            "[51]\ttest-rmse:4.01477\n",
            "[52]\ttest-rmse:3.98908\n",
            "[53]\ttest-rmse:3.96491\n",
            "[54]\ttest-rmse:3.94187\n",
            "[55]\ttest-rmse:3.92001\n",
            "[56]\ttest-rmse:3.89732\n",
            "[57]\ttest-rmse:3.87732\n",
            "[58]\ttest-rmse:3.85905\n",
            "[59]\ttest-rmse:3.84186\n",
            "[60]\ttest-rmse:3.82555\n",
            "[61]\ttest-rmse:3.81014\n",
            "[62]\ttest-rmse:3.79476\n",
            "[63]\ttest-rmse:3.77990\n",
            "[64]\ttest-rmse:3.76780\n",
            "[65]\ttest-rmse:3.75453\n",
            "[66]\ttest-rmse:3.74278\n",
            "[67]\ttest-rmse:3.73177\n",
            "[68]\ttest-rmse:3.72142\n",
            "[69]\ttest-rmse:3.71177\n",
            "[70]\ttest-rmse:3.70174\n",
            "[71]\ttest-rmse:3.69279\n",
            "[72]\ttest-rmse:3.68450\n",
            "[73]\ttest-rmse:3.67578\n",
            "[74]\ttest-rmse:3.66774\n",
            "[75]\ttest-rmse:3.66004\n",
            "[76]\ttest-rmse:3.65337\n",
            "[77]\ttest-rmse:3.64653\n",
            "[78]\ttest-rmse:3.64081\n",
            "[79]\ttest-rmse:3.63428\n",
            "[80]\ttest-rmse:3.62900\n",
            "[81]\ttest-rmse:3.62326\n",
            "[82]\ttest-rmse:3.61800\n",
            "[83]\ttest-rmse:3.61342\n",
            "[84]\ttest-rmse:3.60917\n",
            "[85]\ttest-rmse:3.60536\n",
            "[86]\ttest-rmse:3.60075\n",
            "[87]\ttest-rmse:3.59700\n",
            "[88]\ttest-rmse:3.59338\n",
            "[89]\ttest-rmse:3.59009\n",
            "[90]\ttest-rmse:3.58690\n",
            "[91]\ttest-rmse:3.58352\n",
            "[92]\ttest-rmse:3.58041\n",
            "[93]\ttest-rmse:3.57814\n",
            "[94]\ttest-rmse:3.57582\n",
            "[95]\ttest-rmse:3.57283\n",
            "[96]\ttest-rmse:3.57062\n",
            "[97]\ttest-rmse:3.56881\n",
            "[98]\ttest-rmse:3.56668\n",
            "[99]\ttest-rmse:3.56463\n",
            "   MSE: 12.7066, MAE: 2.8353, R²: 0.9168\n",
            "\n",
            "Best model for overall: xgboost\n",
            "  MSE: 12.7066\n",
            "  MAE: 2.8353\n",
            "  R²: 0.9168\n",
            "\n",
            "=== Training classifier for Strong/Average/Weak categories ===\n",
            "\n",
            "Training classifier for Strong/Average/Weak categories...\n",
            "GPU acceleration: Enabled\n",
            "1. Training Random Forest Classifier...\n",
            "   Accuracy: 0.8905\n",
            "2. Training XGBoost Classifier...\n",
            "   Using GPU acceleration for XGBoost\n",
            "[0]\ttest-mlogloss:1.01835\n",
            "[1]\ttest-mlogloss:0.95056\n",
            "[2]\ttest-mlogloss:0.89180\n",
            "[3]\ttest-mlogloss:0.84111\n",
            "[4]\ttest-mlogloss:0.79684\n",
            "[5]\ttest-mlogloss:0.75760\n",
            "[6]\ttest-mlogloss:0.72282\n",
            "[7]\ttest-mlogloss:0.69201\n",
            "[8]\ttest-mlogloss:0.66418\n",
            "[9]\ttest-mlogloss:0.63918\n",
            "[10]\ttest-mlogloss:0.61663\n",
            "[11]\ttest-mlogloss:0.59588\n",
            "[12]\ttest-mlogloss:0.57712\n",
            "[13]\ttest-mlogloss:0.55983\n",
            "[14]\ttest-mlogloss:0.54436\n",
            "[15]\ttest-mlogloss:0.52950\n",
            "[16]\ttest-mlogloss:0.51622\n",
            "[17]\ttest-mlogloss:0.50372\n",
            "[18]\ttest-mlogloss:0.49217\n",
            "[19]\ttest-mlogloss:0.48119\n",
            "[20]\ttest-mlogloss:0.47099\n",
            "[21]\ttest-mlogloss:0.46183\n",
            "[22]\ttest-mlogloss:0.45292\n",
            "[23]\ttest-mlogloss:0.44423\n",
            "[24]\ttest-mlogloss:0.43649\n",
            "[25]\ttest-mlogloss:0.42910\n",
            "[26]\ttest-mlogloss:0.42196\n",
            "[27]\ttest-mlogloss:0.41508\n",
            "[28]\ttest-mlogloss:0.40902\n",
            "[29]\ttest-mlogloss:0.40280\n",
            "[30]\ttest-mlogloss:0.39687\n",
            "[31]\ttest-mlogloss:0.39133\n",
            "[32]\ttest-mlogloss:0.38588\n",
            "[33]\ttest-mlogloss:0.38103\n",
            "[34]\ttest-mlogloss:0.37611\n",
            "[35]\ttest-mlogloss:0.37158\n",
            "[36]\ttest-mlogloss:0.36712\n",
            "[37]\ttest-mlogloss:0.36289\n",
            "[38]\ttest-mlogloss:0.35884\n",
            "[39]\ttest-mlogloss:0.35482\n",
            "[40]\ttest-mlogloss:0.35112\n",
            "[41]\ttest-mlogloss:0.34770\n",
            "[42]\ttest-mlogloss:0.34429\n",
            "[43]\ttest-mlogloss:0.34107\n",
            "[44]\ttest-mlogloss:0.33813\n",
            "[45]\ttest-mlogloss:0.33508\n",
            "[46]\ttest-mlogloss:0.33205\n",
            "[47]\ttest-mlogloss:0.32913\n",
            "[48]\ttest-mlogloss:0.32635\n",
            "[49]\ttest-mlogloss:0.32371\n",
            "[50]\ttest-mlogloss:0.32104\n",
            "[51]\ttest-mlogloss:0.31857\n",
            "[52]\ttest-mlogloss:0.31614\n",
            "[53]\ttest-mlogloss:0.31385\n",
            "[54]\ttest-mlogloss:0.31134\n",
            "[55]\ttest-mlogloss:0.30909\n",
            "[56]\ttest-mlogloss:0.30691\n",
            "[57]\ttest-mlogloss:0.30480\n",
            "[58]\ttest-mlogloss:0.30270\n",
            "[59]\ttest-mlogloss:0.30074\n",
            "[60]\ttest-mlogloss:0.29885\n",
            "[61]\ttest-mlogloss:0.29694\n",
            "[62]\ttest-mlogloss:0.29518\n",
            "[63]\ttest-mlogloss:0.29341\n",
            "[64]\ttest-mlogloss:0.29172\n",
            "[65]\ttest-mlogloss:0.29002\n",
            "[66]\ttest-mlogloss:0.28840\n",
            "[67]\ttest-mlogloss:0.28689\n",
            "[68]\ttest-mlogloss:0.28522\n",
            "[69]\ttest-mlogloss:0.28367\n",
            "[70]\ttest-mlogloss:0.28214\n",
            "[71]\ttest-mlogloss:0.28078\n",
            "[72]\ttest-mlogloss:0.27952\n",
            "[73]\ttest-mlogloss:0.27811\n",
            "[74]\ttest-mlogloss:0.27671\n",
            "[75]\ttest-mlogloss:0.27546\n",
            "[76]\ttest-mlogloss:0.27421\n",
            "[77]\ttest-mlogloss:0.27309\n",
            "[78]\ttest-mlogloss:0.27183\n",
            "[79]\ttest-mlogloss:0.27072\n",
            "[80]\ttest-mlogloss:0.26960\n",
            "[81]\ttest-mlogloss:0.26847\n",
            "[82]\ttest-mlogloss:0.26741\n",
            "[83]\ttest-mlogloss:0.26623\n",
            "[84]\ttest-mlogloss:0.26518\n",
            "[85]\ttest-mlogloss:0.26414\n",
            "[86]\ttest-mlogloss:0.26313\n",
            "[87]\ttest-mlogloss:0.26222\n",
            "[88]\ttest-mlogloss:0.26124\n",
            "[89]\ttest-mlogloss:0.26029\n",
            "[90]\ttest-mlogloss:0.25942\n",
            "[91]\ttest-mlogloss:0.25846\n",
            "[92]\ttest-mlogloss:0.25755\n",
            "[93]\ttest-mlogloss:0.25658\n",
            "[94]\ttest-mlogloss:0.25574\n",
            "[95]\ttest-mlogloss:0.25489\n",
            "[96]\ttest-mlogloss:0.25411\n",
            "[97]\ttest-mlogloss:0.25328\n",
            "[98]\ttest-mlogloss:0.25241\n",
            "[99]\ttest-mlogloss:0.25168\n",
            "   Accuracy: 0.8990\n",
            "\n",
            "Best classifier: xgboost (Accuracy: 0.8990)\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Weak       0.89      0.74      0.81      3929\n",
            "     Average       0.90      0.97      0.93     14896\n",
            "      Strong       0.90      0.50      0.64      1175\n",
            "\n",
            "    accuracy                           0.90     20000\n",
            "   macro avg       0.90      0.74      0.79     20000\n",
            "weighted avg       0.90      0.90      0.89     20000\n",
            "\n",
            "\n",
            "=== Evaluating models on full dataset ===\n",
            "    Score range: 5.40 to 97.56, average: 53.30\n",
            "    Score range: 0.66 to 93.49, average: 51.05\n",
            "    Score range: 3.21 to 96.69, average: 51.60\n",
            "    Score range: 4.49 to 97.58, average: 50.13\n",
            "    Score range: 4.81 to 95.92, average: 50.58\n",
            "    Classification results: {'Strong': 3830, 'Average': 79176, 'Weak': 16994}\n",
            "\n",
            "All models saved successfully to interview_evaluation_models.pkl\n",
            "\n",
            "Model training completed successfully!\n",
            "\n",
            "Execution complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 8: Interview Process"
      ],
      "metadata": {
        "id": "1t8CprB7jWXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# Section 8: Interview Process\"\"\"\n",
        "\n",
        "#!/usr/bin/env python3\n",
        "# Interview Process - Interactive interview evaluation tool\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import re\n",
        "import time\n",
        "from collections import Counter\n",
        "\n",
        "# Try to import transformers for sentiment analysis\n",
        "try:\n",
        "    from transformers import pipeline\n",
        "    TRANSFORMERS_AVAILABLE = True\n",
        "    # Initialize sentiment analysis pipeline\n",
        "    sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "except ImportError:\n",
        "    TRANSFORMERS_AVAILABLE = False\n",
        "    print(\"Transformers library not available. Using basic sentiment analysis.\")\n",
        "\n",
        "# Check if models exist, if not, try to train them\n",
        "if not os.path.exists(\"interview_evaluation_models.pkl\"):\n",
        "    print(\"Interview evaluation models not found. Please run train_ml_models.py first.\")\n",
        "    print(\"Attempting to run training script...\")\n",
        "    try:\n",
        "        import train_ml_models\n",
        "        print(\"Training completed.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error training models: {e}\")\n",
        "        print(\"Please run train_ml_models.py manually before using this script.\")\n",
        "        exit(1)\n",
        "\n",
        "# Load the trained models\n",
        "try:\n",
        "    print(\"Loading interview evaluation models...\")\n",
        "    model_package = joblib.load(\"interview_evaluation_models.pkl\")\n",
        "    print(\"Models loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading models: {e}\")\n",
        "    exit(1)\n",
        "\n",
        "def clear_screen():\n",
        "    \"\"\"Clear the terminal screen.\"\"\"\n",
        "    os.system('cls' if os.name == 'nt' else 'clear')\n",
        "\n",
        "def typing_effect(text, speed=0.03):\n",
        "    \"\"\"Display text with a typing effect.\"\"\"\n",
        "    for char in text:\n",
        "        print(char, end='', flush=True)\n",
        "        time.sleep(speed)\n",
        "    print()\n",
        "\n",
        "def extract_features_from_text(text, language=\"english\"):\n",
        "    \"\"\"\n",
        "    Extract features from interview text response.\n",
        "\n",
        "    Args:\n",
        "        text: The user's response text\n",
        "        language: The language of the response (english or arabic)\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of features\n",
        "    \"\"\"\n",
        "    # Ensure text is a string and not empty\n",
        "    if text is None:\n",
        "        text = \"\"\n",
        "    text = str(text).strip()\n",
        "\n",
        "    # Length-based features\n",
        "    word_count = len(text.split())\n",
        "    char_count = len(text)\n",
        "    avg_word_length = char_count / max(word_count, 1)\n",
        "\n",
        "    # Count sentence endings\n",
        "    sentence_count = text.count('.') + text.count('!') + text.count('?')\n",
        "\n",
        "    # Set default sentiment value\n",
        "    sentiment_value = 0.5\n",
        "\n",
        "    # Basic sentiment analysis using predefined word lists if transformers is not available\n",
        "    positive_words = [\"good\", \"great\", \"excellent\", \"amazing\", \"awesome\", \"fantastic\", \"positive\",\n",
        "                       \"happy\", \"love\", \"enjoy\", \"thank\", \"appreciate\", \"enthusiastic\", \"success\"]\n",
        "\n",
        "    negative_words = [\"bad\", \"poor\", \"terrible\", \"horrible\", \"awful\", \"negative\", \"sad\", \"hate\",\n",
        "                       \"dislike\", \"angry\", \"upset\", \"fail\", \"disappointing\", \"never\", \"no\", \"not\"]\n",
        "\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    # Perform sentiment analysis\n",
        "    if TRANSFORMERS_AVAILABLE and word_count > 3:\n",
        "        try:\n",
        "            # Use transformers pipeline for better sentiment analysis\n",
        "            sentiment_result = sentiment_analyzer(text)[0]\n",
        "            sentiment_score = sentiment_result['score']\n",
        "            sentiment_label = sentiment_result['label']\n",
        "\n",
        "            # Convert sentiment to numeric values (0-1 range)\n",
        "            if sentiment_label == \"POSITIVE\":\n",
        "                sentiment_value = sentiment_score\n",
        "            else:\n",
        "                sentiment_value = 1 - sentiment_score\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in sentiment analysis: {e}\")\n",
        "            # Fallback to basic sentiment analysis\n",
        "            positive_count = sum(1 for word in positive_words if word in text_lower)\n",
        "            negative_count = sum(1 for word in negative_words if word in text_lower)\n",
        "            sentiment_value = positive_count / (positive_count + negative_count + 1)\n",
        "    else:\n",
        "        # Basic sentiment analysis based on word matching\n",
        "        positive_count = sum(1 for word in positive_words if word in text_lower)\n",
        "        negative_count = sum(1 for word in negative_words if word in text_lower)\n",
        "        sentiment_value = positive_count / (positive_count + negative_count + 1)\n",
        "\n",
        "    # Technical keywords - basic example (would be more extensive in real app)\n",
        "    technical_keywords = [\n",
        "        \"algorithm\", \"data structure\", \"code\", \"programming\", \"software\",\n",
        "        \"development\", \"database\", \"api\", \"function\", \"class\", \"object\",\n",
        "        \"method\", \"variable\", \"framework\", \"library\", \"interface\", \"architecture\",\n",
        "        \"test\", \"debug\", \"deploy\", \"cloud\", \"server\", \"client\", \"network\"\n",
        "    ]\n",
        "\n",
        "    # Communication keywords\n",
        "    communication_keywords = [\n",
        "        \"communicate\", \"team\", \"collaborate\", \"explain\", \"present\",\n",
        "        \"discuss\", \"share\", \"meetings\", \"documentation\", \"report\",\n",
        "        \"clarity\", \"articulate\", \"express\", \"listen\", \"feedback\"\n",
        "    ]\n",
        "\n",
        "    # Problem-solving keywords\n",
        "    problem_solving_keywords = [\n",
        "        \"solve\", \"solution\", \"analyze\", \"optimize\", \"improve\", \"debug\",\n",
        "        \"troubleshoot\", \"fix\", \"enhance\", \"approach\", \"method\", \"strategy\",\n",
        "        \"plan\", \"design\", \"implement\", \"test\"\n",
        "    ]\n",
        "\n",
        "    # Cultural fit keywords\n",
        "    cultural_fit_keywords = [\n",
        "        \"team\", \"culture\", \"values\", \"mission\", \"collaborate\", \"work ethic\",\n",
        "        \"adaptable\", \"flexible\", \"learn\", \"growth\", \"positive\", \"attitude\",\n",
        "        \"initiative\", \"proactive\", \"responsible\", \"accountable\"\n",
        "    ]\n",
        "\n",
        "    # Negative response indicators\n",
        "    negative_phrases = [\n",
        "        \"i don't know\", \"no idea\", \"not sure\", \"never heard\", \"no experience\",\n",
        "        \"i haven't\", \"can't answer\", \"don't understand\", \"no clue\", \"not familiar\"\n",
        "    ]\n",
        "\n",
        "    # Check for minimal or negative responses\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    # Detect if response is minimal or negative\n",
        "    is_minimal_response = word_count <= 5\n",
        "    has_negative_phrase = any(phrase in text_lower for phrase in negative_phrases)\n",
        "\n",
        "    # Apply severe scoring penalty for minimal or negative responses\n",
        "    minimal_response_penalty = 0.1 if is_minimal_response or has_negative_phrase else 1.0\n",
        "\n",
        "    # Count occurrences of each keyword type\n",
        "    tech_count = sum(1 for word in technical_keywords if word in text_lower)\n",
        "    comm_count = sum(1 for word in communication_keywords if word in text_lower)\n",
        "    prob_count = sum(1 for word in problem_solving_keywords if word in text_lower)\n",
        "    cult_count = sum(1 for word in cultural_fit_keywords if word in text_lower)\n",
        "\n",
        "    # Ratios\n",
        "    tech_ratio = tech_count / max(word_count, 1)\n",
        "    comm_ratio = comm_count / max(word_count, 1)\n",
        "    prob_ratio = prob_count / max(word_count, 1)\n",
        "    cult_ratio = cult_count / max(word_count, 1)\n",
        "\n",
        "    # Simple confidence score based on word count and keyword usage\n",
        "    base_confidence = min(100, (word_count / 50) * 100 + (tech_count + comm_count) * 5)\n",
        "    confidence_score = base_confidence * minimal_response_penalty\n",
        "\n",
        "    # Calculate quality score based on response length and complexity\n",
        "    quality_score = min(100, word_count * 2) * minimal_response_penalty\n",
        "\n",
        "    # Adjust scores for minimal or negative responses\n",
        "    if is_minimal_response or has_negative_phrase:\n",
        "        tech_count *= 0.1\n",
        "        comm_count *= 0.1\n",
        "        prob_count *= 0.1\n",
        "        cult_count *= 0.1\n",
        "        tech_ratio *= 0.1\n",
        "        comm_ratio *= 0.1\n",
        "        prob_ratio *= 0.1\n",
        "        cult_ratio *= 0.1\n",
        "\n",
        "    # Create feature dictionary (matching expected model feature names)\n",
        "    features = {\n",
        "        'feature_0': float(word_count * minimal_response_penalty),         # Word count\n",
        "        'feature_1': float(char_count * minimal_response_penalty),         # Character count\n",
        "        'feature_2': float(avg_word_length),                              # Average word length\n",
        "        'feature_3': float(sentence_count * minimal_response_penalty),     # Number of sentences\n",
        "        'feature_4': float(tech_count),                                   # Technical keyword count\n",
        "        'feature_5': float(comm_count),                                   # Communication keyword count\n",
        "        'feature_6': float(prob_count),                                   # Problem-solving keyword count\n",
        "        'feature_7': float(cult_count),                                   # Cultural fit keyword count\n",
        "        'feature_8': float(confidence_score),                             # Confidence score\n",
        "        'feature_9': float((tech_ratio + comm_ratio + prob_ratio + cult_ratio) * 25 * minimal_response_penalty),  # Overall keyword ratio\n",
        "        'feature_10': float(sentiment_value),                                # Sentiment score\n",
        "    }\n",
        "\n",
        "    return features\n",
        "\n",
        "def predict_response_scores(features):\n",
        "    \"\"\"\n",
        "    Predict scores for an interview response using loaded models.\n",
        "\n",
        "    Args:\n",
        "        features: Dictionary of features extracted from response\n",
        "\n",
        "    Returns:\n",
        "        Dictionary of predicted scores and category\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Extract word count from features (stored as feature_0)\n",
        "        word_count = features.get('feature_0', 0)\n",
        "\n",
        "        # Convert features to DataFrame\n",
        "        features_df = pd.DataFrame([features])\n",
        "\n",
        "        # Ensure all required features (feature_0 through feature_10) are present\n",
        "        for i in range(11):\n",
        "            feature_name = f'feature_{i}'\n",
        "            if feature_name not in features_df.columns:\n",
        "                print(f\"Warning: {feature_name} not found in extracted features. Using default value 0.\")\n",
        "                features_df[feature_name] = 0\n",
        "\n",
        "        # Ensure all feature values are numeric\n",
        "        for col in features_df.columns:\n",
        "            features_df[col] = pd.to_numeric(features_df[col], errors='coerce').fillna(0)\n",
        "\n",
        "        # Make predictions\n",
        "        predictions = predict_with_models(features_df, model_package, verbose=False)\n",
        "\n",
        "        # If predictions are None, return a default response\n",
        "        if predictions is None:\n",
        "            print(\"Warning: Model prediction failed. Returning default scores.\")\n",
        "            # For a minimal response like \"idk\", use appropriate low scores and Weak category\n",
        "            if word_count < 5:\n",
        "                return {\n",
        "                    'communication': np.array([20.0]),\n",
        "                    'technical': np.array([20.0]),\n",
        "                    'problem_solving': np.array([20.0]),\n",
        "                    'cultural_fit': np.array([20.0]),\n",
        "                    'overall': np.array([20.0]),\n",
        "                    'category': np.array(['Weak'])\n",
        "                }\n",
        "            else:\n",
        "                return {\n",
        "                    'communication': np.array([50.0]),\n",
        "                    'technical': np.array([50.0]),\n",
        "                    'problem_solving': np.array([50.0]),\n",
        "                    'cultural_fit': np.array([50.0]),\n",
        "                    'overall': np.array([50.0]),\n",
        "                    'category': np.array(['Average'])\n",
        "                }\n",
        "\n",
        "        # Ensure all predictions are in array format\n",
        "        for key in predictions:\n",
        "            if key != 'category' and not isinstance(predictions[key], (list, np.ndarray)):\n",
        "                predictions[key] = np.array([predictions[key]])\n",
        "            elif key == 'category' and not isinstance(predictions[key], (list, np.ndarray)):\n",
        "                predictions[key] = np.array([predictions[key]])\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in predict_response_scores: {e}\")\n",
        "        # Return a default set of scores based on word count\n",
        "        if word_count < 5:\n",
        "            return {\n",
        "                'communication': np.array([20.0]),\n",
        "                'technical': np.array([20.0]),\n",
        "                'problem_solving': np.array([20.0]),\n",
        "                'cultural_fit': np.array([20.0]),\n",
        "                'overall': np.array([20.0]),\n",
        "                'category': np.array(['Weak'])\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                'communication': np.array([50.0]),\n",
        "                'technical': np.array([50.0]),\n",
        "                'problem_solving': np.array([50.0]),\n",
        "                'cultural_fit': np.array([50.0]),\n",
        "                'overall': np.array([50.0]),\n",
        "                'category': np.array(['Average'])\n",
        "            }\n",
        "\n",
        "def predict_with_models(data, model_package, verbose=False):\n",
        "    \"\"\"Simplified version of the predict_with_models function from train_ml_models.py\"\"\"\n",
        "    try:\n",
        "        # Extract components from the package\n",
        "        models = model_package['models']\n",
        "        scaler = model_package['scaler']\n",
        "        feature_columns = model_package['feature_columns']\n",
        "\n",
        "        # Ensure all needed features are present\n",
        "        for col in feature_columns:\n",
        "            if col not in data.columns:\n",
        "                if verbose:\n",
        "                    print(f\"Adding missing feature column: {col}\")\n",
        "                data[col] = 0  # Default value for missing features\n",
        "\n",
        "        # Extract required features in the correct order\n",
        "        X = data[feature_columns].copy()\n",
        "\n",
        "        # Ensure all feature values are numeric\n",
        "        for col in X.columns:\n",
        "            if not np.issubdtype(X[col].dtype, np.number):\n",
        "                X[col] = X[col].astype(float)\n",
        "\n",
        "        # Scale features\n",
        "        X_scaled = scaler.transform(X)\n",
        "\n",
        "        # Import XGBoost if needed for predictions\n",
        "        try:\n",
        "            import xgboost as xgb\n",
        "            xgboost_available = True\n",
        "        except ImportError:\n",
        "            xgboost_available = False\n",
        "\n",
        "        # Create DMatrix for XGBoost models if needed\n",
        "        needs_dmatrix = False\n",
        "        X_dmatrix = None\n",
        "\n",
        "        for model_name, model in models.items():\n",
        "            if model_name != 'classifier_type' and isinstance(model, xgb.Booster):\n",
        "                needs_dmatrix = True\n",
        "                break\n",
        "\n",
        "        if needs_dmatrix and xgboost_available:\n",
        "            try:\n",
        "                X_dmatrix = xgb.DMatrix(X_scaled)\n",
        "            except Exception as e:\n",
        "                print(f\"Error creating DMatrix: {e}\")\n",
        "                return None\n",
        "\n",
        "        # Make predictions with all models\n",
        "        predictions = {}\n",
        "        for target_name, model in models.items():\n",
        "            if target_name == 'classifier_type':\n",
        "                continue  # Skip the type indicator\n",
        "\n",
        "            try:\n",
        "                if target_name == 'classifier':\n",
        "                    # Check classifier type\n",
        "                    if 'classifier_type' in models and models['classifier_type'] == 'xgboost':\n",
        "                        if X_dmatrix is None:\n",
        "                            predictions['category'] = 'Average'  # Default if DMatrix failed\n",
        "                            continue\n",
        "                        # For XGBoost booster\n",
        "                        class_preds_num = model.predict(X_dmatrix)[0]\n",
        "                        # Map back to labels\n",
        "                        class_mapping = {0: 'Weak', 1: 'Average', 2: 'Strong'}\n",
        "                        predictions['category'] = class_mapping[int(class_preds_num)]\n",
        "                    else:\n",
        "                        # For sklearn classifiers\n",
        "                        class_pred_num = model.predict(X_scaled)[0]\n",
        "                        # Map back to labels\n",
        "                        class_mapping = {0: 'Weak', 1: 'Average', 2: 'Strong'}\n",
        "                        predictions['category'] = class_mapping[int(class_pred_num)]\n",
        "                else:\n",
        "                    # Regression predictions\n",
        "                    if isinstance(model, xgb.Booster):\n",
        "                        if X_dmatrix is None:\n",
        "                            predictions[target_name] = 50.0  # Default score if DMatrix failed\n",
        "                            continue\n",
        "                        # For XGBoost booster\n",
        "                        predictions[target_name] = float(model.predict(X_dmatrix)[0])\n",
        "                    else:\n",
        "                        # For sklearn models\n",
        "                        predictions[target_name] = float(model.predict(X_scaled)[0])\n",
        "            except Exception as e:\n",
        "                print(f\"Error predicting {target_name}: {e}\")\n",
        "                # Set a default prediction value on error\n",
        "                if target_name == 'classifier':\n",
        "                    predictions['category'] = 'Average'\n",
        "                else:\n",
        "                    predictions[target_name] = 50.0\n",
        "\n",
        "        # Apply additional sentiment-based adjustment\n",
        "        # Penalize inappropriate responses more heavily based on sentiment\n",
        "        if 'feature_10' in data.columns:  # Check if sentiment feature exists\n",
        "            sentiment_score = data['feature_10'].values[0]\n",
        "            minimal_words = data['feature_0'].values[0] < 5  # Check if response is too short\n",
        "\n",
        "            # Detect extremely negative or inappropriate responses\n",
        "            # Lower sentiment with few words indicates potential inappropriate response\n",
        "            if minimal_words and sentiment_score < 0.4:\n",
        "                # Apply stronger penalty to all scores\n",
        "                penalty_factor = 0.3\n",
        "                for key in predictions:\n",
        "                    if key != 'category':  # Don't penalize the category\n",
        "                        predictions[key] = max(20, predictions[key] * penalty_factor)\n",
        "\n",
        "        # Ensure all scores are within valid range (0-100)\n",
        "        for key in predictions:\n",
        "            if key != 'category':  # Don't adjust the category\n",
        "                predictions[key] = max(0, min(100, predictions[key]))\n",
        "\n",
        "        # Determine category based on overall score if not already set or if predictions are very low\n",
        "        overall_score = predictions.get('overall', 0)\n",
        "        minimal_response = data['feature_0'].values[0] < 5  # Check if response is too short\n",
        "\n",
        "        # Override category based on overall score for very low scores or minimal responses\n",
        "        if minimal_response or overall_score < 30:\n",
        "            predictions['category'] = 'Weak'\n",
        "        elif 'category' not in predictions:\n",
        "            # Set category based on overall score\n",
        "            if overall_score >= 75:\n",
        "                predictions['category'] = 'Strong'\n",
        "            elif overall_score >= 40:\n",
        "                predictions['category'] = 'Average'\n",
        "            else:\n",
        "                predictions['category'] = 'Weak'\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in predict_with_models: {e}\")\n",
        "        return None\n",
        "\n",
        "def format_score(score):\n",
        "    \"\"\"Format a numerical score for display.\"\"\"\n",
        "    return f\"{score:.1f}/100\"\n",
        "\n",
        "def get_score_emoji(score):\n",
        "    \"\"\"Get an appropriate emoji for a score.\"\"\"\n",
        "    if score >= 80:\n",
        "        return \"🔥\"\n",
        "    elif score >= 60:\n",
        "        return \"👍\"\n",
        "    elif score >= 40:\n",
        "        return \"👌\"\n",
        "    else:\n",
        "        return \"🤔\"\n",
        "\n",
        "def get_category_description(category):\n",
        "    \"\"\"Get a description for a category.\"\"\"\n",
        "    if category == 'Strong':\n",
        "        return \"Excellent performance! You demonstrate strong skills and understanding.\"\n",
        "    elif category == 'Average':\n",
        "        return \"Solid performance. You show competence but could improve in some areas.\"\n",
        "    else:  # Weak\n",
        "        return \"You need to develop your skills further in this area.\"\n",
        "\n",
        "def display_results(predictions, question):\n",
        "    \"\"\"Display the evaluation results to the user.\"\"\"\n",
        "    clear_screen()\n",
        "\n",
        "    # Safely extract prediction values, handle both arrays and single values\n",
        "    def safe_extract(pred_dict, key):\n",
        "        if key not in pred_dict:\n",
        "            return 50.0 if key != 'category' else 'Average'\n",
        "\n",
        "        value = pred_dict[key]\n",
        "        if isinstance(value, (list, np.ndarray)) and len(value) > 0:\n",
        "            return value[0]\n",
        "        return value\n",
        "\n",
        "    # Get prediction values\n",
        "    technical = safe_extract(predictions, 'technical')\n",
        "    communication = safe_extract(predictions, 'communication')\n",
        "    problem_solving = safe_extract(predictions, 'problem_solving')\n",
        "    cultural_fit = safe_extract(predictions, 'cultural_fit')\n",
        "    overall = safe_extract(predictions, 'overall')\n",
        "    category = safe_extract(predictions, 'category')\n",
        "\n",
        "    # Make sure numerical values are floats\n",
        "    technical = float(technical) if isinstance(technical, (int, float, np.number)) else 50.0\n",
        "    communication = float(communication) if isinstance(communication, (int, float, np.number)) else 50.0\n",
        "    problem_solving = float(problem_solving) if isinstance(problem_solving, (int, float, np.number)) else 50.0\n",
        "    cultural_fit = float(cultural_fit) if isinstance(cultural_fit, (int, float, np.number)) else 50.0\n",
        "    overall = float(overall) if isinstance(overall, (int, float, np.number)) else 50.0\n",
        "\n",
        "    # Ensure category is a string\n",
        "    if isinstance(category, (list, np.ndarray)) and len(category) > 0:\n",
        "        category = category[0]\n",
        "    category = str(category) if category is not None else \"Average\"\n",
        "\n",
        "    typing_effect(\"\\n📊 INTERVIEW RESPONSE EVALUATION 📊\\n\", speed=0.01)\n",
        "    time.sleep(0.5)\n",
        "\n",
        "    print(f\"Question: {question}\\n\")\n",
        "\n",
        "    typing_effect(\"Your performance metrics:\", speed=0.02)\n",
        "    time.sleep(0.3)\n",
        "\n",
        "    # Display individual scores with emojis\n",
        "    print(f\"  Technical Proficiency:  {format_score(technical)} {get_score_emoji(technical)}\")\n",
        "    time.sleep(0.2)\n",
        "    print(f\"  Communication Skills:   {format_score(communication)} {get_score_emoji(communication)}\")\n",
        "    time.sleep(0.2)\n",
        "    print(f\"  Problem-Solving:        {format_score(problem_solving)} {get_score_emoji(problem_solving)}\")\n",
        "    time.sleep(0.2)\n",
        "    print(f\"  Cultural Fit:           {format_score(cultural_fit)} {get_score_emoji(cultural_fit)}\")\n",
        "    time.sleep(0.2)\n",
        "    print(f\"  Overall Score:          {format_score(overall)} {get_score_emoji(overall)}\")\n",
        "    time.sleep(0.2)\n",
        "\n",
        "    # Display category with description\n",
        "    print(\"\\nOverall Classification:\")\n",
        "    if category == \"Strong\":\n",
        "        typing_effect(f\"  ⭐ {category} ⭐\", speed=0.02)\n",
        "    elif category == \"Average\":\n",
        "        typing_effect(f\"  ✓ {category} ✓\", speed=0.02)\n",
        "    else:\n",
        "        typing_effect(f\"  △ {category} △\", speed=0.02)\n",
        "\n",
        "    print(f\"  {get_category_description(category)}\")\n",
        "\n",
        "    print(\"\\nAreas for improvement:\")\n",
        "    scores = {\n",
        "        \"Technical knowledge\": technical,\n",
        "        \"Communication clarity\": communication,\n",
        "        \"Problem-solving approach\": problem_solving,\n",
        "        \"Team and culture fit\": cultural_fit\n",
        "    }\n",
        "\n",
        "    # Sort scores to find weakest areas\n",
        "    sorted_scores = sorted(scores.items(), key=lambda x: x[1])\n",
        "\n",
        "    # Show suggestions for the two weakest areas\n",
        "    for area, score in sorted_scores[:2]:\n",
        "        print(f\"  • {area} ({format_score(score)})\")\n",
        "        if area == \"Technical knowledge\":\n",
        "            print(\"    Consider reviewing key concepts and practicing coding problems.\")\n",
        "        elif area == \"Communication clarity\":\n",
        "            print(\"    Work on articulating your thoughts more clearly and concisely.\")\n",
        "        elif area == \"Problem-solving approach\":\n",
        "            print(\"    Practice breaking down complex problems into smaller steps.\")\n",
        "        else:  # Cultural fit\n",
        "            print(\"    Focus on demonstrating teamwork and alignment with company values.\")\n",
        "\n",
        "def run_interview(max_questions=3, all_results=None, question_num=1):\n",
        "    \"\"\"Run the interactive interview process.\n",
        "\n",
        "    Args:\n",
        "        max_questions: Maximum number of questions to ask\n",
        "        all_results: List to store all predictions\n",
        "        question_num: Current question number\n",
        "    \"\"\"\n",
        "    # Initialize all_results if this is the first question\n",
        "    if all_results is None:\n",
        "        all_results = []\n",
        "\n",
        "    clear_screen()\n",
        "\n",
        "    # Only show the welcome message on the first question\n",
        "    if question_num == 1:\n",
        "        typing_effect(\"🤖 Welcome to the AI Interview Evaluation System! 🤖\", speed=0.02)\n",
        "        typing_effect(\"I'll ask you a few interview questions, analyze your responses,\", speed=0.02)\n",
        "        typing_effect(\"and provide feedback on your performance.\", speed=0.02)\n",
        "        typing_effect(f\"You will answer {max_questions} questions in total.\", speed=0.02)\n",
        "        print()\n",
        "\n",
        "    # List of interview questions\n",
        "    questions = [\n",
        "        \"Tell me about yourself and your experience in software engineering.\",\n",
        "        \"Describe a challenging project you worked on and how you overcame obstacles.\",\n",
        "        \"How do you approach debugging a complex issue in your code?\",\n",
        "        \"Explain how you would design a scalable web application architecture.\",\n",
        "        \"How do you stay updated with the latest technologies in your field?\",\n",
        "        \"Describe a situation where you had to work with a difficult team member.\",\n",
        "        \"What's your experience with agile development methodologies?\",\n",
        "        \"How would you optimize a slow-performing database query?\",\n",
        "        \"Tell me about a time when you had to learn a new technology quickly.\"\n",
        "    ]\n",
        "\n",
        "    # Randomly select a question\n",
        "    import random\n",
        "    question = random.choice(questions)\n",
        "\n",
        "    typing_effect(f\"\\nQuestion {question_num}/{max_questions}: {question}\", speed=0.03)\n",
        "    print(\"\\nYour response (type your answer and press Enter when done):\")\n",
        "\n",
        "    # Get user's response\n",
        "    response = input(\"> \")\n",
        "\n",
        "    if not response.strip():\n",
        "        typing_effect(\"I notice your response is empty. Let's try again with a more detailed answer.\", speed=0.03)\n",
        "        response = input(\"> \")\n",
        "\n",
        "    print(\"\\nAnalyzing your response...\")\n",
        "    time.sleep(1)\n",
        "\n",
        "    # Process the response\n",
        "    features = extract_features_from_text(response)\n",
        "\n",
        "    # Redirect stderr to hide error messages from the user\n",
        "    import sys\n",
        "    import io\n",
        "    original_stderr = sys.stderr\n",
        "    sys.stderr = io.StringIO()  # Redirect stderr to a string buffer\n",
        "\n",
        "    try:\n",
        "        # Make predictions\n",
        "        predictions = predict_response_scores(features)\n",
        "\n",
        "        # Store question and predictions for summary\n",
        "        result = {\n",
        "            'question': question,\n",
        "            'response': response,\n",
        "            'predictions': predictions\n",
        "        }\n",
        "        all_results.append(result)\n",
        "\n",
        "        # Display results\n",
        "        display_results(predictions, question)\n",
        "    finally:\n",
        "        # Restore stderr\n",
        "        sys.stderr = original_stderr\n",
        "\n",
        "    # Check if we've reached the maximum number of questions\n",
        "    if question_num < max_questions:\n",
        "        print(f\"\\nMoving on to question {question_num + 1} of {max_questions}...\")\n",
        "        print(\"Press Enter to continue.\")\n",
        "        input()\n",
        "        # Continue to next question\n",
        "        run_interview(max_questions, all_results, question_num + 1)\n",
        "    else:\n",
        "        # Show final summary after all questions\n",
        "        display_summary(all_results)\n",
        "        typing_effect(\"\\nThank you for completing the AI Interview Evaluation!\", speed=0.02)\n",
        "        typing_effect(\"Good luck with your interviews! 👋\", speed=0.02)\n",
        "\n",
        "def display_summary(all_results):\n",
        "    \"\"\"Display a summary of all interview responses.\n",
        "\n",
        "    Args:\n",
        "        all_results: List of dictionaries containing questions, responses, and predictions\n",
        "    \"\"\"\n",
        "    clear_screen()\n",
        "    typing_effect(\"\\n📊 INTERVIEW EVALUATION SUMMARY 📊\\n\", speed=0.01)\n",
        "    time.sleep(0.5)\n",
        "\n",
        "    typing_effect(f\"You answered {len(all_results)} questions in total.\\n\", speed=0.02)\n",
        "    time.sleep(0.3)\n",
        "\n",
        "    # Calculate average scores\n",
        "    avg_technical = 0\n",
        "    avg_communication = 0\n",
        "    avg_problem_solving = 0\n",
        "    avg_cultural_fit = 0\n",
        "    avg_overall = 0\n",
        "    categories = []\n",
        "\n",
        "    for i, result in enumerate(all_results):\n",
        "        predictions = result['predictions']\n",
        "\n",
        "        # Safely extract values\n",
        "        def safe_extract(pred_dict, key):\n",
        "            if key not in pred_dict:\n",
        "                return 50.0 if key != 'category' else 'Average'\n",
        "\n",
        "            value = pred_dict[key]\n",
        "            if isinstance(value, (list, np.ndarray)) and len(value) > 0:\n",
        "                return value[0]\n",
        "            return value\n",
        "\n",
        "        # Extract scores\n",
        "        technical = float(safe_extract(predictions, 'technical'))\n",
        "        communication = float(safe_extract(predictions, 'communication'))\n",
        "        problem_solving = float(safe_extract(predictions, 'problem_solving'))\n",
        "        cultural_fit = float(safe_extract(predictions, 'cultural_fit'))\n",
        "        overall = float(safe_extract(predictions, 'overall'))\n",
        "        category = str(safe_extract(predictions, 'category'))\n",
        "\n",
        "        # Add to averages\n",
        "        avg_technical += technical\n",
        "        avg_communication += communication\n",
        "        avg_problem_solving += problem_solving\n",
        "        avg_cultural_fit += cultural_fit\n",
        "        avg_overall += overall\n",
        "        categories.append(category)\n",
        "\n",
        "        # Display brief summary of each question\n",
        "        print(f\"Question {i+1}: {result['question'][:60]}...\")\n",
        "        print(f\"  Overall Score: {format_score(overall)} {get_score_emoji(overall)}\")\n",
        "        print(f\"  Classification: {category}\\n\")\n",
        "\n",
        "    # Calculate averages\n",
        "    num_results = len(all_results)\n",
        "    if num_results > 0:\n",
        "        avg_technical /= num_results\n",
        "        avg_communication /= num_results\n",
        "        avg_problem_solving /= num_results\n",
        "        avg_cultural_fit /= num_results\n",
        "        avg_overall /= num_results\n",
        "\n",
        "    # Determine overall category\n",
        "    category_values = {'Strong': 3, 'Average': 2, 'Weak': 1}\n",
        "    category_counts = {'Strong': 0, 'Average': 0, 'Weak': 0}\n",
        "\n",
        "    for cat in categories:\n",
        "        if cat in category_counts:\n",
        "            category_counts[cat] += 1\n",
        "\n",
        "    # Determine most frequent category\n",
        "    final_category = max(category_counts.items(), key=lambda x: x[1])[0]\n",
        "\n",
        "    # Display overall averages\n",
        "    typing_effect(\"\\n📈 OVERALL PERFORMANCE 📈\\n\", speed=0.01)\n",
        "    time.sleep(0.5)\n",
        "\n",
        "    print(f\"Technical Proficiency:  {format_score(avg_technical)} {get_score_emoji(avg_technical)}\")\n",
        "    print(f\"Communication Skills:   {format_score(avg_communication)} {get_score_emoji(avg_communication)}\")\n",
        "    print(f\"Problem-Solving:        {format_score(avg_problem_solving)} {get_score_emoji(avg_problem_solving)}\")\n",
        "    print(f\"Cultural Fit:           {format_score(avg_cultural_fit)} {get_score_emoji(avg_cultural_fit)}\")\n",
        "    print(f\"Overall Interview Score: {format_score(avg_overall)} {get_score_emoji(avg_overall)}\")\n",
        "\n",
        "    print(\"\\nFinal Classification:\")\n",
        "    if final_category == \"Strong\":\n",
        "        typing_effect(f\"  ⭐ {final_category} ⭐\", speed=0.02)\n",
        "    elif final_category == \"Average\":\n",
        "        typing_effect(f\"  ✓ {final_category} ✓\", speed=0.02)\n",
        "    else:\n",
        "        typing_effect(f\"  △ {final_category} △\", speed=0.02)\n",
        "\n",
        "    print(f\"  {get_category_description(final_category)}\")\n",
        "\n",
        "    # Career advice based on overall performance\n",
        "    print(\"\\nKey takeaways and advice:\")\n",
        "\n",
        "    # Find weakest and strongest areas\n",
        "    areas = {\n",
        "        \"Technical knowledge\": avg_technical,\n",
        "        \"Communication clarity\": avg_communication,\n",
        "        \"Problem-solving approach\": avg_problem_solving,\n",
        "        \"Team and culture fit\": avg_cultural_fit\n",
        "    }\n",
        "\n",
        "    sorted_areas = sorted(areas.items(), key=lambda x: x[1])\n",
        "    weakest_area = sorted_areas[0]\n",
        "    strongest_area = sorted_areas[-1]\n",
        "\n",
        "    print(f\"  • Your strongest area is {strongest_area[0]} ({format_score(strongest_area[1])}).\")\n",
        "    print(f\"  • Focus on improving {weakest_area[0]} ({format_score(weakest_area[1])}).\")\n",
        "\n",
        "    if avg_overall >= 80:\n",
        "        print(\"  • You show strong interview skills. Consider applying for more challenging positions.\")\n",
        "    elif avg_overall >= 60:\n",
        "        print(\"  • You have a good foundation. Practice more mock interviews to boost confidence.\")\n",
        "    else:\n",
        "        print(\"  • Continue practicing interview scenarios and focus on your weaker areas.\")\n",
        "\n",
        "    print(\"\\nRecommended next steps:\")\n",
        "    print(\"  1. Review and practice questions in your weaker areas\")\n",
        "    print(\"  2. Record yourself answering questions to analyze your communication\")\n",
        "    print(\"  3. Seek feedback from industry professionals if possible\")\n",
        "    print(\"  4. Consider another practice session focusing on your weakest topics\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_interview()"
      ],
      "metadata": {
        "id": "VczzgQaxjZ_Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7e651c6-92db-4bd9-ff13-3a6c6d8393a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading interview evaluation models...\n",
            "Models loaded successfully!\n",
            "🤖 Welcom"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [01:41:18] WARNING: /workspace/src/gbm/gbtree.cc:363: \n",
            "  Loading from a raw memory buffer (like pickle in Python, RDS in R) on a CPU-only\n",
            "  machine. Consider using `save_model/load_model` instead. See:\n",
            "\n",
            "    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
            "\n",
            "  for more details about differences between saving model and serializing.  Changing `tree_method` to `hist`.\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [01:41:18] WARNING: /workspace/src/gbm/gbtree.cc:388: Changing updater from `grow_gpu_hist` to `grow_quantile_histmaker`.\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [01:41:18] WARNING: /workspace/src/context.cc:43: No visible GPU is found, setting device to CPU.\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [01:41:18] WARNING: /workspace/src/context.cc:196: XGBoost is not compiled with CUDA support.\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "e to the AI Interview Evaluation System! 🤖\n",
            "I'll ask you a few interview questions, analyze your responses,\n",
            "and provide feedback on your performance.\n",
            "You will answer 3 questions in total.\n",
            "\n",
            "\n",
            "Question 1/3: Describe a situation where you had to work with a difficult team member.\n",
            "\n",
            "Your response (type your answer and press Enter when done):\n",
            "> i slapped them on the face\n",
            "\n",
            "Analyzing your response...\n",
            "Error predicting overall: training data did not have the following fields: feature_0, feature_1, feature_2, feature_3, feature_4, feature_5, feature_6, feature_7, feature_8, feature_9\n",
            "Error predicting classifier: training data did not have the following fields: feature_0, feature_1, feature_2, feature_3, feature_4, feature_5, feature_6, feature_7, feature_8, feature_9\n",
            "\n",
            "📊 INTERVIEW RESPONSE EVALUATION 📊\n",
            "\n",
            "Question: Describe a situation where you had to work with a difficult team member.\n",
            "\n",
            "Your performance metrics:\n",
            "  Technical Proficiency:  100.0/100 🔥\n",
            "  Communication Skills:   80.0/100 🔥\n",
            "  Problem-Solving:        51.4/100 👌\n",
            "  Cultural Fit:           51.0/100 👌\n",
            "  Overall Score:          50.0/100 👌\n",
            "\n",
            "Overall Classification:\n",
            "  ✓ Average ✓\n",
            "  Solid performance. You show competence but could improve in some areas.\n",
            "\n",
            "Areas for improvement:\n",
            "  • Team and culture fit (51.0/100)\n",
            "    Focus on demonstrating teamwork and alignment with company values.\n",
            "  • Problem-solving approach (51.4/100)\n",
            "    Practice breaking down complex problems into smaller steps.\n",
            "\n",
            "Moving on to question 2 of 3...\n",
            "Press Enter to continue.\n",
            "\n",
            "\n",
            "Question 2/3: Tell me about a time when you had to learn a new technology quickly.\n",
            "\n",
            "Your response (type your answer and press Enter when done):\n",
            "> i don't know\n",
            "\n",
            "Analyzing your response...\n",
            "Error predicting overall: training data did not have the following fields: feature_0, feature_1, feature_2, feature_3, feature_4, feature_5, feature_6, feature_7, feature_8, feature_9\n",
            "Error predicting classifier: training data did not have the following fields: feature_0, feature_1, feature_2, feature_3, feature_4, feature_5, feature_6, feature_7, feature_8, feature_9\n",
            "\n",
            "📊 INTERVIEW RESPONSE EVALUATION 📊\n",
            "\n",
            "Question: Tell me about a time when you had to learn a new technology quickly.\n",
            "\n",
            "Your performance metrics:\n",
            "  Technical Proficiency:  20.0/100 🤔\n",
            "  Communication Skills:   24.0/100 🤔\n",
            "  Problem-Solving:        20.0/100 🤔\n",
            "  Cultural Fit:           20.0/100 🤔\n",
            "  Overall Score:          20.0/100 🤔\n",
            "\n",
            "Overall Classification:\n",
            "  △ Weak △\n",
            "  You need to develop your skills further in this area.\n",
            "\n",
            "Areas for improvement:\n",
            "  • Technical knowledge (20.0/100)\n",
            "    Consider reviewing key concepts and practicing coding problems.\n",
            "  • Problem-solving approach (20.0/100)\n",
            "    Practice breaking down complex problems into smaller steps.\n",
            "\n",
            "Moving on to question 3 of 3...\n",
            "Press Enter to continue.\n",
            "\n",
            "\n",
            "Question 3/3: How do you approach debugging a complex issue in your code?\n",
            "\n",
            "Your response (type your answer and press Enter when done):\n",
            "> i simply solve it\n",
            "\n",
            "Analyzing your response...\n",
            "Error predicting overall: training data did not have the following fields: feature_0, feature_1, feature_2, feature_3, feature_4, feature_5, feature_6, feature_7, feature_8, feature_9\n",
            "Error predicting classifier: training data did not have the following fields: feature_0, feature_1, feature_2, feature_3, feature_4, feature_5, feature_6, feature_7, feature_8, feature_9\n",
            "\n",
            "📊 INTERVIEW RESPONSE EVALUATION 📊\n",
            "\n",
            "Question: How do you approach debugging a complex issue in your code?\n",
            "\n",
            "Your performance metrics:\n",
            "  Technical Proficiency:  64.8/100 👍\n",
            "  Communication Skills:   80.0/100 🔥\n",
            "  Problem-Solving:        51.5/100 👌\n",
            "  Cultural Fit:           51.0/100 👌\n",
            "  Overall Score:          50.0/100 👌\n",
            "\n",
            "Overall Classification:\n",
            "  △ Weak △\n",
            "  You need to develop your skills further in this area.\n",
            "\n",
            "Areas for improvement:\n",
            "  • Team and culture fit (51.0/100)\n",
            "    Focus on demonstrating teamwork and alignment with company values.\n",
            "  • Problem-solving approach (51.5/100)\n",
            "    Practice breaking down complex problems into smaller steps.\n",
            "\n",
            "📊 INTERVIEW EVALUATION SUMMARY 📊\n",
            "\n",
            "You answered 3 questions in total.\n",
            "\n",
            "Question 1: Describe a situation where you had to work with a difficult ...\n",
            "  Overall Score: 50.0/100 👌\n",
            "  Classification: Average\n",
            "\n",
            "Question 2: Tell me about a time when you had to learn a new technology ...\n",
            "  Overall Score: 20.0/100 🤔\n",
            "  Classification: Weak\n",
            "\n",
            "Question 3: How do you approach debugging a complex issue in your code?...\n",
            "  Overall Score: 50.0/100 👌\n",
            "  Classification: Weak\n",
            "\n",
            "\n",
            "📈 OVERALL PERFORMANCE 📈\n",
            "\n",
            "Technical Proficiency:  61.6/100 👍\n",
            "Communication Skills:   61.4/100 👍\n",
            "Problem-Solving:        41.0/100 👌\n",
            "Cultural Fit:           40.6/100 👌\n",
            "Overall Interview Score: 40.0/100 👌\n",
            "\n",
            "Final Classification:\n",
            "  △ Weak △\n",
            "  You need to develop your skills further in this area.\n",
            "\n",
            "Key takeaways and advice:\n",
            "  • Your strongest area is Technical knowledge (61.6/100).\n",
            "  • Focus on improving Team and culture fit (40.6/100).\n",
            "  • Continue practicing interview scenarios and focus on your weaker areas.\n",
            "\n",
            "Recommended next steps:\n",
            "  1. Review and practice questions in your weaker areas\n",
            "  2. Record yourself answering questions to analyze your communication\n",
            "  3. Seek feedback from industry professionals if possible\n",
            "  4. Consider another practice session focusing on your weakest topics\n",
            "\n",
            "Thank you for completing the AI Interview Evaluation!\n",
            "Good luck with your interviews! 👋\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 9: Colab Utilities"
      ],
      "metadata": {
        "id": "Hi5QJOkdjd4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============== SECTION 9: COLAB UTILITIES ==============\n",
        "# These utilities are helpful when running in Google Colab\n",
        "\n",
        "def colab_utils_setup():\n",
        "    \"\"\"Set up utilities for Google Colab environment\"\"\"\n",
        "    if not USING_COLAB:\n",
        "        print(\"Not running in Google Colab, skipping utilities setup.\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        from google.colab import files\n",
        "        import matplotlib.pyplot as plt\n",
        "        from IPython.display import display, HTML\n",
        "\n",
        "        # Define utility functions for Colab\n",
        "        def download_file(filename):\n",
        "            \"\"\"Trigger file download in Colab\"\"\"\n",
        "            try:\n",
        "                files.download(filename)\n",
        "                print(f\"Download initiated for {filename}\")\n",
        "                return True\n",
        "            except Exception as e:\n",
        "                print(f\"Error downloading file: {e}\")\n",
        "                return False\n",
        "\n",
        "        def upload_file():\n",
        "            \"\"\"Handle file upload in Colab\"\"\"\n",
        "            try:\n",
        "                uploaded = files.upload()\n",
        "                print(f\"Uploaded {len(uploaded)} files\")\n",
        "                return list(uploaded.keys())\n",
        "            except Exception as e:\n",
        "                print(f\"Error uploading files: {e}\")\n",
        "                return []\n",
        "\n",
        "        def display_html(html_content):\n",
        "            \"\"\"Display rich HTML in Colab\"\"\"\n",
        "            display(HTML(html_content))\n",
        "\n",
        "        def display_evaluation_chart(results):\n",
        "            \"\"\"Display a radar chart for interview evaluation\"\"\"\n",
        "            categories = ['Technical', 'Communication', 'Problem Solving', 'Cultural Fit']\n",
        "            values = [\n",
        "                results.get('technical_score', 0)/10,\n",
        "                results.get('communication_score', 0)/10,\n",
        "                results.get('problem_solving', 0)/10,\n",
        "                results.get('cultural_fit', 0)/10\n",
        "            ]\n",
        "\n",
        "            # Create radar chart\n",
        "            fig = plt.figure(figsize=(10, 6))\n",
        "            ax = fig.add_subplot(111, polar=True)\n",
        "\n",
        "            # Set the angles for each category\n",
        "            angles = np.linspace(0, 2*np.pi, len(categories), endpoint=False).tolist()\n",
        "            # Make the plot circular\n",
        "            values = values + [values[0]]\n",
        "            angles = angles + [angles[0]]\n",
        "            categories = categories + [categories[0]]\n",
        "\n",
        "            # Plot and fill\n",
        "            ax.plot(angles, values, 'o-', linewidth=2)\n",
        "            ax.fill(angles, values, alpha=0.25)\n",
        "\n",
        "            # Set category labels\n",
        "            ax.set_thetagrids(np.degrees(angles[:-1]), categories[:-1])\n",
        "\n",
        "            # Set radial ticks and labels\n",
        "            ax.set_rlabel_position(0)\n",
        "            ax.set_rticks([0, 2.5, 5, 7.5, 10])\n",
        "            ax.set_rlim(0, 10)\n",
        "\n",
        "            # Add title\n",
        "            recommendation = results.get('recommendation', 'No recommendation')\n",
        "            plt.title(f\"Interview Evaluation\\nOverall Score: {results.get('overall_score', 0)/10:.1f}/10 - {recommendation}\",\n",
        "                    size=15, color='blue', y=1.1)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "        # Add these functions to globals\n",
        "        globals()['download_file'] = download_file\n",
        "        globals()['upload_file'] = upload_file\n",
        "        globals()['display_html'] = display_html\n",
        "        globals()['display_evaluation_chart'] = display_evaluation_chart\n",
        "\n",
        "        print(\"✅ Colab utilities successfully configured\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error setting up Colab utilities: {e}\")\n",
        "        return False\n",
        "\n",
        "# Set up Colab utilities if we're in Colab\n",
        "if USING_COLAB:\n",
        "    colab_utils_setup()\n",
        "\n",
        "    # Modify the run_interview function to use Colab utilities when available\n",
        "    original_run_interview = run_interview\n",
        "\n",
        "    def enhanced_colab_run_interview():\n",
        "        \"\"\"Enhanced version of run_interview with Colab-specific features\"\"\"\n",
        "        results = original_run_interview()\n",
        "\n",
        "        # If we have evaluation results and the display function exists\n",
        "        if results and 'display_evaluation_chart' in globals():\n",
        "            print(\"\\nGenerating visual evaluation chart...\")\n",
        "            display_evaluation_chart(results)\n",
        "\n",
        "        return results\n",
        "\n",
        "    # Replace the original function with our enhanced version\n",
        "    run_interview = enhanced_colab_run_interview\n",
        "\n",
        "# Function to analyze and visualize the datasets in Colab\n",
        "def explore_datasets():\n",
        "    \"\"\"Analyze and visualize interview questions and evaluations datasets in Colab\"\"\"\n",
        "    if not USING_COLAB:\n",
        "        print(\"This function is designed for Google Colab environment\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # Import visualization libraries\n",
        "        import matplotlib.pyplot as plt\n",
        "        import seaborn as sns\n",
        "        sns.set_style(\"whitegrid\")\n",
        "\n",
        "        # Load datasets\n",
        "        print(\"Loading datasets...\")\n",
        "        questions_df = pd.read_csv(\"interview_questions.csv\")\n",
        "        evaluations_df = pd.read_csv(\"evaluation_scores.csv\")\n",
        "\n",
        "        print(f\"Interview Questions: {len(questions_df)} records\")\n",
        "        print(f\"Evaluation Scores: {len(evaluations_df)} records\")\n",
        "\n",
        "        # Display dataset heads\n",
        "        print(\"\\n--- Interview Questions Sample ---\")\n",
        "        display(questions_df.head(3))\n",
        "\n",
        "        print(\"\\n--- Evaluation Scores Sample ---\")\n",
        "        display(evaluations_df.head(3))\n",
        "\n",
        "        # Visualize question distributions\n",
        "        plt.figure(figsize=(16, 10))\n",
        "\n",
        "        # Plot profession distribution\n",
        "        plt.subplot(2, 2, 1)\n",
        "        profession_counts = questions_df['profession'].value_counts()\n",
        "        sns.barplot(x=profession_counts.index, y=profession_counts.values)\n",
        "        plt.title('Distribution of Professions')\n",
        "        plt.xticks(rotation=90)\n",
        "        plt.ylabel('Count')\n",
        "\n",
        "        # Plot question type distribution\n",
        "        plt.subplot(2, 2, 2)\n",
        "        q_type_counts = questions_df['question_type'].value_counts()\n",
        "        sns.barplot(x=q_type_counts.index, y=q_type_counts.values)\n",
        "        plt.title('Distribution of Question Types')\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.ylabel('Count')\n",
        "\n",
        "        # Plot difficulty level distribution\n",
        "        plt.subplot(2, 2, 3)\n",
        "        difficulty_counts = questions_df['difficulty_level'].value_counts()\n",
        "        sns.barplot(x=difficulty_counts.index, y=difficulty_counts.values)\n",
        "        plt.title('Distribution of Difficulty Levels')\n",
        "        plt.ylabel('Count')\n",
        "\n",
        "        # Histogram of evaluation scores\n",
        "        plt.subplot(2, 2, 4)\n",
        "        sns.histplot(evaluations_df['score'], bins=20, kde=True)\n",
        "        plt.title('Distribution of Evaluation Scores')\n",
        "        plt.xlabel('Score')\n",
        "        plt.ylabel('Frequency')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Show correlation heatmap for merged data\n",
        "        print(\"\\nMerging datasets for correlation analysis...\")\n",
        "        merged_df = pd.merge(evaluations_df, questions_df, on='question_id')\n",
        "\n",
        "        # Convert categorical variables to numeric for correlation\n",
        "        merged_df = pd.get_dummies(merged_df, columns=['profession', 'question_type', 'difficulty_level'])\n",
        "        merged_df['evaluation_date'] = pd.to_datetime(merged_df['evaluation_date'])\n",
        "        merged_df['month'] = merged_df['evaluation_date'].dt.month\n",
        "        merged_df['year'] = merged_df['evaluation_date'].dt.year\n",
        "\n",
        "        # Select numeric columns for correlation\n",
        "        numeric_cols = merged_df.select_dtypes(include=['number']).columns\n",
        "\n",
        "        # Plot correlation heatmap\n",
        "        plt.figure(figsize=(12, 10))\n",
        "        corr_matrix = merged_df[numeric_cols].corr()\n",
        "        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
        "        sns.heatmap(corr_matrix, mask=mask, annot=False, cmap='coolwarm', center=0)\n",
        "        plt.title('Correlation Matrix of Dataset Features')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Interactive plot for score distribution by profession\n",
        "        plt.figure(figsize=(14, 6))\n",
        "        sns.boxplot(x='profession', y='score', data=merged_df)\n",
        "        plt.title('Score Distribution by Profession')\n",
        "        plt.xticks(rotation=90)\n",
        "        plt.show()\n",
        "\n",
        "        # Interactive plot for score distribution by question type\n",
        "        plt.figure(figsize=(14, 6))\n",
        "        sns.boxplot(x='question_type', y='score', data=merged_df)\n",
        "        plt.title('Score Distribution by Question Type')\n",
        "        plt.show()\n",
        "\n",
        "        # Interactive plot for score distribution by difficulty level\n",
        "        plt.figure(figsize=(14, 6))\n",
        "        sns.boxplot(x='difficulty_level', y='score', data=merged_df)\n",
        "        plt.title('Score Distribution by Difficulty Level')\n",
        "        plt.show()\n",
        "\n",
        "        print(\"Datasets exploration complete!\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"Error: Dataset files not found. Make sure you've generated the datasets first.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error exploring datasets: {e}\")\n",
        "\n",
        "# Add this function to Colab utilities if we're in Colab\n",
        "if USING_COLAB and 'colab_utils_setup' in globals():\n",
        "    globals()['explore_datasets'] = explore_datasets\n",
        "    print(\"✅ Dataset exploration utility added. Call explore_datasets() to analyze your datasets.\")"
      ],
      "metadata": {
        "id": "HIVzUG2xjghv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 10: Extra UI"
      ],
      "metadata": {
        "id": "eqZQ3a3OOaBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import display, HTML, clear_output\n",
        "import random\n",
        "import sys\n",
        "import os\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Check if running in Google Colab\n",
        "USING_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "# No need to import functions from section_8 as they should be available\n",
        "# in the global namespace when running as cells in Colab\n",
        "\n",
        "# Define fallback functions only if they don't exist in the global namespace\n",
        "if 'format_score' not in globals():\n",
        "    def format_score(score):\n",
        "        return f\"{score:.1f}/10\"\n",
        "\n",
        "if 'get_score_emoji' not in globals():\n",
        "    def get_score_emoji(score):\n",
        "        if score >= 80: return \"🌟\"\n",
        "        elif score >= 60: return \"✅\"\n",
        "        elif score >= 40: return \"⚠️\"\n",
        "        else: return \"❌\"\n",
        "\n",
        "if 'get_category_description' not in globals():\n",
        "    def get_category_description(category):\n",
        "        descriptions = {\n",
        "            \"Expert\": \"You demonstrate exceptional knowledge and communication skills.\",\n",
        "            \"Proficient\": \"You show strong understanding and good communication.\",\n",
        "            \"Competent\": \"You have a solid foundation but could improve in some areas.\",\n",
        "            \"Developing\": \"You show some understanding but need significant improvement.\",\n",
        "            \"Novice\": \"You need to develop fundamental knowledge and skills in this area.\",\n",
        "            \"Average\": \"Your response shows average competency across all dimensions.\"\n",
        "        }\n",
        "        return descriptions.get(category, \"Your response received a standard evaluation.\")\n",
        "\n",
        "if 'clear_screen' not in globals():\n",
        "    def clear_screen():\n",
        "        if not USING_COLAB:\n",
        "            os.system('cls' if os.name == 'nt' else 'clear')\n",
        "\n",
        "# ============================\n",
        "# COLAB UTILITIES\n",
        "# ============================\n",
        "\n",
        "def setup_colab_environment():\n",
        "    \"\"\"Set up the Google Colab environment with necessary libraries and styling.\"\"\"\n",
        "    if not USING_COLAB:\n",
        "        print(\"Not running in Google Colab. Skipping environment setup.\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        from google.colab import files\n",
        "\n",
        "        # Set custom styling for better UI\n",
        "        custom_css = \"\"\"\n",
        "        <style>\n",
        "            .interview-header {\n",
        "                background-color: #4285F4;\n",
        "                color: white;\n",
        "                padding: 15px;\n",
        "                border-radius: 5px;\n",
        "                margin-bottom: 20px;\n",
        "                text-align: center;\n",
        "                font-size: 24px;\n",
        "                font-weight: bold;\n",
        "            }\n",
        "\n",
        "            .question-box {\n",
        "                background-color: #f8f9fa;\n",
        "                border-left: 5px solid #4285F4;\n",
        "                padding: 10px 15px;\n",
        "                margin: 20px 0;\n",
        "                border-radius: 0 5px 5px 0;\n",
        "                font-size: 18px;\n",
        "            }\n",
        "\n",
        "            .response-box {\n",
        "                background-color: #e8f0fe;\n",
        "                border: 1px solid #4285F4;\n",
        "                padding: 15px;\n",
        "                margin: 15px 0;\n",
        "                border-radius: 5px;\n",
        "                min-height: 100px;\n",
        "            }\n",
        "\n",
        "            .result-card {\n",
        "                background-color: white;\n",
        "                border: 1px solid #dadce0;\n",
        "                border-radius: 8px;\n",
        "                padding: 20px;\n",
        "                margin: 20px 0;\n",
        "                box-shadow: 0 1px 2px rgba(60,64,67,0.3);\n",
        "            }\n",
        "\n",
        "            .metric-row {\n",
        "                display: flex;\n",
        "                justify-content: space-between;\n",
        "                margin: 10px 0;\n",
        "                padding: 5px 0;\n",
        "                border-bottom: 1px solid #f1f3f4;\n",
        "            }\n",
        "\n",
        "            .metric-label {\n",
        "                font-weight: bold;\n",
        "                color: #4285F4;\n",
        "            }\n",
        "\n",
        "            .progress-container {\n",
        "                width: 100%;\n",
        "                background-color: #e0e0e0;\n",
        "                border-radius: 5px;\n",
        "                margin: 5px 0;\n",
        "            }\n",
        "\n",
        "            .progress-bar {\n",
        "                height: 20px;\n",
        "                border-radius: 5px;\n",
        "                text-align: center;\n",
        "                color: white;\n",
        "                font-weight: bold;\n",
        "            }\n",
        "\n",
        "            .feedback-box {\n",
        "                background-color: #f1f3f4;\n",
        "                border-left: 5px solid #fbbc04;\n",
        "                padding: 15px;\n",
        "                margin-top: 20px;\n",
        "                border-radius: 0 5px 5px 0;\n",
        "            }\n",
        "\n",
        "            .summary-title {\n",
        "                text-align: center;\n",
        "                font-size: 22px;\n",
        "                font-weight: bold;\n",
        "                margin: 20px 0;\n",
        "                color: #4285F4;\n",
        "                border-bottom: 2px solid #4285F4;\n",
        "                padding-bottom: 10px;\n",
        "            }\n",
        "\n",
        "            .recommendation {\n",
        "                background-color: #e6f4ea;\n",
        "                border-left: 5px solid #34a853;\n",
        "                padding: 15px;\n",
        "                margin: 15px 0;\n",
        "                border-radius: 0 5px 5px 0;\n",
        "            }\n",
        "\n",
        "            .warning {\n",
        "                background-color: #fef7e0;\n",
        "                border-left: 5px solid #fbbc04;\n",
        "                padding: 15px;\n",
        "                margin: 15px 0;\n",
        "                border-radius: 0 5px 5px 0;\n",
        "            }\n",
        "\n",
        "            button {\n",
        "                background-color: #4285F4;\n",
        "                color: white;\n",
        "                border: none;\n",
        "                padding: 10px 15px;\n",
        "                border-radius: 5px;\n",
        "                cursor: pointer;\n",
        "                font-size: 16px;\n",
        "                margin: 10px 5px;\n",
        "            }\n",
        "\n",
        "            button:hover {\n",
        "                background-color: #3367d6;\n",
        "            }\n",
        "        </style>\n",
        "        \"\"\"\n",
        "\n",
        "        # Display the custom CSS\n",
        "        display(HTML(custom_css))\n",
        "\n",
        "        print(\"✅ Colab environment setup complete\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error setting up Colab environment: {e}\")\n",
        "        return False\n",
        "\n",
        "def display_header(title):\n",
        "    \"\"\"Display a styled header in Colab.\"\"\"\n",
        "    if not USING_COLAB:\n",
        "        print(f\"\\n{title}\\n{'=' * len(title)}\")\n",
        "        return\n",
        "\n",
        "    header_html = f'<div class=\"interview-header\">{title}</div>'\n",
        "    display(HTML(header_html))\n",
        "\n",
        "def display_question(question_num, total_questions, question_text):\n",
        "    \"\"\"Display a formatted interview question.\"\"\"\n",
        "    if not USING_COLAB:\n",
        "        print(f\"\\nQuestion {question_num}/{total_questions}: {question_text}\")\n",
        "        return\n",
        "\n",
        "    question_html = f'''\n",
        "    <div class=\"question-box\">\n",
        "        <strong>Question {question_num}/{total_questions}:</strong><br>\n",
        "        {question_text}\n",
        "    </div>\n",
        "    '''\n",
        "    display(HTML(question_html))\n",
        "\n",
        "def create_response_area():\n",
        "    \"\"\"Create an interactive response area for the user.\"\"\"\n",
        "    if not USING_COLAB:\n",
        "        print(\"\\nPlease type your response below:\")\n",
        "        return\n",
        "\n",
        "    response_html = '''\n",
        "    <div class=\"response-box\" id=\"response-area\" contenteditable=\"true\">\n",
        "        Type your interview response here...\n",
        "    </div>\n",
        "\n",
        "    <button onclick=\"submitResponse()\">Submit Response</button>\n",
        "\n",
        "    <script>\n",
        "    // Define a function to submit the response\n",
        "    function submitResponse() {\n",
        "        var responseText = document.getElementById('response-area').innerText;\n",
        "        if (responseText.trim() === \"\" || responseText === \"Type your interview response here...\") {\n",
        "            alert(\"Please provide a response before submitting.\");\n",
        "            return;\n",
        "        }\n",
        "\n",
        "        // Send the response back to Python\n",
        "        google.colab.kernel.invokeFunction('notebook.getResponse', [responseText], {});\n",
        "\n",
        "        // Disable editing after submission\n",
        "        document.getElementById('response-area').contentEditable = false;\n",
        "        document.getElementById('response-area').style.backgroundColor = \"#f1f3f4\";\n",
        "\n",
        "        // Change button text to show processing\n",
        "        var buttons = document.getElementsByTagName('button');\n",
        "        for (var i = 0; i < buttons.length; i++) {\n",
        "            buttons[i].disabled = true;\n",
        "            buttons[i].innerText = \"Processing...\";\n",
        "        }\n",
        "    }\n",
        "    </script>\n",
        "    '''\n",
        "    display(HTML(response_html))\n",
        "\n",
        "    # Register the callback function to get the response\n",
        "    from google.colab import output\n",
        "\n",
        "    response_text = [None]  # Using a list to store the value that will be modified by the callback\n",
        "\n",
        "    def get_response_callback(response):\n",
        "        response_text[0] = response\n",
        "\n",
        "    output.register_callback('notebook.getResponse', get_response_callback)\n",
        "\n",
        "    # Wait for the response\n",
        "    while response_text[0] is None:\n",
        "        time.sleep(0.5)\n",
        "\n",
        "    return response_text[0]\n",
        "\n",
        "def display_processing_animation():\n",
        "    \"\"\"Display an animation while processing the response.\"\"\"\n",
        "    if not USING_COLAB:\n",
        "        print(\"\\nAnalyzing your response...\")\n",
        "        return\n",
        "\n",
        "    processing_html = '''\n",
        "    <div style=\"text-align: center; margin: 20px 0;\">\n",
        "        <p>Analyzing your response...</p>\n",
        "        <div style=\"display: inline-block; position: relative; width: 80px; height: 80px;\">\n",
        "            <div style=\"position: absolute; border: 4px solid #4285F4; opacity: 1; border-radius: 50%; animation: ripple 1s cubic-bezier(0, 0.2, 0.8, 1) infinite;\" ></div>\n",
        "            <div style=\"position: absolute; border: 4px solid #4285F4; opacity: 1; border-radius: 50%; animation: ripple 1s cubic-bezier(0, 0.2, 0.8, 1) infinite; animation-delay: -0.5s;\" ></div>\n",
        "        </div>\n",
        "    </div>\n",
        "\n",
        "    <style>\n",
        "    @keyframes ripple {\n",
        "      0% {\n",
        "        top: 36px;\n",
        "        left: 36px;\n",
        "        width: 0;\n",
        "        height: 0;\n",
        "        opacity: 1;\n",
        "      }\n",
        "      100% {\n",
        "        top: 0px;\n",
        "        left: 0px;\n",
        "        width: 72px;\n",
        "        height: 72px;\n",
        "        opacity: 0;\n",
        "      }\n",
        "    }\n",
        "    </style>\n",
        "    '''\n",
        "    display(HTML(processing_html))\n",
        "    time.sleep(2)  # Simulate processing time\n",
        "\n",
        "def display_score_bar(score, label, color):\n",
        "    \"\"\"Display a score as a progress bar.\"\"\"\n",
        "    if not USING_COLAB:\n",
        "        print(f\"{label}: {format_score(score)} {get_score_emoji(score)}\")\n",
        "        return\n",
        "\n",
        "    # Ensure score is between 0 and 100\n",
        "    score = max(0, min(100, score))\n",
        "\n",
        "    # Determine bar color based on score\n",
        "    if color is None:\n",
        "        if score >= 80:\n",
        "            color = \"#34a853\"  # Green\n",
        "        elif score >= 60:\n",
        "            color = \"#fbbc04\"  # Yellow\n",
        "        elif score >= 40:\n",
        "            color = \"#fa7b17\"  # Orange\n",
        "        else:\n",
        "            color = \"#ea4335\"  # Red\n",
        "\n",
        "    bar_html = f'''\n",
        "    <div class=\"metric-row\">\n",
        "        <span class=\"metric-label\">{label}:</span>\n",
        "        <span>{format_score(score)} {get_score_emoji(score)}</span>\n",
        "    </div>\n",
        "    <div class=\"progress-container\">\n",
        "        <div class=\"progress-bar\" style=\"width: {score}%; background-color: {color};\">{score}%</div>\n",
        "    </div>\n",
        "    '''\n",
        "    return bar_html\n",
        "\n",
        "def display_results_card(predictions, question=None):\n",
        "    \"\"\"Display evaluation results in a visually appealing card format.\"\"\"\n",
        "    if not USING_COLAB:\n",
        "        # Fall back to the original display_results function\n",
        "        if 'display_results' in globals():\n",
        "            display_results(predictions, question)\n",
        "        else:\n",
        "            print(\"\\nEvaluation Results:\")\n",
        "            print(f\"Technical: {format_score(predictions.get('technical', 50))}\")\n",
        "            print(f\"Communication: {format_score(predictions.get('communication', 50))}\")\n",
        "            print(f\"Problem Solving: {format_score(predictions.get('problem_solving', 50))}\")\n",
        "            print(f\"Cultural Fit: {format_score(predictions.get('cultural_fit', 50))}\")\n",
        "            print(f\"Overall: {format_score(predictions.get('overall', 50))}\")\n",
        "        return\n",
        "\n",
        "    # Safely extract prediction values\n",
        "    def safe_extract(pred_dict, key):\n",
        "        if key not in pred_dict:\n",
        "            return 50.0 if key != 'category' else 'Average'\n",
        "\n",
        "        value = pred_dict[key]\n",
        "        if isinstance(value, (list, np.ndarray)) and len(value) > 0:\n",
        "            return value[0]\n",
        "        return value\n",
        "\n",
        "    # Get prediction values\n",
        "    technical = float(safe_extract(predictions, 'technical'))\n",
        "    communication = float(safe_extract(predictions, 'communication'))\n",
        "    problem_solving = float(safe_extract(predictions, 'problem_solving'))\n",
        "    cultural_fit = float(safe_extract(predictions, 'cultural_fit'))\n",
        "    overall = float(safe_extract(predictions, 'overall'))\n",
        "    category = str(safe_extract(predictions, 'category'))\n",
        "\n",
        "    # Create the results card HTML\n",
        "    results_html = f'''\n",
        "    <div class=\"result-card\">\n",
        "        <h2 style=\"text-align: center; color: #4285F4; margin-top: 0;\">Response Evaluation</h2>\n",
        "\n",
        "        {display_score_bar(technical, \"Technical Proficiency\", \"#4285F4\")}\n",
        "        {display_score_bar(communication, \"Communication Skills\", \"#34a853\")}\n",
        "        {display_score_bar(problem_solving, \"Problem-Solving Approach\", \"#fbbc04\")}\n",
        "        {display_score_bar(cultural_fit, \"Team & Culture Fit\", \"#ea4335\")}\n",
        "\n",
        "        <div style=\"border-top: 2px solid #f1f3f4; margin: 15px 0; padding-top: 15px;\">\n",
        "            {display_score_bar(overall, \"Overall Performance\", None)}\n",
        "        </div>\n",
        "\n",
        "        <div class=\"feedback-box\">\n",
        "            <strong>Performance Category:</strong> {category}<br>\n",
        "            <p>{get_category_description(category)}</p>\n",
        "        </div>\n",
        "    </div>\n",
        "    '''\n",
        "\n",
        "    display(HTML(results_html))\n",
        "\n",
        "def plot_radar_chart(predictions):\n",
        "    \"\"\"Create and display a radar chart of the evaluation metrics.\"\"\"\n",
        "    if not USING_COLAB:\n",
        "        return\n",
        "\n",
        "    # Extract scores\n",
        "    def safe_extract(pred_dict, key):\n",
        "        if key not in pred_dict:\n",
        "            return 50.0\n",
        "\n",
        "        value = pred_dict[key]\n",
        "        if isinstance(value, (list, np.ndarray)) and len(value) > 0:\n",
        "            return value[0]\n",
        "        return value\n",
        "\n",
        "    # Get values and normalize to 0-10 scale\n",
        "    technical = float(safe_extract(predictions, 'technical')) / 10\n",
        "    communication = float(safe_extract(predictions, 'communication')) / 10\n",
        "    problem_solving = float(safe_extract(predictions, 'problem_solving')) / 10\n",
        "    cultural_fit = float(safe_extract(predictions, 'cultural_fit')) / 10\n",
        "    overall = float(safe_extract(predictions, 'overall')) / 10\n",
        "\n",
        "    # Categories for radar chart\n",
        "    categories = ['Technical\\nProficiency', 'Communication\\nSkills',\n",
        "                  'Problem-Solving\\nApproach', 'Team &\\nCulture Fit']\n",
        "\n",
        "    values = [technical, communication, problem_solving, cultural_fit]\n",
        "\n",
        "    # Set up the radar chart\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    # Set background color\n",
        "    plt.rcParams['axes.facecolor'] = '#f8f9fa'\n",
        "\n",
        "    # Create angles for each category\n",
        "    angles = np.linspace(0, 2*np.pi, len(categories), endpoint=False).tolist()\n",
        "\n",
        "    # Make the plot circular\n",
        "    values = values + [values[0]]\n",
        "    angles = angles + [angles[0]]\n",
        "    categories = categories + [categories[0]]\n",
        "\n",
        "    # Create the plot\n",
        "    ax = plt.subplot(111, polar=True)\n",
        "\n",
        "    # Plot and fill the polygon\n",
        "    ax.plot(angles, values, 'o-', linewidth=2, color='#4285F4', label='Your Scores')\n",
        "    ax.fill(angles, values, alpha=0.25, color='#4285F4')\n",
        "\n",
        "    # Add a reference circle at score 7.5 (75%)\n",
        "    reference_values = [7.5] * (len(categories))\n",
        "    ax.plot(angles, reference_values, '--', linewidth=1, color='#34a853', alpha=0.5, label='Expert Level (7.5)')\n",
        "\n",
        "    # Set category labels\n",
        "    ax.set_thetagrids(np.degrees(angles[:-1]), categories[:-1], fontsize=12)\n",
        "\n",
        "    # Set radial ticks and labels\n",
        "    ax.set_rlabel_position(0)\n",
        "    ax.set_rticks([2.5, 5, 7.5, 10])\n",
        "    ax.set_rlim(0, 10)\n",
        "\n",
        "    # Add title and legend\n",
        "    plt.title(f\"Interview Performance Metrics\\nOverall Score: {overall:.1f}/10\",\n",
        "              size=16, color='#4285F4', y=1.1)\n",
        "\n",
        "    plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def display_summary_dashboard(all_results):\n",
        "    \"\"\"Display a comprehensive summary dashboard of all interview responses.\"\"\"\n",
        "    if not USING_COLAB:\n",
        "        return\n",
        "\n",
        "    # Calculate average scores\n",
        "    avg_technical = 0\n",
        "    avg_communication = 0\n",
        "    avg_problem_solving = 0\n",
        "    avg_cultural_fit = 0\n",
        "    avg_overall = 0\n",
        "    categories = []\n",
        "\n",
        "    for result in all_results:\n",
        "        predictions = result['predictions']\n",
        "\n",
        "        # Safely extract values\n",
        "        def safe_extract(pred_dict, key):\n",
        "            if key not in pred_dict:\n",
        "                return 50.0 if key != 'category' else 'Average'\n",
        "\n",
        "            value = pred_dict[key]\n",
        "            if isinstance(value, (list, np.ndarray)) and len(value) > 0:\n",
        "                return value[0]\n",
        "            return value\n",
        "\n",
        "        # Extract scores\n",
        "        technical = float(safe_extract(predictions, 'technical'))\n",
        "        communication = float(safe_extract(predictions, 'communication'))\n",
        "        problem_solving = float(safe_extract(predictions, 'problem_solving'))\n",
        "        cultural_fit = float(safe_extract(predictions, 'cultural_fit'))\n",
        "        overall = float(safe_extract(predictions, 'overall'))\n",
        "        category = str(safe_extract(predictions, 'category'))\n",
        "\n",
        "        # Add to totals\n",
        "        avg_technical += technical\n",
        "        avg_communication += communication\n",
        "        avg_problem_solving += problem_solving\n",
        "        avg_cultural_fit += cultural_fit\n",
        "        avg_overall += overall\n",
        "        categories.append(category)\n",
        "\n",
        "    # Calculate averages\n",
        "    n = len(all_results)\n",
        "    avg_technical /= n\n",
        "    avg_communication /= n\n",
        "    avg_problem_solving /= n\n",
        "    avg_cultural_fit /= n\n",
        "    avg_overall /= n\n",
        "\n",
        "    # Display summary header\n",
        "    display_header(\"Interview Performance Summary\")\n",
        "\n",
        "    # Create summary HTML\n",
        "    summary_html = f'''\n",
        "    <div class=\"result-card\">\n",
        "        <div class=\"summary-title\">Overall Performance</div>\n",
        "\n",
        "        {display_score_bar(avg_overall, \"Average Overall Score\", None)}\n",
        "\n",
        "        <div style=\"margin-top: 30px;\">\n",
        "            {display_score_bar(avg_technical, \"Technical Proficiency\", \"#4285F4\")}\n",
        "            {display_score_bar(avg_communication, \"Communication Skills\", \"#34a853\")}\n",
        "            {display_score_bar(avg_problem_solving, \"Problem-Solving Approach\", \"#fbbc04\")}\n",
        "            {display_score_bar(avg_cultural_fit, \"Team & Culture Fit\", \"#ea4335\")}\n",
        "        </div>\n",
        "    </div>\n",
        "    '''\n",
        "\n",
        "    display(HTML(summary_html))\n",
        "\n",
        "    # Create a bar chart comparing areas\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    metrics = ['Technical', 'Communication', 'Problem Solving', 'Cultural Fit', 'Overall']\n",
        "    values = [avg_technical, avg_communication, avg_problem_solving, avg_cultural_fit, avg_overall]\n",
        "    colors = ['#4285F4', '#34a853', '#fbbc04', '#ea4335', '#673ab7']\n",
        "\n",
        "    bars = plt.bar(metrics, values, color=colors)\n",
        "\n",
        "    # Add values on top of bars\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
        "                f'{height:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "    plt.ylim(0, 105)\n",
        "    plt.title('Average Performance by Category', fontsize=16, pad=20)\n",
        "    plt.ylabel('Score (%)', fontsize=12)\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Career advice based on overall performance\n",
        "    advice_html = '''\n",
        "    <div class=\"result-card\">\n",
        "        <div class=\"summary-title\">Key Takeaways and Advice</div>\n",
        "    '''\n",
        "\n",
        "    # Find weakest and strongest areas\n",
        "    areas = {\n",
        "        \"Technical knowledge\": avg_technical,\n",
        "        \"Communication clarity\": avg_communication,\n",
        "        \"Problem-solving approach\": avg_problem_solving,\n",
        "        \"Team and culture fit\": avg_cultural_fit\n",
        "    }\n",
        "\n",
        "    sorted_areas = sorted(areas.items(), key=lambda x: x[1])\n",
        "    weakest_area = sorted_areas[0]\n",
        "    strongest_area = sorted_areas[-1]\n",
        "\n",
        "    advice_html += f'''\n",
        "        <div class=\"recommendation\">\n",
        "            <strong>Your strongest area is {strongest_area[0]} ({format_score(strongest_area[1])}).</strong><br>\n",
        "            Continue to leverage this strength in interviews. Share specific examples that highlight your capabilities in this area.\n",
        "        </div>\n",
        "\n",
        "        <div class=\"warning\">\n",
        "            <strong>Focus on improving {weakest_area[0]} ({format_score(weakest_area[1])}).</strong><br>\n",
        "            Consider preparing specific examples and practicing responses related to this area before your next interview.\n",
        "        </div>\n",
        "    '''\n",
        "\n",
        "    if avg_overall >= 80:\n",
        "        advice_html += '''\n",
        "        <div class=\"recommendation\">\n",
        "            <strong>You show strong interview skills.</strong><br>\n",
        "            Consider applying for more challenging positions. Your interview performance suggests you're ready for advanced roles.\n",
        "        </div>\n",
        "        '''\n",
        "    elif avg_overall >= 60:\n",
        "        advice_html += '''\n",
        "        <div class=\"recommendation\">\n",
        "            <strong>You have a good foundation.</strong><br>\n",
        "            Practice more mock interviews to boost confidence. Focus on providing more specific examples in your answers.\n",
        "        </div>\n",
        "        '''\n",
        "    else:\n",
        "        advice_html += '''\n",
        "        <div class=\"warning\">\n",
        "            <strong>Continue practicing interview scenarios.</strong><br>\n",
        "            Focus on your weaker areas and consider recording yourself to review your communication style and body language.\n",
        "        </div>\n",
        "        '''\n",
        "\n",
        "    advice_html += '''\n",
        "        <div style=\"margin-top: 30px;\">\n",
        "            <strong>Recommended next steps:</strong>\n",
        "            <ol>\n",
        "                <li>Review and practice questions in your weaker areas</li>\n",
        "                <li>Record yourself answering questions to analyze your communication</li>\n",
        "                <li>Seek feedback from industry professionals if possible</li>\n",
        "                <li>Consider another practice session focusing on your weakest topics</li>\n",
        "            </ol>\n",
        "        </div>\n",
        "    </div>\n",
        "    '''\n",
        "\n",
        "    display(HTML(advice_html))\n",
        "\n",
        "    # Question breakdown\n",
        "    question_html = '''\n",
        "    <div class=\"result-card\">\n",
        "        <div class=\"summary-title\">Question Breakdown</div>\n",
        "        <table style=\"width: 100%; border-collapse: collapse;\">\n",
        "            <tr style=\"background-color: #4285F4; color: white;\">\n",
        "                <th style=\"padding: 10px; text-align: left;\">Question</th>\n",
        "                <th style=\"padding: 10px; text-align: center;\">Overall Score</th>\n",
        "                <th style=\"padding: 10px; text-align: center;\">Category</th>\n",
        "            </tr>\n",
        "    '''\n",
        "\n",
        "    for i, result in enumerate(all_results):\n",
        "        predictions = result['predictions']\n",
        "        question = result['question']\n",
        "        overall = float(safe_extract(predictions, 'overall'))\n",
        "        category = str(safe_extract(predictions, 'category'))\n",
        "\n",
        "        # Alternate row colors\n",
        "        bg_color = \"#f8f9fa\" if i % 2 == 0 else \"white\"\n",
        "\n",
        "        question_html += f'''\n",
        "            <tr style=\"background-color: {bg_color};\">\n",
        "                <td style=\"padding: 10px; border-bottom: 1px solid #dadce0;\">{question}</td>\n",
        "                <td style=\"padding: 10px; text-align: center; border-bottom: 1px solid #dadce0;\">{format_score(overall)} {get_score_emoji(overall)}</td>\n",
        "                <td style=\"padding: 10px; text-align: center; border-bottom: 1px solid #dadce0;\">{category}</td>\n",
        "            </tr>\n",
        "        '''\n",
        "\n",
        "    question_html += '''\n",
        "        </table>\n",
        "    </div>\n",
        "    '''\n",
        "\n",
        "    display(HTML(question_html))\n",
        "\n",
        "# ============================\n",
        "# ENHANCED INTERVIEW FUNCTIONS\n",
        "# ============================\n",
        "\n",
        "def enhanced_run_interview(max_questions=3, all_results=None, question_num=1):\n",
        "    \"\"\"\n",
        "    Run the interactive interview process with an enhanced Colab UI.\n",
        "\n",
        "    Args:\n",
        "        max_questions: Maximum number of questions to ask\n",
        "        all_results: List to store all predictions\n",
        "        question_num: Current question number\n",
        "    \"\"\"\n",
        "    # Initialize all_results if this is the first question\n",
        "    if all_results is None:\n",
        "        all_results = []\n",
        "\n",
        "        # Set up Colab environment if first question\n",
        "        if USING_COLAB:\n",
        "            setup_colab_environment()\n",
        "\n",
        "    # Clear the output\n",
        "    if USING_COLAB:\n",
        "        clear_output(wait=True)\n",
        "    else:\n",
        "        if 'clear_screen' in globals():\n",
        "            clear_screen()\n",
        "        else:\n",
        "            print(\"\\n\" * 5)  # Simple fallback\n",
        "\n",
        "    # Show welcome message on the first question\n",
        "    if question_num == 1:\n",
        "        display_header(\"🤖 AI Interview Evaluation System 🤖\")\n",
        "\n",
        "        welcome_html = f'''\n",
        "        <div style=\"text-align: center; margin: 20px 0;\">\n",
        "            <p>Welcome to the enhanced interview evaluation experience!</p>\n",
        "            <p>I'll ask you {max_questions} interview questions, analyze your responses,\n",
        "            and provide detailed feedback on your performance.</p>\n",
        "        </div>\n",
        "        '''\n",
        "\n",
        "        if USING_COLAB:\n",
        "            display(HTML(welcome_html))\n",
        "        else:\n",
        "            print(f\"Welcome to the AI Interview Evaluation System!\")\n",
        "            print(f\"I'll ask you {max_questions} interview questions, analyze your responses,\")\n",
        "            print(f\"and provide feedback on your performance.\\n\")\n",
        "\n",
        "    # List of interview questions\n",
        "    questions = [\n",
        "        \"Tell me about yourself and your experience in software engineering.\",\n",
        "        \"Describe a challenging project you worked on and how you overcame obstacles.\",\n",
        "        \"How do you approach debugging a complex issue in your code?\",\n",
        "        \"Explain how you would design a scalable web application architecture.\",\n",
        "        \"How do you stay updated with the latest technologies in your field?\",\n",
        "        \"Describe a situation where you had to work with a difficult team member.\",\n",
        "        \"What's your experience with agile development methodologies?\",\n",
        "        \"How would you optimize a slow-performing database query?\",\n",
        "        \"Tell me about a time when you had to learn a new technology quickly.\"\n",
        "    ]\n",
        "\n",
        "    # Randomly select a question\n",
        "    question = random.choice(questions)\n",
        "\n",
        "    # Display the question\n",
        "    display_question(question_num, max_questions, question)\n",
        "\n",
        "    # Get the user's response\n",
        "    if USING_COLAB:\n",
        "        response = create_response_area()\n",
        "    else:\n",
        "        response = input(\"Your response: \")\n",
        "\n",
        "    # Show processing animation\n",
        "    if USING_COLAB:\n",
        "        clear_output(wait=True)\n",
        "        display_processing_animation()\n",
        "    else:\n",
        "        print(\"\\nAnalyzing your response...\")\n",
        "\n",
        "    # Extract features from the response\n",
        "    features = extract_features_from_text(response)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = predict_response_scores(features)\n",
        "\n",
        "    # Clear the output\n",
        "    if USING_COLAB:\n",
        "        clear_output(wait=True)\n",
        "\n",
        "    # Display results\n",
        "    if USING_COLAB:\n",
        "        display_results_card(predictions, question)\n",
        "        plot_radar_chart(predictions)\n",
        "    else:\n",
        "        display_results(predictions, question)\n",
        "\n",
        "    # Store result\n",
        "    result = {\n",
        "        'question': question,\n",
        "        'response': response,\n",
        "        'predictions': predictions\n",
        "    }\n",
        "    all_results.append(result)\n",
        "\n",
        "    # Ask if the user wants to continue to the next question\n",
        "    if question_num < max_questions:\n",
        "        if USING_COLAB:\n",
        "            next_question_html = '''\n",
        "            <div style=\"margin: 30px 0; text-align: center;\">\n",
        "                <button onclick=\"continueInterview()\">Continue to Next Question</button>\n",
        "            </div>\n",
        "\n",
        "            <script>\n",
        "            function continueInterview() {\n",
        "                google.colab.kernel.invokeFunction('notebook.continueInterview', [], {});\n",
        "\n",
        "                // Disable button after click\n",
        "                var buttons = document.getElementsByTagName('button');\n",
        "                for (var i = 0; i < buttons.length; i++) {\n",
        "                    buttons[i].disabled = true;\n",
        "                    buttons[i].innerText = \"Loading...\";\n",
        "                }\n",
        "            }\n",
        "            </script>\n",
        "            '''\n",
        "            display(HTML(next_question_html))\n",
        "\n",
        "            # Register the callback function\n",
        "            from google.colab import output\n",
        "\n",
        "            continue_interview = [False]\n",
        "\n",
        "            def continue_interview_callback():\n",
        "                continue_interview[0] = True\n",
        "\n",
        "            output.register_callback('notebook.continueInterview', continue_interview_callback)\n",
        "\n",
        "            # Wait for the user to click the button\n",
        "            while not continue_interview[0]:\n",
        "                time.sleep(0.5)\n",
        "\n",
        "            # Continue to the next question\n",
        "            return enhanced_run_interview(max_questions, all_results, question_num + 1)\n",
        "        else:\n",
        "            continue_interview = input(\"\\nContinue to next question? (y/n): \").lower().strip()\n",
        "            if continue_interview == 'y':\n",
        "                return enhanced_run_interview(max_questions, all_results, question_num + 1)\n",
        "    else:\n",
        "        # Display final summary\n",
        "        if USING_COLAB:\n",
        "            clear_output(wait=True)\n",
        "\n",
        "        display_summary_dashboard(all_results)\n",
        "\n",
        "    return all_results\n",
        "\n",
        "# ============================\n",
        "# MAIN EXECUTION\n",
        "# ============================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the enhanced interview process.\"\"\"\n",
        "    print(\"Section 10: Enhanced Colab Interview Interface\")\n",
        "    print(\"=============================================\")\n",
        "\n",
        "    if USING_COLAB:\n",
        "        print(\"Running in Google Colab environment\")\n",
        "        setup_colab_environment()\n",
        "    else:\n",
        "        print(\"Not running in Google Colab. Some features will be limited.\")\n",
        "\n",
        "    # Prompt for the number of questions\n",
        "    if USING_COLAB:\n",
        "        questions_html = '''\n",
        "        <div style=\"text-align: center; margin: 20px 0;\">\n",
        "            <h2>How many interview questions would you like to practice?</h2>\n",
        "            <input type=\"number\" id=\"num-questions\" min=\"1\" max=\"10\" value=\"3\" style=\"padding: 10px; font-size: 16px; width: 100px; text-align: center;\">\n",
        "            <button onclick=\"startInterview()\">Start Interview</button>\n",
        "        </div>\n",
        "\n",
        "        <script>\n",
        "        function startInterview() {\n",
        "            var numQuestions = document.getElementById('num-questions').value;\n",
        "            google.colab.kernel.invokeFunction('notebook.startInterview', [numQuestions], {});\n",
        "\n",
        "            // Disable button after click\n",
        "            var buttons = document.getElementsByTagName('button');\n",
        "            for (var i = 0; i < buttons.length; i++) {\n",
        "                buttons[i].disabled = true;\n",
        "                buttons[i].innerText = \"Starting...\";\n",
        "            }\n",
        "        }\n",
        "        </script>\n",
        "        '''\n",
        "        display(HTML(questions_html))\n",
        "\n",
        "        # Register the callback function\n",
        "        from google.colab import output\n",
        "\n",
        "        num_questions = [3]  # Default\n",
        "\n",
        "        def start_interview_callback(n):\n",
        "            try:\n",
        "                num_questions[0] = int(n)\n",
        "            except:\n",
        "                num_questions[0] = 3\n",
        "\n",
        "        output.register_callback('notebook.startInterview', start_interview_callback)\n",
        "\n",
        "        # Wait for the user to click the start button\n",
        "        start_interview = [False]\n",
        "\n",
        "        def trigger_start():\n",
        "            start_interview[0] = True\n",
        "\n",
        "        output.register_callback('notebook.startInterview', trigger_start)\n",
        "\n",
        "        # Wait for the user to start\n",
        "        while not start_interview[0]:\n",
        "            time.sleep(0.5)\n",
        "\n",
        "        # Start the interview with the selected number of questions\n",
        "        enhanced_run_interview(max_questions=num_questions[0])\n",
        "    else:\n",
        "        # CLI version\n",
        "        try:\n",
        "            max_questions = int(input(\"How many interview questions would you like to practice? (1-10): \"))\n",
        "            max_questions = max(1, min(10, max_questions))  # Ensure between 1 and 10\n",
        "        except:\n",
        "            max_questions = 3\n",
        "            print(\"Using default: 3 questions\")\n",
        "\n",
        "        enhanced_run_interview(max_questions=max_questions)\n",
        "\n",
        "# Run the main function when this cell is executed in Colab\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "u_9ioXCKOevq",
        "outputId": "dc664b5e-6bda-4208-864f-8dc8642f8d5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Section 10: Enhanced Colab Interview Interface\n",
            "=============================================\n",
            "Running in Google Colab environment\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "        <style>\n",
              "            .interview-header {\n",
              "                background-color: #4285F4;\n",
              "                color: white;\n",
              "                padding: 15px;\n",
              "                border-radius: 5px;\n",
              "                margin-bottom: 20px;\n",
              "                text-align: center;\n",
              "                font-size: 24px;\n",
              "                font-weight: bold;\n",
              "            }\n",
              "\n",
              "            .question-box {\n",
              "                background-color: #f8f9fa;\n",
              "                border-left: 5px solid #4285F4;\n",
              "                padding: 10px 15px;\n",
              "                margin: 20px 0;\n",
              "                border-radius: 0 5px 5px 0;\n",
              "                font-size: 18px;\n",
              "            }\n",
              "\n",
              "            .response-box {\n",
              "                background-color: #e8f0fe;\n",
              "                border: 1px solid #4285F4;\n",
              "                padding: 15px;\n",
              "                margin: 15px 0;\n",
              "                border-radius: 5px;\n",
              "                min-height: 100px;\n",
              "            }\n",
              "\n",
              "            .result-card {\n",
              "                background-color: white;\n",
              "                border: 1px solid #dadce0;\n",
              "                border-radius: 8px;\n",
              "                padding: 20px;\n",
              "                margin: 20px 0;\n",
              "                box-shadow: 0 1px 2px rgba(60,64,67,0.3);\n",
              "            }\n",
              "\n",
              "            .metric-row {\n",
              "                display: flex;\n",
              "                justify-content: space-between;\n",
              "                margin: 10px 0;\n",
              "                padding: 5px 0;\n",
              "                border-bottom: 1px solid #f1f3f4;\n",
              "            }\n",
              "\n",
              "            .metric-label {\n",
              "                font-weight: bold;\n",
              "                color: #4285F4;\n",
              "            }\n",
              "\n",
              "            .progress-container {\n",
              "                width: 100%;\n",
              "                background-color: #e0e0e0;\n",
              "                border-radius: 5px;\n",
              "                margin: 5px 0;\n",
              "            }\n",
              "\n",
              "            .progress-bar {\n",
              "                height: 20px;\n",
              "                border-radius: 5px;\n",
              "                text-align: center;\n",
              "                color: white;\n",
              "                font-weight: bold;\n",
              "            }\n",
              "\n",
              "            .feedback-box {\n",
              "                background-color: #f1f3f4;\n",
              "                border-left: 5px solid #fbbc04;\n",
              "                padding: 15px;\n",
              "                margin-top: 20px;\n",
              "                border-radius: 0 5px 5px 0;\n",
              "            }\n",
              "\n",
              "            .summary-title {\n",
              "                text-align: center;\n",
              "                font-size: 22px;\n",
              "                font-weight: bold;\n",
              "                margin: 20px 0;\n",
              "                color: #4285F4;\n",
              "                border-bottom: 2px solid #4285F4;\n",
              "                padding-bottom: 10px;\n",
              "            }\n",
              "\n",
              "            .recommendation {\n",
              "                background-color: #e6f4ea;\n",
              "                border-left: 5px solid #34a853;\n",
              "                padding: 15px;\n",
              "                margin: 15px 0;\n",
              "                border-radius: 0 5px 5px 0;\n",
              "            }\n",
              "\n",
              "            .warning {\n",
              "                background-color: #fef7e0;\n",
              "                border-left: 5px solid #fbbc04;\n",
              "                padding: 15px;\n",
              "                margin: 15px 0;\n",
              "                border-radius: 0 5px 5px 0;\n",
              "            }\n",
              "\n",
              "            button {\n",
              "                background-color: #4285F4;\n",
              "                color: white;\n",
              "                border: none;\n",
              "                padding: 10px 15px;\n",
              "                border-radius: 5px;\n",
              "                cursor: pointer;\n",
              "                font-size: 16px;\n",
              "                margin: 10px 5px;\n",
              "            }\n",
              "\n",
              "            button:hover {\n",
              "                background-color: #3367d6;\n",
              "            }\n",
              "        </style>\n",
              "        "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Colab environment setup complete\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "        <div style=\"text-align: center; margin: 20px 0;\">\n",
              "            <h2>How many interview questions would you like to practice?</h2>\n",
              "            <input type=\"number\" id=\"num-questions\" min=\"1\" max=\"10\" value=\"3\" style=\"padding: 10px; font-size: 16px; width: 100px; text-align: center;\">\n",
              "            <button onclick=\"startInterview()\">Start Interview</button>\n",
              "        </div>\n",
              "\n",
              "        <script>\n",
              "        function startInterview() {\n",
              "            var numQuestions = document.getElementById('num-questions').value;\n",
              "            google.colab.kernel.invokeFunction('notebook.startInterview', [numQuestions], {});\n",
              "\n",
              "            // Disable button after click\n",
              "            var buttons = document.getElementsByTagName('button');\n",
              "            for (var i = 0; i < buttons.length; i++) {\n",
              "                buttons[i].disabled = true;\n",
              "                buttons[i].innerText = \"Starting...\";\n",
              "            }\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-02b75557c4a3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m \u001b[0;31m# Run the main function when this cell is executed in Colab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 896\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-02b75557c4a3>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    878\u001b[0m         \u001b[0;31m# Wait for the user to start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstart_interview\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;31m# Start the interview with the selected number of questions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}